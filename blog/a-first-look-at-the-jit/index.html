<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>Erlang Programming Language</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta name="description" content="Erlang Programming Language"/><meta name="keywords" content="erlang, functional, programming, fault-tolerant, distributed, multi-platform, portable, software, multi-core, smp, concurrency"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="alternate" type="application/rss+xml" title="Overall RSS 2.0 Feed" href="/rss"/><link rel="alternate" type="application/rss+xml" title="News RSS 2.0 Feed" href="/rss/news"/><link rel="alternate" type="application/rss+xml" title="Article RSS 2.0 Feed" href="/rss/articles"/><link rel="alternate" type="application/rss+xml" title="Events RSS 2.0 Feed" href="/rss/event"/><link rel="alternate" type="application/rss+xml" title="Downloads RSS 2.0 Feed" href="/rss/download"/><script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/css/4ee9685e1cc0da7f69f0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4ee9685e1cc0da7f69f0.css" data-n-g=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-a9a86200c2afaf3f233a.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.8b4ad2366e12f882e3d5.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-290342a5f5c41b7caf9d.js" as="script"/><link rel="preload" href="/_next/static/chunks/9f96d65d.6d2fb2f6923d41a412a8.js" as="script"/><link rel="preload" href="/_next/static/chunks/87b308f4a72b1b263da2fe072492c20d1199252a.8368050723e578161621.js" as="script"/><link rel="preload" href="/_next/static/chunks/2968c2e854f156f56dcf4f3a0f05db49ee39d399.938502fc97c89e1a18f7.js" as="script"/><link rel="preload" href="/_next/static/chunks/a9595808cf5ee3285d96b2a14ee913304b9362ca.20c663042d024f8cc93c.js" as="script"/><link rel="preload" href="/_next/static/chunks/202314b546da1167b19f69946a64c65ef91ce335.8b78b06e27cf5f3f1fd9.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/blog/%5Bblog%5D-68965b4ff10aaca11026.js" as="script"/></head><body><div id="__next"><div class="navbar" style="background-color:#FFF;margin-bottom:0px"><div class="container"><button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse" style="position:absolute;right:5px;margin-bottom:0px"><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand" href="/"><img src="/img/erlang.png" width="60"/></a><div class="nav-collapse collapse navbar-responsive-collapse" style="padding:20px"><ul class="nav navbar-nav"><li><a class="menu-headlines" href="/downloads/"> DOWNLOADS </a></li><li><a class="menu-headlines" href="/docs/"> DOCUMENTATION </a></li><li><a class="menu-headlines" href="/community/"> COMMUNITY </a></li><li><a class="menu-headlines" href="/news/"> NEWS </a></li><li><a class="menu-headlines" href="/eeps/"> EEPS </a></li><li><a class="menu-headlines" href="/blog/"> BLOG </a></li><li><a class="menu-headlines" href="/about/"> ABOUT </a></li></ul></div></div></div><div class="container"><div class="row"><div class="col-lg-12"><div class="divider"><p></p></div></div><div class="col-lg-12"><h3 class="sub-headlines"><img src="/img/news.png"/><span style="position:relative;top:5px;left:20px">NEWS</span></h3></div><div class="col-lg-2"><p></p></div><div class="col-lg-8"><div class="inside-cols"><h3><a href="/blog/a-first-look-at-the-jit/">A first look at the JIT</a></h3><p><em>Tuesday, 3 November 2020<!-- --> - <!-- -->John HÃ¶gberg</em></p><p>Now that we&#x27;ve had a look at <a href="http://blog.erlang.org/a-brief-BEAM-primer/">BEAM</a> and the <a href="http://blog.erlang.org/a-closer-look-at-the-interpreter/">interpreter</a> we&#x27;re going to
explore one of the most exciting additions in OTP 24: the just-in-time
compiler, or &quot;JIT&quot; for short.</p><p>If you&#x27;re like me the word &quot;JIT&quot; probably makes you think of Hotspot (Java) or
V8 (Javascript). These are very impressive pieces of engineering but they seem
to have hijacked the term; not all JITs are that sophisticated, nor do they
have to be in order to be fast.</p><p>We&#x27;ve made many attempts at a JIT over the years that aimed for the stars only
to fall down. Our latest and by far most successful attempt went for simplicity
instead, trading slight inefficiencies in the generated code for ease of
implementation. If we exclude the run-time assembler library we use, <a href="https://asmjit.com/">asmjit</a>,
the entire thing is roughly as big as the interpreter.</p><p>I believe much of our success can be attributed to four ideas we had early in
the project:</p><ol><li><p><strong>All modules are always compiled to machine code.</strong></p><p>Previous attempts (and HiPE too) had a difficult time switching between the
interpreter and machine code: it was either too slow, too difficult to
maintain, or both.</p><p>Always running machine code means we never have to switch.</p></li><li><p><strong>Data may only be kept (passed) in BEAM registers between instructions.</strong></p><p>This may seem silly, aren&#x27;t machine registers faster?</p><p>Yes, but in practice not by much and it would make things more complicated.
By always passing data in BEAM registers we can use the register allocation
given to us by the Erlang compiler, saving us from having to do this very
expensive step at runtime.</p><p>More importantly, this minimizes the difference between the interpreter and
the JIT from the runtime system&#x27;s point of view.</p></li><li><p><strong>Modules are compiled one instruction at a time.</strong></p><p>One of the most difficult problems in our prior attempts was to strike a
good balance between the time it took to compile something and the eagerness
to do so. If we&#x27;re too eager, we&#x27;ll spend too much time compiling, and if
we&#x27;re too lax we won&#x27;t see any improvements.</p><p>This problem was largely self-inflicted and caused by the compiler being too
slow (we often used LLVM), which was made worse by us giving it large pieces
of code to allow more optimizations.</p><p>By limiting ourselves to compiling one instruction at a time, we leave some
performance on the table but greatly improve compilation speed.</p></li><li><p><strong>Every instruction has a handwritten machine code template.</strong></p><p>This makes compilation <em>extremely</em> fast as we basically just copy-paste
the template every time the instruction is used, only performing some minor
tweaks depending on its arguments.</p><p>This may seem daunting at first but it&#x27;s actually not that bad once you get
used to it. While it certainly takes a lot of code to achieve even the
smallest of things, it&#x27;s inherently simple and easy to follow as long as
the code is kept short.</p><p>The downside is that every instruction needs to be implemented for each
architecture, but luckily there&#x27;s not a lot of popular ones and we hope to
support the two most common ones by the time we release OTP 24: <code>x86_64</code>
and <code>AArch64</code>. The others will continue to use the interpreter.</p></li></ol><p>When compiling a module the JIT goes through the instructions one by one,
invoking machine code templates as it goes. This has two very large benefits
over the interpreter: there&#x27;s no need to jump between them because they&#x27;re
emitted back-to-back and the end of each is the start of the next one, and the
arguments don&#x27;t need to be resolved at runtime because they&#x27;re already &quot;burnt
in.&quot;</p><p>Now that we have some background, let&#x27;s look at the machine code template for
our example in the previous post, <code>is_nonempty_list</code>:</p><pre><code class="language-c++">/* Arguments are passed as `ArgVal` objects which hold a
 * type and a value, for example saying &quot;X register 4&quot;,
 * &quot;the atom &#x27;hello&#x27;&quot;, &quot;label 57&quot; and so on. */
void BeamModuleAssembler::emit_is_nonempty_list(const ArgVal &amp;Fail,
                                                const ArgVal &amp;Src) {
    /* Figure out which memory address `Src` lives in. */
    x86:Mem list_ptr = getArgRef(Src);

    /* Emit a `test` instruction, which does a non-
     * destructive AND on the memory pointed at by
     * list_ptr, clearing the zero flag if the list is
     * empty. */
    a.test(list_ptr, imm(_TAG_PRIMARY_MASK - TAG_PRIMARY_LIST));

    /* Emit a `jnz` instruction, jumping to the fail label
     * if the zero flag is clear (the list is empty). */
    a.jnz(labels[Fail.getValue()]);

    /* Unlike the interpreter there&#x27;s no need to jump to
     * the next instruction on success as it immediately
     * follows this one. */
}</code></pre><p>This template will generate code that looks almost identical to the template
itself. Let&#x27;s say our source is &quot;<code>X</code> register 1&quot; and our fail label is 57:</p><pre><code>test qword ptr [rbx+8], _TAG_PRIMARY_MASK - TAG_PRIMARY_LIST
jnz label_57</code></pre><p>This is much faster than the interpreter, and even a bit more compact than the
threaded code, but this is a trivial instruction. What about more complex
ones? Let&#x27;s have a look at the <code>timeout</code> instruction in the interpreter:</p><pre><code class="language-c++">timeout() {
    if (IS_TRACED_FL(c_p, F_TRACE_RECEIVE)) {
        trace_receive(c_p, am_clock_service, am_timeout, NULL);
    }
    if (ERTS_PROC_GET_SAVED_CALLS_BUF(c_p)) {
        save_calls(c_p, &amp;exp_timeout);
    }
    c_p-&gt;flags &amp;= ~F_TIMO;
    JOIN_MESSAGE(c_p);
}</code></pre><p>That&#x27;s bound to be a lot of code, and those macros will be really annoying to
convert by hand. How on earth are we going to do this without losing our minds?</p><p>By cheating, that&#x27;s how :D</p><pre><code class="language-c++">static void timeout(Process *c_p) {
    if (IS_TRACED_FL(c_p, F_TRACE_RECEIVE)) {
        trace_receive(c_p, am_clock_service, am_timeout, NULL);
    }
    if (ERTS_PROC_GET_SAVED_CALLS_BUF(c_p)) {
        save_calls(c_p, &amp;exp_timeout);
    }
    c_p-&gt;flags &amp;= ~F_TIMO;
    JOIN_MESSAGE(c_p);
}

void BeamModuleAssembler::emit_timeout() {
    /* Set the first C argument to our currently executing
     * process, c_p, and then call the above C function. */
    a.mov(ARG1, c_p);
    a.call(imm(timeout));
}</code></pre><p>This little escape hatch saved us from having to write everything in assembler
from the start, and many instructions remain like this because there hasn&#x27;t
been any point to changing them.</p><p>That&#x27;s all for today. In the next post we&#x27;ll walk through our conventions and
some of the techniques we&#x27;ve used to reduce the code size.</p></div></div><div class="col-lg-2"><p><a href="/rss/blog/"><img src="/img/rss-icon.png" width="64"/></a></p></div></div></div><div class="container"><div class="row"><div class="col-lg-12"><div class="divider"><p></p></div></div><div class="col-lg-12 text-center"><div class="col-lg-4"><a title="DOWNLOAD" href="/download.html"><img src="/img/download.png"/></a></div><div class="col-lg-4"><a href="http://www.github.com/erlang/otp/"><img src="/img/GitHub-Mark-32px.png"/></a></div><div class="col-lg-4"><a href="http://www.twitter.com/erlang_org/"><img src="/img/twitter.png" width="32"/></a></div></div><div class="col-lg-12"><div class="divider"><p></p></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"item":{"id":"a-first-look-at-the-jit","title":"A first look at the JIT","author":"John HÃ¶gberg","excerpt":"\nNow that we've had a look at [BEAM] and the [interpreter] we're going to\nexplore one of the most exciting additions in OTP 24: the just-in-time\ncompiler, or \"JIT\" for short.\n\n[asmjit]: https://asmjit.com/,\n[BEAM]: http://blog.erlang.org/a-brief-BEAM-primer/,\n[interpreter]: http://blog.erlang.org/a-closer-look-at-the-interpreter/","article_date":1604361600000,"tags":["BEAM","erts","jit"],"frontmatter":{"layout":"post","title":"A first look at the JIT","tags":"BEAM erts jit","author":"John HÃ¶gberg"},"content":"\nNow that we've had a look at [BEAM] and the [interpreter] we're going to\nexplore one of the most exciting additions in OTP 24: the just-in-time\ncompiler, or \"JIT\" for short.\n\nIf you're like me the word \"JIT\" probably makes you think of Hotspot (Java) or\nV8 (Javascript). These are very impressive pieces of engineering but they seem\nto have hijacked the term; not all JITs are that sophisticated, nor do they\nhave to be in order to be fast.\n\nWe've made many attempts at a JIT over the years that aimed for the stars only\nto fall down. Our latest and by far most successful attempt went for simplicity\ninstead, trading slight inefficiencies in the generated code for ease of\nimplementation. If we exclude the run-time assembler library we use, [asmjit],\nthe entire thing is roughly as big as the interpreter.\n\nI believe much of our success can be attributed to four ideas we had early in\nthe project:\n\n1. **All modules are always compiled to machine code.**\n\n   Previous attempts (and HiPE too) had a difficult time switching between the\n   interpreter and machine code: it was either too slow, too difficult to\n   maintain, or both.\n\n   Always running machine code means we never have to switch.\n\n2. **Data may only be kept (passed) in BEAM registers between instructions.**\n\n   This may seem silly, aren't machine registers faster?\n\n   Yes, but in practice not by much and it would make things more complicated.\n   By always passing data in BEAM registers we can use the register allocation\n   given to us by the Erlang compiler, saving us from having to do this very\n   expensive step at runtime.\n\n   More importantly, this minimizes the difference between the interpreter and\n   the JIT from the runtime system's point of view.\n\n3. **Modules are compiled one instruction at a time.**\n\n   One of the most difficult problems in our prior attempts was to strike a\n   good balance between the time it took to compile something and the eagerness\n   to do so. If we're too eager, we'll spend too much time compiling, and if\n   we're too lax we won't see any improvements.\n\n   This problem was largely self-inflicted and caused by the compiler being too\n   slow (we often used LLVM), which was made worse by us giving it large pieces\n   of code to allow more optimizations.\n\n   By limiting ourselves to compiling one instruction at a time, we leave some\n   performance on the table but greatly improve compilation speed.\n\n4. **Every instruction has a handwritten machine code template.**\n\n   This makes compilation _extremely_ fast as we basically just copy-paste\n   the template every time the instruction is used, only performing some minor\n   tweaks depending on its arguments.\n\n   This may seem daunting at first but it's actually not that bad once you get\n   used to it. While it certainly takes a lot of code to achieve even the\n   smallest of things, it's inherently simple and easy to follow as long as\n   the code is kept short.\n\n   The downside is that every instruction needs to be implemented for each\n   architecture, but luckily there's not a lot of popular ones and we hope to\n   support the two most common ones by the time we release OTP 24: `x86_64`\n   and `AArch64`. The others will continue to use the interpreter.\n\nWhen compiling a module the JIT goes through the instructions one by one,\ninvoking machine code templates as it goes. This has two very large benefits\nover the interpreter: there's no need to jump between them because they're\nemitted back-to-back and the end of each is the start of the next one, and the\narguments don't need to be resolved at runtime because they're already \"burnt\nin.\"\n\nNow that we have some background, let's look at the machine code template for\nour example in the previous post, `is_nonempty_list`:\n\n```c++\n/* Arguments are passed as `ArgVal` objects which hold a\n * type and a value, for example saying \"X register 4\",\n * \"the atom 'hello'\", \"label 57\" and so on. */\nvoid BeamModuleAssembler::emit_is_nonempty_list(const ArgVal \u0026Fail,\n                                                const ArgVal \u0026Src) {\n    /* Figure out which memory address `Src` lives in. */\n    x86:Mem list_ptr = getArgRef(Src);\n\n    /* Emit a `test` instruction, which does a non-\n     * destructive AND on the memory pointed at by\n     * list_ptr, clearing the zero flag if the list is\n     * empty. */\n    a.test(list_ptr, imm(_TAG_PRIMARY_MASK - TAG_PRIMARY_LIST));\n\n    /* Emit a `jnz` instruction, jumping to the fail label\n     * if the zero flag is clear (the list is empty). */\n    a.jnz(labels[Fail.getValue()]);\n\n    /* Unlike the interpreter there's no need to jump to\n     * the next instruction on success as it immediately\n     * follows this one. */\n}\n```\n\nThis template will generate code that looks almost identical to the template\nitself. Let's say our source is \"`X` register 1\" and our fail label is 57:\n\n```\ntest qword ptr [rbx+8], _TAG_PRIMARY_MASK - TAG_PRIMARY_LIST\njnz label_57\n```\n\nThis is much faster than the interpreter, and even a bit more compact than the\nthreaded code, but this is a trivial instruction. What about more complex\nones? Let's have a look at the `timeout` instruction in the interpreter:\n\n```c++\ntimeout() {\n    if (IS_TRACED_FL(c_p, F_TRACE_RECEIVE)) {\n        trace_receive(c_p, am_clock_service, am_timeout, NULL);\n    }\n    if (ERTS_PROC_GET_SAVED_CALLS_BUF(c_p)) {\n        save_calls(c_p, \u0026exp_timeout);\n    }\n    c_p-\u003eflags \u0026= ~F_TIMO;\n    JOIN_MESSAGE(c_p);\n}\n```\n\nThat's bound to be a lot of code, and those macros will be really annoying to\nconvert by hand. How on earth are we going to do this without losing our minds?\n\nBy cheating, that's how :D\n\n```c++\nstatic void timeout(Process *c_p) {\n    if (IS_TRACED_FL(c_p, F_TRACE_RECEIVE)) {\n        trace_receive(c_p, am_clock_service, am_timeout, NULL);\n    }\n    if (ERTS_PROC_GET_SAVED_CALLS_BUF(c_p)) {\n        save_calls(c_p, \u0026exp_timeout);\n    }\n    c_p-\u003eflags \u0026= ~F_TIMO;\n    JOIN_MESSAGE(c_p);\n}\n\nvoid BeamModuleAssembler::emit_timeout() {\n    /* Set the first C argument to our currently executing\n     * process, c_p, and then call the above C function. */\n    a.mov(ARG1, c_p);\n    a.call(imm(timeout));\n}\n```\n\nThis little escape hatch saved us from having to write everything in assembler\nfrom the start, and many instructions remain like this because there hasn't\nbeen any point to changing them.\n\nThat's all for today. In the next post we'll walk through our conventions and\nsome of the techniques we've used to reduce the code size.\n\n[asmjit]: https://asmjit.com/\n[BEAM]: http://blog.erlang.org/a-brief-BEAM-primer/\n[interpreter]: http://blog.erlang.org/a-closer-look-at-the-interpreter/\n"}},"__N_SSG":true},"page":"/blog/[blog]","query":{"blog":"a-first-look-at-the-jit"},"buildId":"Ouo7gQeKiYSKAal9g8wsl","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"charSet":"utf-8"}],["title",{"children":"Erlang Programming Language"}],["meta",{"httpEquiv":"Content-Type","content":"text/html;charset=utf-8"}],["meta",{"name":"description","content":"Erlang Programming Language"}],["meta",{"name":"keywords","content":"erlang, functional, programming, fault-tolerant, distributed, multi-platform, portable, software, multi-core, smp, concurrency"}],["meta",{"name":"viewport","content":"width=device-width, initial-scale=1.0"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Overall RSS 2.0 Feed","href":"/rss"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"News RSS 2.0 Feed","href":"/rss/news"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Article RSS 2.0 Feed","href":"/rss/articles"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Events RSS 2.0 Feed","href":"/rss/event"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Downloads RSS 2.0 Feed","href":"/rss/download"}],["script",{"src":"https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"}],["script",{"src":"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js","integrity":"sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa","crossOrigin":"anonymous"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-8bebc7aacc0076174a73.js"></script><script src="/_next/static/chunks/main-a9a86200c2afaf3f233a.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" async=""></script><script src="/_next/static/chunks/commons.8b4ad2366e12f882e3d5.js" async=""></script><script src="/_next/static/chunks/pages/_app-290342a5f5c41b7caf9d.js" async=""></script><script src="/_next/static/chunks/9f96d65d.6d2fb2f6923d41a412a8.js" async=""></script><script src="/_next/static/chunks/87b308f4a72b1b263da2fe072492c20d1199252a.8368050723e578161621.js" async=""></script><script src="/_next/static/chunks/2968c2e854f156f56dcf4f3a0f05db49ee39d399.938502fc97c89e1a18f7.js" async=""></script><script src="/_next/static/chunks/a9595808cf5ee3285d96b2a14ee913304b9362ca.20c663042d024f8cc93c.js" async=""></script><script src="/_next/static/chunks/202314b546da1167b19f69946a64c65ef91ce335.8b78b06e27cf5f3f1fd9.js" async=""></script><script src="/_next/static/chunks/pages/blog/%5Bblog%5D-68965b4ff10aaca11026.js" async=""></script><script src="/_next/static/Ouo7gQeKiYSKAal9g8wsl/_buildManifest.js" async=""></script><script src="/_next/static/Ouo7gQeKiYSKAal9g8wsl/_ssgManifest.js" async=""></script></body></html>