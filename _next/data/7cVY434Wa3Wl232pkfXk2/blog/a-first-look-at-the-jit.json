{"pageProps":{"item":{"id":"a-first-look-at-the-jit","title":"A first look at the JIT","author":"John Högberg","excerpt":"\nNow that we've had a look at [BEAM] and the [interpreter] we're going to\nexplore one of the most exciting additions in OTP 24: the just-in-time\ncompiler, or \"JIT\" for short.","article_date":1604361600000,"tags":["BEAM erts jit"],"frontmatter":{"layout":"post","title":"A first look at the JIT","tags":"BEAM erts jit","author":"John Högberg"},"content":"\nNow that we've had a look at [BEAM] and the [interpreter] we're going to\nexplore one of the most exciting additions in OTP 24: the just-in-time\ncompiler, or \"JIT\" for short.\n\nIf you're like me the word \"JIT\" probably makes you think of Hotspot (Java) or\nV8 (Javascript). These are very impressive pieces of engineering but they seem\nto have hijacked the term; not all JITs are that sophisticated, nor do they\nhave to be in order to be fast.\n\nWe've made many attempts at a JIT over the years that aimed for the stars only\nto fall down. Our latest and by far most successful attempt went for simplicity\ninstead, trading slight inefficiencies in the generated code for ease of\nimplementation. If we exclude the run-time assembler library we use, [asmjit],\nthe entire thing is roughly as big as the interpreter.\n\nI believe much of our success can be attributed to four ideas we had early in\nthe project:\n\n1. **All modules are always compiled to machine code.**\n\n   Previous attempts (and HiPE too) had a difficult time switching between the\n   interpreter and machine code: it was either too slow, too difficult to\n   maintain, or both.\n\n   Always running machine code means we never have to switch.\n\n2. **Data may only be kept (passed) in BEAM registers between instructions.**\n\n   This may seem silly, aren't machine registers faster?\n\n   Yes, but in practice not by much and it would make things more complicated.\n   By always passing data in BEAM registers we can use the register allocation\n   given to us by the Erlang compiler, saving us from having to do this very\n   expensive step at runtime.\n\n   More importantly, this minimizes the difference between the interpreter and\n   the JIT from the runtime system's point of view.\n\n3. **Modules are compiled one instruction at a time.**\n\n   One of the most difficult problems in our prior attempts was to strike a\n   good balance between the time it took to compile something and the eagerness\n   to do so. If we're too eager, we'll spend too much time compiling, and if\n   we're too lax we won't see any improvements.\n\n   This problem was largely self-inflicted and caused by the compiler being too\n   slow (we often used LLVM), which was made worse by us giving it large pieces\n   of code to allow more optimizations.\n\n   By limiting ourselves to compiling one instruction at a time, we leave some\n   performance on the table but greatly improve compilation speed.\n\n4. **Every instruction has a handwritten machine code template.**\n\n   This makes compilation _extremely_ fast as we basically just copy-paste\n   the template every time the instruction is used, only performing some minor\n   tweaks depending on its arguments.\n\n   This may seem daunting at first but it's actually not that bad once you get\n   used to it. While it certainly takes a lot of code to achieve even the\n   smallest of things, it's inherently simple and easy to follow as long as\n   the code is kept short.\n\n   The downside is that every instruction needs to be implemented for each\n   architecture, but luckily there's not a lot of popular ones and we hope to\n   support the two most common ones by the time we release OTP 24: `x86_64`\n   and `AArch64`. The others will continue to use the interpreter.\n\nWhen compiling a module the JIT goes through the instructions one by one,\ninvoking machine code templates as it goes. This has two very large benefits\nover the interpreter: there's no need to jump between them because they're\nemitted back-to-back and the end of each is the start of the next one, and the\narguments don't need to be resolved at runtime because they're already \"burnt\nin.\"\n\nNow that we have some background, let's look at the machine code template for\nour example in the previous post, `is_nonempty_list`:\n\n```c++\n/* Arguments are passed as `ArgVal` objects which hold a\n * type and a value, for example saying \"X register 4\",\n * \"the atom 'hello'\", \"label 57\" and so on. */\nvoid BeamModuleAssembler::emit_is_nonempty_list(const ArgVal &Fail,\n                                                const ArgVal &Src) {\n    /* Figure out which memory address `Src` lives in. */\n    x86:Mem list_ptr = getArgRef(Src);\n\n    /* Emit a `test` instruction, which does a non-\n     * destructive AND on the memory pointed at by\n     * list_ptr, clearing the zero flag if the list is\n     * empty. */\n    a.test(list_ptr, imm(_TAG_PRIMARY_MASK - TAG_PRIMARY_LIST));\n\n    /* Emit a `jnz` instruction, jumping to the fail label\n     * if the zero flag is clear (the list is empty). */\n    a.jnz(labels[Fail.getValue()]);\n\n    /* Unlike the interpreter there's no need to jump to\n     * the next instruction on success as it immediately\n     * follows this one. */\n}\n```\n\nThis template will generate code that looks almost identical to the template\nitself. Let's say our source is \"`X` register 1\" and our fail label is 57:\n\n```\ntest qword ptr [rbx+8], _TAG_PRIMARY_MASK - TAG_PRIMARY_LIST\njnz label_57\n```\n\nThis is much faster than the interpreter, and even a bit more compact than the\nthreaded code, but this is a trivial instruction. What about more complex\nones? Let's have a look at the `timeout` instruction in the interpreter:\n\n```c++\ntimeout() {\n    if (IS_TRACED_FL(c_p, F_TRACE_RECEIVE)) {\n        trace_receive(c_p, am_clock_service, am_timeout, NULL);\n    }\n    if (ERTS_PROC_GET_SAVED_CALLS_BUF(c_p)) {\n        save_calls(c_p, &exp_timeout);\n    }\n    c_p->flags &= ~F_TIMO;\n    JOIN_MESSAGE(c_p);\n}\n```\n\nThat's bound to be a lot of code, and those macros will be really annoying to\nconvert by hand. How on earth are we going to do this without losing our minds?\n\nBy cheating, that's how :D\n\n```c++\nstatic void timeout(Process *c_p) {\n    if (IS_TRACED_FL(c_p, F_TRACE_RECEIVE)) {\n        trace_receive(c_p, am_clock_service, am_timeout, NULL);\n    }\n    if (ERTS_PROC_GET_SAVED_CALLS_BUF(c_p)) {\n        save_calls(c_p, &exp_timeout);\n    }\n    c_p->flags &= ~F_TIMO;\n    JOIN_MESSAGE(c_p);\n}\n\nvoid BeamModuleAssembler::emit_timeout() {\n    /* Set the first C argument to our currently executing\n     * process, c_p, and then call the above C function. */\n    a.mov(ARG1, c_p);\n    a.call(imm(timeout));\n}\n```\n\nThis little escape hatch saved us from having to write everything in assembler\nfrom the start, and many instructions remain like this because there hasn't\nbeen any point to changing them.\n\nThat's all for today. In the next post we'll walk through our conventions and\nsome of the techniques we've used to reduce the code size.\n\n[asmjit]: https://asmjit.com/\n[BEAM]: http://blog.erlang.org/a-brief-BEAM-primer/\n[interpreter]: http://blog.erlang.org/a-closer-look-at-the-interpreter/\n"}},"__N_SSG":true}