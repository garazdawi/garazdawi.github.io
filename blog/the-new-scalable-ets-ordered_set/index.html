<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>Erlang Programming Language</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta name="description" content="Erlang Programming Language"/><meta name="keywords" content="erlang, functional, programming, fault-tolerant, distributed, multi-platform, portable, software, multi-core, smp, concurrency"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="alternate" type="application/rss+xml" title="Overall RSS 2.0 Feed" href="/rss"/><link rel="alternate" type="application/rss+xml" title="News RSS 2.0 Feed" href="/rss/news"/><link rel="alternate" type="application/rss+xml" title="Article RSS 2.0 Feed" href="/rss/articles"/><link rel="alternate" type="application/rss+xml" title="Events RSS 2.0 Feed" href="/rss/event"/><link rel="alternate" type="application/rss+xml" title="Downloads RSS 2.0 Feed" href="/rss/download"/><script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/css/4ee9685e1cc0da7f69f0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4ee9685e1cc0da7f69f0.css" data-n-g=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-a9a86200c2afaf3f233a.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.8b4ad2366e12f882e3d5.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-290342a5f5c41b7caf9d.js" as="script"/><link rel="preload" href="/_next/static/chunks/9f96d65d.6d2fb2f6923d41a412a8.js" as="script"/><link rel="preload" href="/_next/static/chunks/87b308f4a72b1b263da2fe072492c20d1199252a.8368050723e578161621.js" as="script"/><link rel="preload" href="/_next/static/chunks/2968c2e854f156f56dcf4f3a0f05db49ee39d399.938502fc97c89e1a18f7.js" as="script"/><link rel="preload" href="/_next/static/chunks/a9595808cf5ee3285d96b2a14ee913304b9362ca.20c663042d024f8cc93c.js" as="script"/><link rel="preload" href="/_next/static/chunks/202314b546da1167b19f69946a64c65ef91ce335.8b78b06e27cf5f3f1fd9.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/blog/%5Bblog%5D-68965b4ff10aaca11026.js" as="script"/></head><body><div id="__next"><div class="navbar" style="background-color:#FFF;margin-bottom:0px"><div class="container"><button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse" style="position:absolute;right:5px;margin-bottom:0px"><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand" href="/"><img src="/img/erlang.png" width="60"/></a><div class="nav-collapse collapse navbar-responsive-collapse" style="padding:20px"><ul class="nav navbar-nav"><li><a class="menu-headlines" href="/downloads/"> DOWNLOADS </a></li><li><a class="menu-headlines" href="/docs/"> DOCUMENTATION </a></li><li><a class="menu-headlines" href="/community/"> COMMUNITY </a></li><li><a class="menu-headlines" href="/news/"> NEWS </a></li><li><a class="menu-headlines" href="/eeps/"> EEPS </a></li><li><a class="menu-headlines" href="/blog/"> BLOG </a></li><li><a class="menu-headlines" href="/about/"> ABOUT </a></li></ul></div></div></div><div class="container"><div class="row"><div class="col-lg-12"><div class="divider"><p></p></div></div><div class="col-lg-12"><h3 class="sub-headlines"><img src="/img/news.png"/><span style="position:relative;top:5px;left:20px">NEWS</span></h3></div><div class="col-lg-2"><p></p></div><div class="col-lg-8"><div class="inside-cols"><h3><a href="/blog/the-new-scalable-ets-ordered_set/">The New Scalable ETS ordered_set</a></h3><p><em>Wednesday, 19 August 2020<!-- --> - <!-- -->Kjell Winblad</em></p><p>The scalability of ETS tables of type <code>ordered_set</code> with the
<code>write_concurrency</code> option is substantially better in Erlang/OTP 22
than earlier releases. In some extreme cases, you can expect
more than 100 times better throughput in Erlang/OTP 22 compared to
Erlang/OTP 21. The cause of this improvement is a new data structure
called <a href="https://doi.org/10.1016/j.jpdc.2017.11.007">the contention adapting search tree</a> (CA tree
for short). This blog post will give you insights into how the CA tree
works and show you benchmark results comparing the performance of ETS
<code>ordered_set</code> tables in OTP 21 and OTP 22.</p><h2>Try it Out!</h2><p><a href="/code/insert_disjoint_ranges.erl">This escript</a> makes it convenient for you
to try the new <code>ordered_set</code> implementation on your own machine with
Erlang/OTP 22+ installed.</p><p>The escript measures the time it takes for <code>P</code> Erlang processes to
insert <code>N</code> integers into an <code>ordered_set</code> ETS table, where <code>P</code> and <code>N</code>
are parameters to the escript. The CA tree is only utilized when the
ETS table options <code>ordered_set</code> and <code>{write_concurrency, true}</code> are
active. One can, therefore, easily compare the new data structure&#x27;s
performance with the old one (an <a href="https://en.wikipedia.org/wiki/AVL_tree">AVL tree</a> protected by a
single readers-writer lock). The <code>write_concurrency</code> option had no
effect on <code>ordered_set</code> tables before the release of Erlang/OTP 22.</p><p>We get the following results when running the escript on a developer laptop with
two cores (Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz):</p><p>{% highlight console %}</p><p>$ escript insert_disjoint_ranges.erl old 1 10000000
Time: 3.352332 seconds
$ escript insert_disjoint_ranges.erl old 2 10000000
Time: 3.961732 seconds
$ escript insert_disjoint_ranges.erl old 4 10000000
Time: 6.382199 seconds
$ escript insert_disjoint_ranges.erl new 1 10000000
Time: 3.832119 seconds
$ escript insert_disjoint_ranges.erl new 2 10000000
Time: 2.109476 seconds
$ escript insert_disjoint_ranges.erl new 4 10000000
Time: 1.66509 seconds</p><p>{% endhighlight %}</p><p>We see that in this particular benchmark, the CA tree has superior
scalability to the old data structure. The benchmark ran about twice
as fast with the new data structure and four processes as with the old
data structure and one process (the machine only has two
cores). We will look at the performance and scalability of the new CA
tree-based implementation in greater detail later after describing how
the CA tree works.</p><h2>The Contention Adapting Search Tree in a Nutshell</h2><p>The key feature that distinguishes the CA tree from other concurrent
data structures is that the CA tree dynamically changes its
synchronization granularity based on how much contention is detected
inside the data structure. This way, the CA tree can avoid the
performance and memory overheads that come from using many unnecessary
locks without sacrificing performance when many operations happen in
parallel. For example, let us imagine a scenario where the CA tree is
initially populated from many threads in parallel, and then it is only
used from a single thread. In this scenario, the CA tree will adapt to
use fine-grained synchronization in the population phase (when
fine-grained synchronization reduces contention). The CA tree will then change
to use coarse-grained synchronization in the single-threaded phase
(when coarse-grained synchronization reduces the locking and memory
overheads).</p><p>The structure of a CA tree is illustrated in the following
picture:</p><p><img src="/images/ca_tree/ca_tree_9.png" alt="alt text" title="Contention Adapting Search Tree Structure"/></p><p>The actual items stored in the CA tree are located in
sequential data structures in the bottom layer. These
sequential data structures are protected by the locks in the base
nodes in the middle layer. The base node locks have counters
associated with them. The counter of a base node lock is increased when
contention is detected in the base node lock and decreased when no
such contention is detected. The value of this base node lock counter
decides if a split or a join should happen after an operation has been
performed in a base node. The routing nodes at the top of the picture
above form a binary search tree that directs the search for a
particular item. A routing node also contains a lock and a flag. These
are used when joining base nodes. The details of how splitting and
joining work will not be described in this article, but
the interested reader can find a detailed description in this <a href="https://doi.org/10.1016/j.jpdc.2017.11.007">CA tree
paper</a> (<a href="http://winsh.me/papers/catree_jpdc_paper.pdf">preprint PDF</a>). We will now
illustrate how the CA tree changes its synchronization granularity by
going through an example:</p><ol><li><p>Initially, a CA tree only consists of a single base node with a
sequential data structure as is depicted in the picture below:</p><p><img src="/images/ca_tree/ca_tree_1.png" alt="alt text" title="Initial Contention Adapting Search Tree"/></p></li><li><p>If parallel threads access the CA tree, the value of a base node&#x27;s
counter may eventually reach the threshold that indicates that the
base node should be split. A base node split divides the items in a
base node between two new base nodes and replaces the original base
node with a routing node where the two new base nodes are
rooted. The following picture shows the CA tree after the base node
pointed to by the tree&#x27;s root has been split:</p><p><img src="/images/ca_tree/ca_tree_2.png" alt="alt text" title="First Split Contention Adapting Search Tree"/></p></li><li><p>The process of base node splitting will continue as long as there
is enough contention in base node locks or until the max depth of the
routing layer is reached. The following picture shows how the CA
tree looks like after another split:</p><p><img src="/images/ca_tree/ca_tree_3.png" alt="alt text" title="Second Split Contention Adapting Search Tree"/></p></li><li><p>The synchronization granularity may differ in different parts of a
CA tree if, for example, a particular part of a CA tree is accessed
more frequently in parallel than the rest. The following picture
shows the CA tree after yet another split:</p><p><img src="/images/ca_tree/ca_tree_4.png" alt="alt text" title="Third Split Contention Adapting Search Tree"/></p></li><li><p>The following picture shows the CA tree after the fourth split:</p><p><img src="/images/ca_tree/ca_tree_5.png" alt="alt text" title="Fourth Split Contention Adapting Search Tree"/></p></li><li><p>The following picture shows the CA tree after the fifth split:</p><p><img src="/images/ca_tree/ca_tree_6.png" alt="alt text" title="Fifth Split Contention Adapting Search Tree"/></p></li><li><p>Two base nodes holding adjacent ranges of items can be joined. Such
a join will be triggered after an operation sees that a base
node counter&#x27;s value is below a certain threshold. Remember that a
base node&#x27;s counter is decreased if a thread does not experience
contention when acquiring the base node&#x27;s lock.</p>&lt;!--The conters The likelihood that
a join will be triggered in a certain base node gets higher when
the probablity of contention that does not detect contention in the
base node lock is high. The likelihood that two base nodes are
joined is also increased if operations that require both base nodes
happens often enough (to reduce the overhead of acquiring locks).--&gt;<p><img src="/images/ca_tree/ca_tree_7.png" alt="alt text" title="Join of two base nodes in a  Contention Adapting Search Tree"/></p></li><li><p>As you might have noticed from the illustrations above, splitting
and joining results in that old base nodes and
routing nodes gets spliced-out from the tree. The memory that these
nodes occupy needs to be reclaimed, but this can not happen directly
after they have got spliced-out as some threads might still be
reading them. The Erlang run-time system has a mechanism called
<a href="https://github.com/erlang/otp/blob/d6285b0a347b9489ce939511ee9a979acd868f71/erts/emulator/internal_doc/ThreadProgress.md">thread progress</a>,
which the ETS CA tree implementation uses to reclaim these nodes
safely.</p><p><img src="/images/ca_tree/ca_tree_8.png" alt="alt text" title="Spliced-out base nodes and routing nodes have been reclaimed."/></p></li></ol><p><a href="/images/ca_tree/ca_tree_ani.gif">Click here</a> to see an animation of the example.</p><h2>Benchmark</h2><p>The performance of the new CA tree-based ETS <code>ordered_set</code>
implementation has been evaluated in a benchmark that measures the
throughput (operations per second) in many scenarios. The
benchmark lets a configurable number of Erlang processes perform a
configurable distribution of operations on a single ETS table. The
curious reader can find the source code of the benchmark in the <a href="https://github.com/erlang/otp/blob/ba2c374d3d6fcba479bb542eb6ecd5d8216ce84b/lib/stdlib/test/ets_SUITE.erl#L7623">test
suite for
ETS</a>.</p><p>The following figures show results from this benchmark on a machine
with two Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz (32 cores in total
with hyper-threading). The average set size in all scenarios was
about 500K. More details about the benchmark machine and configuration
can be found on <a href="http://blog.erlang.org/bench/ets_ord_set_21_vs_22/21_vs_22.html">this
page</a>.</p><p><img src="/bench/ets_ord_set_21_vs_22/plot_1.png" alt="alt text" title="benchmark results"/></p><p><img src="/bench/ets_ord_set_21_vs_22/plot_2.png" alt="alt text" title="benchmark results"/></p><p><img src="/bench/ets_ord_set_21_vs_22/plot_3.png" alt="alt text" title="benchmark results"/></p><p><img src="/bench/ets_ord_set_21_vs_22/plot_7.png" alt="alt text" title="benchmark results"/></p><p><img src="/bench/ets_ord_set_21_vs_22/plot_8.png" alt="alt text" title="benchmark results"/></p><p><img src="/bench/ets_ord_set_21_vs_22/plot_5.png" alt="alt text" title="benchmark results"/></p><p><img src="/bench/ets_ord_set_21_vs_22/plot_6.png" alt="alt text" title="benchmark results"/></p><p><img src="/bench/ets_ord_set_21_vs_22/plot_4.png" alt="alt text" title="benchmark results"/></p><p>We see that the throughput of the CA tree-based <code>ordered_set</code> (OTP-22)
improves when we add cores all the way up to 64 cores, while the old
implementation&#x27;s (OTP-21) throughput often gets worse when more
processes are added. The old implementation&#x27;s write operations are
serialized as the data structure is protected by a single
readers-writer lock. The slowdown of the old version when adding more
cores is mainly caused by increased communication overhead when more
cores try to acquire the same lock and by the fact that the competing
cores frequently invalidate each other&#x27;s cache lines.</p><p>The graph for the 100% lookups scenario (the last graph in the list of
graphs above) looks a bit strange at first sight. Why does the CA tree
scale so much better than the old implementation in this scenario? The
answer is almost impossible to guess without knowing the
implementation details of the <code>ordered_set</code> table type. First of all,
the CA tree uses the same readers-writer lock implementation
for its base node locks as the old implementation uses to protect the whole
table. The difference is thus not due to any lock differences. The
default <code>ordered_set</code> implementation (the one that is active when
<code>write_concurrency</code> is off) has an optimization that mainly improves
usage scenarios where a single process iterates over items of the
table, for example, with a sequence of calls to the <code>ets:next/2</code>
function. This optimization keeps a static stack per table. Some
operations use this stack to reduce the number of tree nodes that need
to be traversed. For example, the <code>ets:next/2</code> operation does not need
to recreate the stack, if the top of the stack contains the same key
as the one passed to the operation (see
<a href="https://github.com/erlang/otp/blob/master/erts/emulator/beam/erl_db_tree.c#L3084">here</a>). As there is only one static stack per
table and potentially many readers (due to the readers-writer lock),
the static stack has to be reserved by the thread that is currently
using it. Unfortunately, the static stack handling is a scalability
bottleneck in scenarios like the one with 100% lookups above. The CA
tree implementation does not have this type of optimization, so it
does not suffer from this scalability bottleneck. However, this also
means that the old implementation may perform better than the new one
when the table is mainly sequentially accessed. One example of when
the old implementation (that still can be used by setting the
<code>write_concurrency</code> option to false) performs better is the single
process case of the 10% <code>insert</code>, 10% <code>delete</code>, 40% <code>lookup</code> and 40%
<code>nextseq1000</code> (a sequence of 1000 <code>ets:next/2</code> calls) scenario (the
second last graph in the list of graphs above).</p><p>Therefore, we can conclude that that turning on <code>write_concurrency</code>
for an <code>ordered_set</code> table is probably a good idea if the table is
accessed from multiple processes in parallel. Still, turning off
<code>write_concurrency</code> might be better if you mainly access the table
sequentially.</p><h2>A Note on Decentralized Counters</h2><p>The CA tree implementation was not the only optimization introduced in
Erlang/OTP 22, affecting the scalability of <code>ordered_set</code> with
<code>write_concurrency</code>. An optimization that decentralized counters in
<code>ordered_set</code> tables with <code>write_concurrency</code> turned on was also
introduced in Erlang/OTP 22 (see <a href="https://github.com/erlang/otp/pull/2190">here</a>).  An
option to enable the same optimization in all table types was
introduced in Erlang/OTP 23 (see <a href="https://github.com/erlang/otp/pull/2229">here</a>). You can
find benchmark results comparing the scalability of the tables with
and without decentralized counters <a href="http://winsh.me/ets_catree_benchmark/azure_D64s_decent_ctrs/hash_decentralized_ctrs.html">here</a>.</p><h2>Further Reading</h2><p>The following paper describes the CA tree and some optimizations (of which some have not been applied to the ETS CA tree yet) in much more detail than this blog post. The paper also includes an experimental comparison with related data structures.</p><ul><li><em><a href="https://doi.org/10.1016/j.jpdc.2017.11.007">A Contention Adapting Approach to Concurrent Ordered Sets</a> (<a href="http://winsh.me/papers/catree_jpdc_paper.pdf">preprint</a>). Journal of Parallel and Distributed Computing, 2018. Konstantinos Sagonas and Kjell Winblad</em></li></ul><p>There is also a lock-free variant of the CA tree that is described in the following paper. The lock-free CA tree uses immutable data structures in its base nodes to substantially reduce the amount of time range queries, and similar operations can conflict with other operations.</p><ul><li><em><a href="https://doi.org/10.1145/3210377.3210413">Lock-free Contention Adapting Search Trees</a> (<a href="http://winsh.me/papers/spaa2018lfcatree.pdf">preprint</a>). In the proceedings of the 30th Symposium on Parallelism in Algorithms and Architectures (SPAA 2018). Kjell Winblad, Konstantinos Sagonas, and Bengt Jonsson.</em></li></ul><p>The following paper, which discusses and evaluates a prototypical CA tree implementation for ETS, was the first CA tree-related paper.</p><ul><li><em><a href="http://dl.acm.org/citation.cfm?id=2633455">More Scalable Ordered Set for ETS Using Adaptation</a> (<a href="http://winsh.me/papers/erlang_workshop_2014.pdf">preprint</a>). In Thirteenth ACM SIGPLAN workshop on Erlang (2014). Konstantinos Sagonas and Kjell Winblad</em></li></ul><p>You can look directly at the <a href="https://github.com/erlang/otp/blob/4ca912b859f779d6d9b235ea0cf6fb7662edcc59/erts/emulator/beam/erl_db_catree.c">ETS CA tree source
code</a> if you are interested in specific
implementation details. Finally, it might also be interesting to look
at the <a href="http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1220366&amp;dswid=6575">author&#x27;s Ph.D. thesis</a> if you want to get
more links to related work or want to know more about the motivation
for concurrent data structures that adapt to contention.</p><h2>Conclusion</h2><p>The Erlang/OTP 22 release introduced a new ETS <code>ordered_set</code>
implementation that is active when the <code>write_concurrency</code> option is
turned on. This data structure (a contention adapting search tree) has
superior scalability to the old data structure in many different
scenarios and a design that gives it excellent performance in a variety
of scenarios that benefit from different synchronization
granularities.</p></div></div><div class="col-lg-2"><p><a href="/rss/blog/"><img src="/img/rss-icon.png" width="64"/></a></p></div></div></div><div class="container"><div class="row"><div class="col-lg-12"><div class="divider"><p></p></div></div><div class="col-lg-12 text-center"><div class="col-lg-4"><a title="DOWNLOAD" href="/download.html"><img src="/img/download.png"/></a></div><div class="col-lg-4"><a href="http://www.github.com/erlang/otp/"><img src="/img/GitHub-Mark-32px.png"/></a></div><div class="col-lg-4"><a href="http://www.twitter.com/erlang_org/"><img src="/img/twitter.png" width="32"/></a></div></div><div class="col-lg-12"><div class="divider"><p></p></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"item":{"id":"the-new-scalable-ets-ordered_set","title":"The New Scalable ETS ordered_set","author":"Kjell Winblad","excerpt":"\nThe scalability of ETS tables of type `ordered_set` with the\n`write_concurrency` option is substantially better in Erlang/OTP 22\nthan earlier releases. In some extreme cases, you can expect\nmore than 100 times better throughput in Erlang/OTP 22 compared to\nErlang/OTP 21. The cause of this improvement is a new data structure\ncalled [the contention adapting search tree][jpdc_ca_tree] (CA tree\nfor short). This blog post will give you insights into how the CA tree\nworks and show you benchmark results comparing the performance of ETS\n`ordered_set` tables in OTP 21 and OTP 22.\n\n[jpdc_ca_tree]: https://doi.org/10.1016/j.jpdc.2017.11.007,\n[jpdc_ca_tree_preprint]: http://winsh.me/papers/catree_jpdc_paper.pdf,\n[lfca_tree]: https://doi.org/10.1145/3210377.3210413,\n[lfca_tree_preprint]: http://winsh.me/papers/spaa2018lfcatree.pdf,\n[erlang_workshop]: http://dl.acm.org/citation.cfm?id=2633455,\n[erlang_workshop_preprint]: http://winsh.me/papers/erlang_workshop_2014.pdf,\n[AVLTree]: https://en.wikipedia.org/wiki/AVL_tree,\n[kjell_phd_thesis]: http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1220366\u0026dswid=6575,\n[ets_lookup_stack_opt]: https://github.com/erlang/otp/blob/4ca912b859f779d6d9b235ea0cf6fb7662edcc59/erts/emulator/beam/erl_db_tree.c#L3306,\n[decent_ctrs_pull1]: https://github.com/erlang/otp/pull/2190,\n[decent_ctrs_pull2]: https://github.com/erlang/otp/pull/2229,\n[decent_ctrs_bench]: http://winsh.me/ets_catree_benchmark/azure_D64s_decent_ctrs/hash_decentralized_ctrs.html,\n[ets_ca_tree_code]: https://github.com/erlang/otp/blob/4ca912b859f779d6d9b235ea0cf6fb7662edcc59/erts/emulator/beam/erl_db_catree.c,\n[ets_next_stack_opt]: https://github.com/erlang/otp/blob/master/erts/emulator/beam/erl_db_tree.c#L3084","article_date":1597795200000,"tags":["ETS","ordered_set","scalability","CA","tree"],"frontmatter":{"layout":"post","title":"The New Scalable ETS ordered_set","tags":"ETS ordered_set scalability CA tree","author":"Kjell Winblad"},"content":"\nThe scalability of ETS tables of type `ordered_set` with the\n`write_concurrency` option is substantially better in Erlang/OTP 22\nthan earlier releases. In some extreme cases, you can expect\nmore than 100 times better throughput in Erlang/OTP 22 compared to\nErlang/OTP 21. The cause of this improvement is a new data structure\ncalled [the contention adapting search tree][jpdc_ca_tree] (CA tree\nfor short). This blog post will give you insights into how the CA tree\nworks and show you benchmark results comparing the performance of ETS\n`ordered_set` tables in OTP 21 and OTP 22.\n\n## Try it Out!\n\n[This escript](/code/insert_disjoint_ranges.erl) makes it convenient for you\nto try the new `ordered_set` implementation on your own machine with\nErlang/OTP 22+ installed.\n\nThe escript measures the time it takes for `P` Erlang processes to\ninsert `N` integers into an `ordered_set` ETS table, where `P` and `N`\nare parameters to the escript. The CA tree is only utilized when the\nETS table options `ordered_set` and `{write_concurrency, true}` are\nactive. One can, therefore, easily compare the new data structure's\nperformance with the old one (an [AVL tree][AVLTree] protected by a\nsingle readers-writer lock). The `write_concurrency` option had no\neffect on `ordered_set` tables before the release of Erlang/OTP 22.\n\nWe get the following results when running the escript on a developer laptop with\ntwo cores (Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz):\n\n{% highlight console %}\n\n$ escript insert_disjoint_ranges.erl old 1 10000000\nTime: 3.352332 seconds\n$ escript insert_disjoint_ranges.erl old 2 10000000\nTime: 3.961732 seconds\n$ escript insert_disjoint_ranges.erl old 4 10000000\nTime: 6.382199 seconds\n$ escript insert_disjoint_ranges.erl new 1 10000000\nTime: 3.832119 seconds\n$ escript insert_disjoint_ranges.erl new 2 10000000\nTime: 2.109476 seconds\n$ escript insert_disjoint_ranges.erl new 4 10000000\nTime: 1.66509 seconds\n\n{% endhighlight %}\n\nWe see that in this particular benchmark, the CA tree has superior\nscalability to the old data structure. The benchmark ran about twice\nas fast with the new data structure and four processes as with the old\ndata structure and one process (the machine only has two\ncores). We will look at the performance and scalability of the new CA\ntree-based implementation in greater detail later after describing how\nthe CA tree works.\n\n## The Contention Adapting Search Tree in a Nutshell\n\nThe key feature that distinguishes the CA tree from other concurrent\ndata structures is that the CA tree dynamically changes its\nsynchronization granularity based on how much contention is detected\ninside the data structure. This way, the CA tree can avoid the\nperformance and memory overheads that come from using many unnecessary\nlocks without sacrificing performance when many operations happen in\nparallel. For example, let us imagine a scenario where the CA tree is\ninitially populated from many threads in parallel, and then it is only\nused from a single thread. In this scenario, the CA tree will adapt to\nuse fine-grained synchronization in the population phase (when\nfine-grained synchronization reduces contention). The CA tree will then change\nto use coarse-grained synchronization in the single-threaded phase\n(when coarse-grained synchronization reduces the locking and memory\noverheads).\n\nThe structure of a CA tree is illustrated in the following\npicture:\n\n![alt text](/images/ca_tree/ca_tree_9.png \"Contention Adapting Search Tree Structure\")\n\nThe actual items stored in the CA tree are located in\nsequential data structures in the bottom layer. These\nsequential data structures are protected by the locks in the base\nnodes in the middle layer. The base node locks have counters\nassociated with them. The counter of a base node lock is increased when\ncontention is detected in the base node lock and decreased when no\nsuch contention is detected. The value of this base node lock counter\ndecides if a split or a join should happen after an operation has been\nperformed in a base node. The routing nodes at the top of the picture\nabove form a binary search tree that directs the search for a\nparticular item. A routing node also contains a lock and a flag. These\nare used when joining base nodes. The details of how splitting and\njoining work will not be described in this article, but\nthe interested reader can find a detailed description in this [CA tree\npaper][jpdc_ca_tree] ([preprint PDF][jpdc_ca_tree_preprint]). We will now\nillustrate how the CA tree changes its synchronization granularity by\ngoing through an example:\n\n1. Initially, a CA tree only consists of a single base node with a\n   sequential data structure as is depicted in the picture below:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_1.png \"Initial Contention Adapting Search Tree\")\n2. If parallel threads access the CA tree, the value of a base node's\n   counter may eventually reach the threshold that indicates that the\n   base node should be split. A base node split divides the items in a\n   base node between two new base nodes and replaces the original base\n   node with a routing node where the two new base nodes are\n   rooted. The following picture shows the CA tree after the base node\n   pointed to by the tree's root has been split:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_2.png \"First Split Contention Adapting Search Tree\")\n3. The process of base node splitting will continue as long as there\n   is enough contention in base node locks or until the max depth of the\n   routing layer is reached. The following picture shows how the CA\n   tree looks like after another split:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_3.png \"Second Split Contention Adapting Search Tree\")\n4. The synchronization granularity may differ in different parts of a\n   CA tree if, for example, a particular part of a CA tree is accessed\n   more frequently in parallel than the rest. The following picture\n   shows the CA tree after yet another split:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_4.png \"Third Split Contention Adapting Search Tree\")\n5. The following picture shows the CA tree after the fourth split:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_5.png \"Fourth Split Contention Adapting Search Tree\")\n6. The following picture shows the CA tree after the fifth split:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_6.png \"Fifth Split Contention Adapting Search Tree\")\n7. Two base nodes holding adjacent ranges of items can be joined. Such\n   a join will be triggered after an operation sees that a base\n   node counter's value is below a certain threshold. Remember that a\n   base node's counter is decreased if a thread does not experience\n   contention when acquiring the base node's lock.\n   \u003c!--The conters The likelihood that\n   a join will be triggered in a certain base node gets higher when\n   the probablity of contention that does not detect contention in the\n   base node lock is high. The likelihood that two base nodes are\n   joined is also increased if operations that require both base nodes\n   happens often enough (to reduce the overhead of acquiring locks).--\u003e\n   \n   ![alt text](/images/ca_tree/ca_tree_7.png \"Join of two base nodes in a  Contention Adapting Search Tree\")\n8. As you might have noticed from the illustrations above, splitting\n   and joining results in that old base nodes and\n   routing nodes gets spliced-out from the tree. The memory that these\n   nodes occupy needs to be reclaimed, but this can not happen directly\n   after they have got spliced-out as some threads might still be\n   reading them. The Erlang run-time system has a mechanism called\n   [thread progress](https://github.com/erlang/otp/blob/d6285b0a347b9489ce939511ee9a979acd868f71/erts/emulator/internal_doc/ThreadProgress.md),\n   which the ETS CA tree implementation uses to reclaim these nodes\n   safely.\n   \n   ![alt text](/images/ca_tree/ca_tree_8.png \"Spliced-out base nodes and routing nodes have been reclaimed.\")\n\n[Click here](/images/ca_tree/ca_tree_ani.gif) to see an animation of the example.\n\n## Benchmark\n\nThe performance of the new CA tree-based ETS `ordered_set`\nimplementation has been evaluated in a benchmark that measures the\nthroughput (operations per second) in many scenarios. The\nbenchmark lets a configurable number of Erlang processes perform a\nconfigurable distribution of operations on a single ETS table. The\ncurious reader can find the source code of the benchmark in the [test\nsuite for\nETS](https://github.com/erlang/otp/blob/ba2c374d3d6fcba479bb542eb6ecd5d8216ce84b/lib/stdlib/test/ets_SUITE.erl#L7623).\n\nThe following figures show results from this benchmark on a machine\nwith two Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz (32 cores in total\nwith hyper-threading). The average set size in all scenarios was\nabout 500K. More details about the benchmark machine and configuration\ncan be found on [this\npage](http://blog.erlang.org/bench/ets_ord_set_21_vs_22/21_vs_22.html).\n\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_1.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_2.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_3.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_7.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_8.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_5.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_6.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_4.png \"benchmark results\")\n\nWe see that the throughput of the CA tree-based `ordered_set` (OTP-22)\nimproves when we add cores all the way up to 64 cores, while the old\nimplementation's (OTP-21) throughput often gets worse when more\nprocesses are added. The old implementation's write operations are\nserialized as the data structure is protected by a single\nreaders-writer lock. The slowdown of the old version when adding more\ncores is mainly caused by increased communication overhead when more\ncores try to acquire the same lock and by the fact that the competing\ncores frequently invalidate each other's cache lines.\n\nThe graph for the 100% lookups scenario (the last graph in the list of\ngraphs above) looks a bit strange at first sight. Why does the CA tree\nscale so much better than the old implementation in this scenario? The\nanswer is almost impossible to guess without knowing the\nimplementation details of the `ordered_set` table type. First of all,\nthe CA tree uses the same readers-writer lock implementation\nfor its base node locks as the old implementation uses to protect the whole\ntable. The difference is thus not due to any lock differences. The\ndefault `ordered_set` implementation (the one that is active when\n`write_concurrency` is off) has an optimization that mainly improves\nusage scenarios where a single process iterates over items of the\ntable, for example, with a sequence of calls to the `ets:next/2`\nfunction. This optimization keeps a static stack per table. Some\noperations use this stack to reduce the number of tree nodes that need\nto be traversed. For example, the `ets:next/2` operation does not need\nto recreate the stack, if the top of the stack contains the same key\nas the one passed to the operation (see\n[here][ets_next_stack_opt]). As there is only one static stack per\ntable and potentially many readers (due to the readers-writer lock),\nthe static stack has to be reserved by the thread that is currently\nusing it. Unfortunately, the static stack handling is a scalability\nbottleneck in scenarios like the one with 100% lookups above. The CA\ntree implementation does not have this type of optimization, so it\ndoes not suffer from this scalability bottleneck. However, this also\nmeans that the old implementation may perform better than the new one\nwhen the table is mainly sequentially accessed. One example of when\nthe old implementation (that still can be used by setting the\n`write_concurrency` option to false) performs better is the single\nprocess case of the 10% `insert`, 10% `delete`, 40% `lookup` and 40%\n`nextseq1000` (a sequence of 1000 `ets:next/2` calls) scenario (the\nsecond last graph in the list of graphs above).\n\nTherefore, we can conclude that that turning on `write_concurrency`\nfor an `ordered_set` table is probably a good idea if the table is\naccessed from multiple processes in parallel. Still, turning off\n`write_concurrency` might be better if you mainly access the table\nsequentially.\n\n## A Note on Decentralized Counters\n\nThe CA tree implementation was not the only optimization introduced in\nErlang/OTP 22, affecting the scalability of `ordered_set` with\n`write_concurrency`. An optimization that decentralized counters in\n`ordered_set` tables with `write_concurrency` turned on was also\nintroduced in Erlang/OTP 22 (see [here][decent_ctrs_pull1]).  An\noption to enable the same optimization in all table types was\nintroduced in Erlang/OTP 23 (see [here][decent_ctrs_pull2]). You can\nfind benchmark results comparing the scalability of the tables with\nand without decentralized counters [here][decent_ctrs_bench].\n\n## Further Reading\n\n\nThe following paper describes the CA tree and some optimizations (of which some have not been applied to the ETS CA tree yet) in much more detail than this blog post. The paper also includes an experimental comparison with related data structures.\n\n* *[A Contention Adapting Approach to Concurrent Ordered Sets][jpdc_ca_tree] ([preprint][jpdc_ca_tree_preprint]). Journal of Parallel and Distributed Computing, 2018. Konstantinos Sagonas and Kjell Winblad*\n\nThere is also a lock-free variant of the CA tree that is described in the following paper. The lock-free CA tree uses immutable data structures in its base nodes to substantially reduce the amount of time range queries, and similar operations can conflict with other operations.\n\n* *[Lock-free Contention Adapting Search Trees][lfca_tree] ([preprint][lfca_tree_preprint]). In the proceedings of the 30th Symposium on Parallelism in Algorithms and Architectures (SPAA 2018). Kjell Winblad, Konstantinos Sagonas, and Bengt Jonsson.*\n\nThe following paper, which discusses and evaluates a prototypical CA tree implementation for ETS, was the first CA tree-related paper.\n\n* *[More Scalable Ordered Set for ETS Using Adaptation][erlang_workshop] ([preprint][erlang_workshop_preprint]). In Thirteenth ACM SIGPLAN workshop on Erlang (2014). Konstantinos Sagonas and Kjell Winblad*\n\nYou can look directly at the [ETS CA tree source\ncode][ets_ca_tree_code] if you are interested in specific\nimplementation details. Finally, it might also be interesting to look\nat the [author's Ph.D. thesis][kjell_phd_thesis] if you want to get\nmore links to related work or want to know more about the motivation\nfor concurrent data structures that adapt to contention.\n\n## Conclusion\n\nThe Erlang/OTP 22 release introduced a new ETS `ordered_set`\nimplementation that is active when the `write_concurrency` option is\nturned on. This data structure (a contention adapting search tree) has\nsuperior scalability to the old data structure in many different\nscenarios and a design that gives it excellent performance in a variety\nof scenarios that benefit from different synchronization\ngranularities.\n\n\n[jpdc_ca_tree]: https://doi.org/10.1016/j.jpdc.2017.11.007\n[jpdc_ca_tree_preprint]: http://winsh.me/papers/catree_jpdc_paper.pdf\n[lfca_tree]: https://doi.org/10.1145/3210377.3210413\n[lfca_tree_preprint]: http://winsh.me/papers/spaa2018lfcatree.pdf\n[erlang_workshop]: http://dl.acm.org/citation.cfm?id=2633455\n[erlang_workshop_preprint]: http://winsh.me/papers/erlang_workshop_2014.pdf\n[AVLTree]: https://en.wikipedia.org/wiki/AVL_tree\n[kjell_phd_thesis]: http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1220366\u0026dswid=6575\n[ets_lookup_stack_opt]: https://github.com/erlang/otp/blob/4ca912b859f779d6d9b235ea0cf6fb7662edcc59/erts/emulator/beam/erl_db_tree.c#L3306\n[decent_ctrs_pull1]: https://github.com/erlang/otp/pull/2190\n[decent_ctrs_pull2]: https://github.com/erlang/otp/pull/2229\n[decent_ctrs_bench]: http://winsh.me/ets_catree_benchmark/azure_D64s_decent_ctrs/hash_decentralized_ctrs.html\n[ets_ca_tree_code]: https://github.com/erlang/otp/blob/4ca912b859f779d6d9b235ea0cf6fb7662edcc59/erts/emulator/beam/erl_db_catree.c\n[ets_next_stack_opt]: https://github.com/erlang/otp/blob/master/erts/emulator/beam/erl_db_tree.c#L3084\n"}},"__N_SSG":true},"page":"/blog/[blog]","query":{"blog":"the-new-scalable-ets-ordered_set"},"buildId":"aoeL97fiGQMaunPBLLCoJ","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"charSet":"utf-8"}],["title",{"children":"Erlang Programming Language"}],["meta",{"httpEquiv":"Content-Type","content":"text/html;charset=utf-8"}],["meta",{"name":"description","content":"Erlang Programming Language"}],["meta",{"name":"keywords","content":"erlang, functional, programming, fault-tolerant, distributed, multi-platform, portable, software, multi-core, smp, concurrency"}],["meta",{"name":"viewport","content":"width=device-width, initial-scale=1.0"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Overall RSS 2.0 Feed","href":"/rss"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"News RSS 2.0 Feed","href":"/rss/news"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Article RSS 2.0 Feed","href":"/rss/articles"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Events RSS 2.0 Feed","href":"/rss/event"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Downloads RSS 2.0 Feed","href":"/rss/download"}],["script",{"src":"https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"}],["script",{"src":"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js","integrity":"sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa","crossOrigin":"anonymous"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-8bebc7aacc0076174a73.js"></script><script src="/_next/static/chunks/main-a9a86200c2afaf3f233a.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" async=""></script><script src="/_next/static/chunks/commons.8b4ad2366e12f882e3d5.js" async=""></script><script src="/_next/static/chunks/pages/_app-290342a5f5c41b7caf9d.js" async=""></script><script src="/_next/static/chunks/9f96d65d.6d2fb2f6923d41a412a8.js" async=""></script><script src="/_next/static/chunks/87b308f4a72b1b263da2fe072492c20d1199252a.8368050723e578161621.js" async=""></script><script src="/_next/static/chunks/2968c2e854f156f56dcf4f3a0f05db49ee39d399.938502fc97c89e1a18f7.js" async=""></script><script src="/_next/static/chunks/a9595808cf5ee3285d96b2a14ee913304b9362ca.20c663042d024f8cc93c.js" async=""></script><script src="/_next/static/chunks/202314b546da1167b19f69946a64c65ef91ce335.8b78b06e27cf5f3f1fd9.js" async=""></script><script src="/_next/static/chunks/pages/blog/%5Bblog%5D-68965b4ff10aaca11026.js" async=""></script><script src="/_next/static/aoeL97fiGQMaunPBLLCoJ/_buildManifest.js" async=""></script><script src="/_next/static/aoeL97fiGQMaunPBLLCoJ/_ssgManifest.js" async=""></script></body></html>