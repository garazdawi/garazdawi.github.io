<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>Erlang Programming Language</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta name="description" content="Erlang Programming Language"/><meta name="keywords" content="erlang, functional, programming, fault-tolerant, distributed, multi-platform, portable, software, multi-core, smp, concurrency"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="alternate" type="application/rss+xml" title="Overall RSS 2.0 Feed" href="/rss"/><link rel="alternate" type="application/rss+xml" title="News RSS 2.0 Feed" href="/rss/news"/><link rel="alternate" type="application/rss+xml" title="Article RSS 2.0 Feed" href="/rss/articles"/><link rel="alternate" type="application/rss+xml" title="Events RSS 2.0 Feed" href="/rss/event"/><link rel="alternate" type="application/rss+xml" title="Downloads RSS 2.0 Feed" href="/rss/download"/><script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/css/f91c265ba4ec340e076d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f91c265ba4ec340e076d.css" data-n-g=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-a9a86200c2afaf3f233a.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.8b4ad2366e12f882e3d5.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-79f3144d8877c67ce98b.js" as="script"/><link rel="preload" href="/_next/static/chunks/9f96d65d.6d2fb2f6923d41a412a8.js" as="script"/><link rel="preload" href="/_next/static/chunks/87b308f4a72b1b263da2fe072492c20d1199252a.8368050723e578161621.js" as="script"/><link rel="preload" href="/_next/static/chunks/2968c2e854f156f56dcf4f3a0f05db49ee39d399.938502fc97c89e1a18f7.js" as="script"/><link rel="preload" href="/_next/static/chunks/a9595808cf5ee3285d96b2a14ee913304b9362ca.20c663042d024f8cc93c.js" as="script"/><link rel="preload" href="/_next/static/chunks/202314b546da1167b19f69946a64c65ef91ce335.ea40b862c8a8db76db57.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/blog-ad17a6542c3c0ffa69b7.js" as="script"/></head><body><div id="__next"><div class="navbar" style="background-color:#FFF;margin-bottom:0px"><div class="container"><button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse" style="position:absolute;right:5px;margin-bottom:0px"><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand" href="/"><img src="/img/erlang.png" width="60"/></a><div class="nav-collapse collapse navbar-responsive-collapse" style="padding:20px"><ul class="nav navbar-nav"><li><a class="menu-headlines" href="/downloads/"> DOWNLOADS </a></li><li><a class="menu-headlines" href="/docs/"> DOCUMENTATION </a></li><li><a class="menu-headlines" href="/community/"> COMMUNITY </a></li><li><a class="menu-headlines" href="/news/"> NEWS </a></li><li><a class="menu-headlines" href="/eeps/"> EEPS </a></li><li><a class="menu-headlines" href="/blog/"> BLOG </a></li><li><a class="menu-headlines" href="/about/"> ABOUT </a></li></ul></div></div></div><div class="container"><div class="row"><div class="col-lg-12"><div class="divider"><p></p></div></div><div class="col-lg-12"><h3 class="sub-headlines"><img src="/img/news.png"/><span style="position:relative;top:5px;left:20px">blog</span></h3></div><div class="col-lg-2"><p></p></div><div class="col-lg-8"><div class="inside-cols"><h3><a href="/blog/a-first-look-at-the-jit/">A first look at the JIT</a></h3><p><em>Tuesday, 3 November 2020<!-- --> - <!-- -->John Högberg</em></p><p>Now that we&#x27;ve had a look at [BEAM] and the [interpreter] we&#x27;re going to
explore one of the most exciting additions in OTP 24: the just-in-time
compiler, or &quot;JIT&quot; for short.</p><h3><a href="/blog/a-closer-look-at-the-interpreter/">A closer look at the interpreter</a></h3><p><em>Tuesday, 27 October 2020<!-- --> - <!-- -->John Högberg</em></p><p>In my [previous post] we had a look at BEAM, and now that we&#x27;re more familiar
with it it&#x27;s time for us to look at the reference implementation: the
interpreter.</p><h3><a href="/blog/a-brief-BEAM-primer/">A brief introduction to BEAM</a></h3><p><em>Tuesday, 20 October 2020<!-- --> - <!-- -->John Högberg</em></p><p>This post is a brief primer on BEAM, the virtual machine that executes user
code in the Erlang Runtime System (ERTS). It&#x27;s intended to help those new to
BEAM follow an upcoming series of posts about the JIT in OTP 24, leaving
implementation details for later.</p><h3><a href="/blog/the-new-scalable-ets-ordered_set/">The New Scalable ETS ordered_set</a></h3><p><em>Wednesday, 19 August 2020<!-- --> - <!-- -->Kjell Winblad</em></p><p>The scalability of ETS tables of type <code>ordered_set</code> with the
<code>write_concurrency</code> option is substantially better in Erlang/OTP 22
than earlier releases. In some extreme cases, you can expect
more than 100 times better throughput in Erlang/OTP 22 compared to
Erlang/OTP 21. The cause of this improvement is a new data structure
called [the contention adapting search tree][jpdc_ca_tree] (CA tree
for short). This blog post will give you insights into how the CA tree
works and show you benchmark results comparing the performance of ETS
<code>ordered_set</code> tables in OTP 21 and OTP 22.</p><h3><a href="/blog/OTP-23-Highlights/">OTP 23 Highlights</a></h3><p><em>Wednesday, 13 May 2020<!-- --> - <!-- -->Kenneth Lundin</em></p><p>OTP 23 has just been released (May 13:th 2020).
It has been a long process with three release
candidates in February, March and April before the final release.
We are very thankful for the feedback we have got regarding the release candidates,
which has revealed some bugs and flaws that our internal testing did not find.</p><h3><a href="/blog/persistent_term/">Clever use of persistent_term</a></h3><p><em>Monday, 9 September 2019<!-- --> - <!-- -->Lukas Larsson</em></p><p>This blog post will go through three different uses of <a href="http://erlang.org/doc/man/persistent_term.html">persistent_term</a>
that I have used since its release and explain a bit why they work so well with
<a href="http://erlang.org/doc/man/persistent_term.html">persistent_term</a>.</p><h3><a href="/blog/OTP-22-Highlights/">OTP 22 Highlights</a></h3><p><em>Monday, 13 May 2019<!-- --> - <!-- -->Lukas Larsson</em></p><p>OTP 22 has just been released. It has been a long process with three release
candidates before the final release. We decided this year to try to get one month
more testing of the major release and I think that the extra time has paid off.
We&#x27;ve received many bug reports from the community about large and small bugs
that our internal tests did not find.</p><h3><a href="/blog/ets-oddity/">ETS oddity</a></h3><p><em>Monday, 7 January 2019<!-- --> - <!-- -->Lukas Larsson</em></p><p>When working with the implementation of the new <a href="https://github.com/erlang/otp/pull/1952">scalable ordered_set</a>
we came across a strangeness with the guarantees when iterating over a table
while inserting elements in parallel.</p><h3><a href="/blog/retired-pitfalls-22/">Retiring old performance pitfalls</a></h3><p><em>Wednesday, 7 November 2018<!-- --> - <!-- -->John Högberg</em></p><p>Erlang/OTP 22 will bring many performance improvements to the table, but most
of them have a broad impact and don&#x27;t affect the way you write efficient code.
In this post I&#x27;d like to highlight a few things that used to be surprisingly
slow but no longer need to be avoided.</p><h3><a href="/blog/ssl-logging-in-otp-22/">TLS logging improvements in OTP 22</a></h3><p><em>Friday, 5 October 2018<!-- --> - <!-- -->Péter Dimitrov</em></p><p>Erlang/OTP 22 will be an important release for the <code>ssl</code> application. We are working on
several new features and improvements such as support for TLS 1.3, some of those are already
on the master branch. This blog post presents the new ssl debug logging built on the new
logger API.</p><h3><a href="/blog/ssa-history/">SSA History</a></h3><p><em>Friday, 28 September 2018<!-- --> - <!-- -->Björn Gustavsson</em></p><p>This blog post looks back on the development of
the [SSA-based intermediate representation][pr1935]
from the beginning of this year to the end
of August when the branch was merged.</p><h3><a href="/blog/digging-deeper-in-ssa/">Digging deeper in SSA</a></h3><p><em>Thursday, 20 September 2018<!-- --> - <!-- -->Björn Gustavsson</em></p><p>This blog post continues the exploration of the [new SSA-based
intermediate representation][pr1935] through multiple examples. Make
sure to read the [Introduction to SSA][prev] if you missed it.</p><h3><a href="/blog/introducing-ssa/">Introduction to SSA</a></h3><p><em>Wednesday, 5 September 2018<!-- --> - <!-- -->Björn Gustavsson</em></p><p>This blog post is an introduction to the [new SSA-based intermediate
representation][pr1935] that has recently been merged to the <code>master</code>
branch in the [Erlang/OTP repository][otp]. It uses the same
example as in the [previous blog post][prev], first looking at the
generated SSA code, and then at some optimizations.</p><h3><a href="/blog/opt-traps-and-pitfalls/">Optimization Traps and Pitfalls</a></h3><p><em>Friday, 24 August 2018<!-- --> - <!-- -->Björn Gustavsson</em></p><p>Back after the summer holidays, this blog will now change tracks and
start a series of blog posts about Static Single Assignment (SSA).
This first installment will set the scene for the posts that follow by
looking at the traps and pitfalls one can fall into when trying to
optimize BEAM assembly code.</p><h3><a href="/blog/beam-compiler-history/">A Brief History of the BEAM Compiler</a></h3><p><em>Monday, 18 June 2018<!-- --> - <!-- -->Björn Gustavsson</em></p><p>This blog post is a brief history lesson about the Erlang compiler for
the BEAM machine. To provide some context, there will first be a quick
look at the abstract machines for Erlang.</p><h3><a href="/blog/Interpreter-Optimizations/">Interpreter optimization</a></h3><p><em>Monday, 11 June 2018<!-- --> - <!-- -->Lukas Larsson</em></p><p>The BEAM [interpreter] in erts has been completely re-written in OTP 21.
Most of the instructions have remained the same, but the perl scripts used
to generate the C code have a new implementation. This blog post will look at
some of the optimizations that were possible because of those changes.</p><h3><a href="/blog/core-erlang-wrapup/">Core Erlang Wrap Up</a></h3><p><em>Wednesday, 30 May 2018<!-- --> - <!-- -->Björn Gustavsson</em></p><p>This blog post wraps up the exploration of Core Erlang started in the
previous two blog posts. The remaining default Core Erlang
passes are described, followed by a look at how Core Erlang is
represented internally in the compiler.</p><h3><a href="/blog/core-erlang-optimizations/">Core Erlang Optimizations</a></h3><p><em>Friday, 18 May 2018<!-- --> - <!-- -->Björn Gustavsson</em></p><p>This blog post continues the exploration of Core Erlang by
looking at some optimizations done by the <code>sys_core_fold</code>
compiler pass. The Core Erlang language was introduced in
the <a href="http://blog.erlang.org/core-erlang-by-example/">previous blog post</a>.</p><h3><a href="/blog/core-erlang-by-example/">Core Erlang by Example</a></h3><p><em>Monday, 7 May 2018<!-- --> - <!-- -->Björn Gustavsson</em></p><p>This blog post is the first about the Core Erlang format. In this
blog post, we introduce the Core Erlang format through examples
that compare Erlang code to the corresponding Core Erlang
code.</p><h3><a href="/blog/Memory-instrumentation-in-OTP-21/">Memory instrumentation in OTP 21</a></h3><p><em>Wednesday, 2 May 2018<!-- --> - <!-- -->John Högberg</em></p><p>The memory instrumentation module was rewritten for Erlang/OTP 21 to make it
easier to use. In this post I&#x27;ll describe the rationale behind the new features
and how to make use of them.</p><h3><a href="/blog/My-OTP-21-Highlights/">My OTP 21 Highlights</a></h3><p><em>Wednesday, 2 May 2018<!-- --> - <!-- -->Lukas Larsson</em></p><p>OTP-21 Release Candidate 1 has just been released. I thought that I would go
through the changes that I am the most excited about. Most likely this will
mostly mean features in erts and the core libraries as those are the
changes that I am the most familiar with.</p><h3><a href="/blog/compiler-lost-in-translation/">Lost in Translation (Exploring the Compiler&#x27;s Front End)</a></h3><p><em>Thursday, 26 April 2018<!-- --> - <!-- -->Björn Gustavsson</em></p><p>In this blog post, we will explore the compiler passes that make up
the compiler&#x27;s front end.</p><h3><a href="/blog/compiler-time-option/">Exploring the Compiler Using the &#x27;time&#x27; Option</a></h3><p><em>Thursday, 19 April 2018<!-- --> - <!-- -->Björn Gustavsson</em></p><p>This is the first of a series of blog posts about the compiler.  There
will be blog posts about how the compiler works now, how it might work
in the future, and some historical notes to explain why some things
are what they are. In this blog post I will talk about one of the most
useful options for exploring the compiler, namely the <code>time</code> option.</p><h3><a href="/blog/IO-Polling/">I/O polling options in OTP 21</a></h3><p><em>Wednesday, 11 April 2018<!-- --> - <!-- -->Lukas Larsson</em></p><p>Erlang/OTP 21 will introduce a completely new IO polling implementation.
This new implementation comes with a new set of tuneable parameters that
can be used to get the most out of your system. This blog post describes
the parameters and attempts to describe what they should be used for.</p></div></div><div class="col-lg-2"><p><a href="/rss/blog/"><img src="/img/rss-icon.png" width="64"/></a></p></div></div></div><div class="container"><div class="row"><div class="col-lg-12"><div class="divider"><p></p></div></div><div class="col-lg-12 text-center"><div class="col-lg-4"><a title="DOWNLOAD" href="/download.html"><img src="/img/download.png"/></a></div><div class="col-lg-4"><a href="http://www.github.com/erlang/otp/"><img src="/img/GitHub-Mark-32px.png"/></a></div><div class="col-lg-4"><a href="http://www.twitter.com/erlang_org/"><img src="/img/twitter.png" width="32"/></a></div></div><div class="col-lg-12"><div class="divider"><p></p></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"blogs":[{"id":"a-first-look-at-the-jit","title":"A first look at the JIT","author":"John Högberg","excerpt":"\nNow that we've had a look at [BEAM] and the [interpreter] we're going to\nexplore one of the most exciting additions in OTP 24: the just-in-time\ncompiler, or \"JIT\" for short.","article_date":1604361600000,"tags":["BEAM erts jit"],"frontmatter":{"layout":"post","title":"A first look at the JIT","tags":"BEAM erts jit","author":"John Högberg"},"content":"\nNow that we've had a look at [BEAM] and the [interpreter] we're going to\nexplore one of the most exciting additions in OTP 24: the just-in-time\ncompiler, or \"JIT\" for short.\n\nIf you're like me the word \"JIT\" probably makes you think of Hotspot (Java) or\nV8 (Javascript). These are very impressive pieces of engineering but they seem\nto have hijacked the term; not all JITs are that sophisticated, nor do they\nhave to be in order to be fast.\n\nWe've made many attempts at a JIT over the years that aimed for the stars only\nto fall down. Our latest and by far most successful attempt went for simplicity\ninstead, trading slight inefficiencies in the generated code for ease of\nimplementation. If we exclude the run-time assembler library we use, [asmjit],\nthe entire thing is roughly as big as the interpreter.\n\nI believe much of our success can be attributed to four ideas we had early in\nthe project:\n\n1. **All modules are always compiled to machine code.**\n\n   Previous attempts (and HiPE too) had a difficult time switching between the\n   interpreter and machine code: it was either too slow, too difficult to\n   maintain, or both.\n\n   Always running machine code means we never have to switch.\n\n2. **Data may only be kept (passed) in BEAM registers between instructions.**\n\n   This may seem silly, aren't machine registers faster?\n\n   Yes, but in practice not by much and it would make things more complicated.\n   By always passing data in BEAM registers we can use the register allocation\n   given to us by the Erlang compiler, saving us from having to do this very\n   expensive step at runtime.\n\n   More importantly, this minimizes the difference between the interpreter and\n   the JIT from the runtime system's point of view.\n\n3. **Modules are compiled one instruction at a time.**\n\n   One of the most difficult problems in our prior attempts was to strike a\n   good balance between the time it took to compile something and the eagerness\n   to do so. If we're too eager, we'll spend too much time compiling, and if\n   we're too lax we won't see any improvements.\n\n   This problem was largely self-inflicted and caused by the compiler being too\n   slow (we often used LLVM), which was made worse by us giving it large pieces\n   of code to allow more optimizations.\n\n   By limiting ourselves to compiling one instruction at a time, we leave some\n   performance on the table but greatly improve compilation speed.\n\n4. **Every instruction has a handwritten machine code template.**\n\n   This makes compilation _extremely_ fast as we basically just copy-paste\n   the template every time the instruction is used, only performing some minor\n   tweaks depending on its arguments.\n\n   This may seem daunting at first but it's actually not that bad once you get\n   used to it. While it certainly takes a lot of code to achieve even the\n   smallest of things, it's inherently simple and easy to follow as long as\n   the code is kept short.\n\n   The downside is that every instruction needs to be implemented for each\n   architecture, but luckily there's not a lot of popular ones and we hope to\n   support the two most common ones by the time we release OTP 24: `x86_64`\n   and `AArch64`. The others will continue to use the interpreter.\n\nWhen compiling a module the JIT goes through the instructions one by one,\ninvoking machine code templates as it goes. This has two very large benefits\nover the interpreter: there's no need to jump between them because they're\nemitted back-to-back and the end of each is the start of the next one, and the\narguments don't need to be resolved at runtime because they're already \"burnt\nin.\"\n\nNow that we have some background, let's look at the machine code template for\nour example in the previous post, `is_nonempty_list`:\n\n```c++\n/* Arguments are passed as `ArgVal` objects which hold a\n * type and a value, for example saying \"X register 4\",\n * \"the atom 'hello'\", \"label 57\" and so on. */\nvoid BeamModuleAssembler::emit_is_nonempty_list(const ArgVal \u0026Fail,\n                                                const ArgVal \u0026Src) {\n    /* Figure out which memory address `Src` lives in. */\n    x86:Mem list_ptr = getArgRef(Src);\n\n    /* Emit a `test` instruction, which does a non-\n     * destructive AND on the memory pointed at by\n     * list_ptr, clearing the zero flag if the list is\n     * empty. */\n    a.test(list_ptr, imm(_TAG_PRIMARY_MASK - TAG_PRIMARY_LIST));\n\n    /* Emit a `jnz` instruction, jumping to the fail label\n     * if the zero flag is clear (the list is empty). */\n    a.jnz(labels[Fail.getValue()]);\n\n    /* Unlike the interpreter there's no need to jump to\n     * the next instruction on success as it immediately\n     * follows this one. */\n}\n```\n\nThis template will generate code that looks almost identical to the template\nitself. Let's say our source is \"`X` register 1\" and our fail label is 57:\n\n```\ntest qword ptr [rbx+8], _TAG_PRIMARY_MASK - TAG_PRIMARY_LIST\njnz label_57\n```\n\nThis is much faster than the interpreter, and even a bit more compact than the\nthreaded code, but this is a trivial instruction. What about more complex\nones? Let's have a look at the `timeout` instruction in the interpreter:\n\n```c++\ntimeout() {\n    if (IS_TRACED_FL(c_p, F_TRACE_RECEIVE)) {\n        trace_receive(c_p, am_clock_service, am_timeout, NULL);\n    }\n    if (ERTS_PROC_GET_SAVED_CALLS_BUF(c_p)) {\n        save_calls(c_p, \u0026exp_timeout);\n    }\n    c_p-\u003eflags \u0026= ~F_TIMO;\n    JOIN_MESSAGE(c_p);\n}\n```\n\nThat's bound to be a lot of code, and those macros will be really annoying to\nconvert by hand. How on earth are we going to do this without losing our minds?\n\nBy cheating, that's how :D\n\n```c++\nstatic void timeout(Process *c_p) {\n    if (IS_TRACED_FL(c_p, F_TRACE_RECEIVE)) {\n        trace_receive(c_p, am_clock_service, am_timeout, NULL);\n    }\n    if (ERTS_PROC_GET_SAVED_CALLS_BUF(c_p)) {\n        save_calls(c_p, \u0026exp_timeout);\n    }\n    c_p-\u003eflags \u0026= ~F_TIMO;\n    JOIN_MESSAGE(c_p);\n}\n\nvoid BeamModuleAssembler::emit_timeout() {\n    /* Set the first C argument to our currently executing\n     * process, c_p, and then call the above C function. */\n    a.mov(ARG1, c_p);\n    a.call(imm(timeout));\n}\n```\n\nThis little escape hatch saved us from having to write everything in assembler\nfrom the start, and many instructions remain like this because there hasn't\nbeen any point to changing them.\n\nThat's all for today. In the next post we'll walk through our conventions and\nsome of the techniques we've used to reduce the code size.\n\n[asmjit]: https://asmjit.com/\n[BEAM]: http://blog.erlang.org/a-brief-BEAM-primer/\n[interpreter]: http://blog.erlang.org/a-closer-look-at-the-interpreter/\n"},{"id":"a-closer-look-at-the-interpreter","title":"A closer look at the interpreter","author":"John Högberg","excerpt":"\nIn my [previous post] we had a look at BEAM, and now that we're more familiar\nwith it it's time for us to look at the reference implementation: the\ninterpreter.","article_date":1603756800000,"tags":["BEAM erts jit"],"frontmatter":{"layout":"post","title":"A closer look at the interpreter","tags":"BEAM erts jit","author":"John Högberg"},"content":"\nIn my [previous post] we had a look at BEAM, and now that we're more familiar\nwith it it's time for us to look at the reference implementation: the\ninterpreter.\n\nThe interpreter can be thought of as an endless loop that looks at the current\ninstruction, executes it, and then moves on to the next one.\n\nIn normal builds our code is laid out as [directly threaded code], where each\ninstruction consists of the _machine code address_ of its handler, followed by\nits arguments, which are in turn followed by the next instruction:\n\n    Instruction address\n        First argument\n        Second argument\n        ... and so on.\n    Instruction address\n        First argument\n        Second argument\n        ... and so on.\n\nWhen an instruction finishes it reads the next instruction address and jumps\nto it, forever following the \"thread\" of instructions.\n\nWith very few exceptions, the instructions are \"pure\" in the sense that they\nalways do the same thing with the same input and that they only affect BEAM,\neither through changing the control flow or writing a result to a register.\nThis makes the instructions very easy to read and reason about in isolation, so\nfor the sake of brevity we'll only have a look at one of them:\n\n```c++\nis_nonempty_list(Label, Src) {\n\n    /* Check if our $Src is not a list. */\n    if (is_not_list($Src)) {\n\n        /* Invoke the $FAIL macro, jumping to our\n         * $Label. */\n        $FAIL($Label);\n    }\n\n    /* Execute the next instruction. */\n}\n```\n\nThe above is written in a domain-specific language (DSL) that is a superset of\nC, where `$`-prefixed macros are expanded and the rest is kept as is.\n\nThe [`beam_makeops`] script takes this definition and generates code for\nthe parts that are cumbersome to write by hand, such as jumping to the next\ninstruction. It also hides argument handling which allows us to reduce the code\nsize by packing small arguments together behind the scenes, which we've\nexplored in an [earlier post] on the subject.\n\nFor performance reasons it also generates different variants based on the\nargument types outlined in [`ops.tab`] to reduce the amount of work we need to\ndo at runtime.\n\nLet's have a look at the generated code for this instruction, specifically the\nvariant for `X` registers:\n\n```c++\n/* (This has been modified slightly for readability) */\ncase is_nonempty_list_fx:\n{\n    Eterm arg_word, term;\n\n    /* Read the argument word from the instruction\n     * stream. */\n    arg_word = I[1];\n\n    /* Unpack the offset of our source register (upper\n     * 32 bits) and then read its contents.\n     *\n     * Note that we address the X registers directly;\n     * had this instruction not been specialized, we\n     * would first need to determine whether the\n     * argument was an X or a Y register. */\n    term = x_registers[arg_word \u003e\u003e 32];\n\n    /* is_not_list(term) */\n    if (term \u0026 (_TAG_PRIMARY_MASK - TAG_PRIMARY_LIST)) {\n\n        /* Unpack our fail label (lower 32 bits) and add\n         * it to our current instruction pointer. This\n         * is an offset and may be negative for backward\n         * jumps. */\n        I += (Sint32)arg_word;\n\n        /* Jump to the instruction at the fail label using\n         * the \"labels as values\" GCC extension. */\n        goto *I;\n    }\n\n    /* Skip the current label address and argument word,\n     * then jump to the next instruction. This was\n     * automatically generated by beam_makeops. */\n    I += 2;\n    goto *I;\n}\n```\n\nThere's a bit more to it, and those who would like to know more can read the\n[documentation for the `beam_makeops` script], but the above covers the gist of\nit.\n\nWhile the above is quite efficient, there's significant overhead from dispatch\nand argument handling. Here's a slightly altered assembly listing of the\nabove:\n\n\u003cpre class=\"highlight\"\u003e\n    \u003cem\u003e; Read the argument word from the instruction\n    ; stream.\u003c/em\u003e\n    mov    rdx, [rbx + 8]\n\n    \u003cem\u003e; Unpack the offset of our source register (upper 32\n    ; 32 bits).\u003c/em\u003e\n    mov    rcx, rdx\n    shr    rcx, 32\n\n    \u003cem\u003e; X registers live in machine register r15, and we\n    ; placed our offset in rcx, so we can find our term at\n    ; [r15 + rcx].\n    ;\n    ; Perform a non-destructive bitwise AND on the term\n    ; using the `test` instruction, and jump to the fail\n    ; label if the result is non-zero.\u003c/em\u003e\n    \u003cb\u003etest   byte [r15 + rcx], (_TAG_PRIMARY_MASK - TAG_PRIMARY_LIST)\n    jne    jump_to_fail_label\u003c/b\u003e\n\n    \u003cem\u003e; Skip the current label address and argument word,\n    ; then jump to the next instruction.\u003c/em\u003e\n    add    rbx, 16\n    jmp    [rbx]\n\njump_to_fail_label:\n    \u003cem\u003e; Unpack our fail label (lower 32 bits) and add\n    ; it to our current instruction pointer.\u003c/em\u003e\n    movsxd rdx, edx\n    lea    rbx, [rbx + rdx * 8]\n\n    \u003cem\u003e; Jump to the instruction at the fail label.\u003c/em\u003e\n    jmp    [rbx]\n\u003c/pre\u003e\n\nThe bold section is the meat of the instruction and the rest is argument\nunpacking and instruction dispatch. While this is not much of a problem for\nlarge instructions, its effect on short ones like this is very large, and when\nlooking through a profiler (e.g. [`perf`]) it's not unusual for the final `jmp`\nto dominate the rest.\n\nTo lessen this effect, the loader combines commonly used instruction sequences\ninto a single instruction. For example, two independent moves may fuse into\n[`move2_par`], and `is_nonempty_list` followed by `get_list` might be fused to\n[`is_nonempty_list_get_list`].\n\nThis reduces the cost of short instructions, but only works to the extent we're\nable to identify common patterns and comes at a significant maintenance cost\nas each combination must be implemented manually. Even so, the effect tends to\nbe moderate and the dispatch overhead remains significant.\n\nAnother, albeit lesser, downside with the interpreter is that modern processors\nare very optimized for patterns commonly found in \"ordinary\" native code. For\nexample, nearly all of them have a special branch predictor just for calls and\nreturns. Assuming that every call has a corresponding return lets it predict\nreturns _perfectly_ unless an exception is thrown, but since the interpreter\ndoes not use native calls and returns, it cannot make use of this optimization.\n\nUnfortunately there's not a whole lot that can be done about this, and after\nover two decades of refinement it's becoming increasingly difficult to optimize\nit in meaningful ways.\n\nBecause of that our quest to improve performance has instead focused on two\nareas: improving the compiler and implementing a JIT. We've made great strides\nwith both as of late, and are very proud to have finally merged the latter.\n\nStay tuned for our next post, where we'll have a look at the JIT and see how it\navoids these issues.\n\n[directly threaded code]: https://en.wikipedia.org/wiki/Threaded_code\n[`ops.tab`]: https://github.com/erlang/otp/blob/OTP-23.1/erts/emulator/beam/ops.tab\n[`beam_makeops`]: https://github.com/erlang/otp/blob/OTP-23.1/erts/emulator/utils/beam_makeops\n[documentation for the `beam_makeops` script]: http://erlang.org/doc/apps/erts/beam_makeops.html\n[`move2_par`]: https://github.com/erlang/otp/blob/OTP-23.1/erts/emulator/beam/instrs.tab#L577\n[`is_nonempty_list_get_list`]: https://github.com/erlang/otp/blob/OTP-23.1/erts/emulator/beam/instrs.tab#L795\n[`perf`]: https://en.wikipedia.org/wiki/Perf_%28Linux%29\n[earlier post]: http://blog.erlang.org/Interpreter-Optimizations/\n[previous post]: http://blog.erlang.org/a-brief-BEAM-primer/\n"},{"id":"a-brief-BEAM-primer","title":"A brief introduction to BEAM","author":"John Högberg","excerpt":"\nThis post is a brief primer on BEAM, the virtual machine that executes user\ncode in the Erlang Runtime System (ERTS). It's intended to help those new to\nBEAM follow an upcoming series of posts about the JIT in OTP 24, leaving\nimplementation details for later.","article_date":1603152000000,"tags":["BEAM compiler erts"],"frontmatter":{"layout":"post","title":"A brief introduction to BEAM","tags":"BEAM compiler erts","author":"John Högberg"},"content":"\nThis post is a brief primer on BEAM, the virtual machine that executes user\ncode in the Erlang Runtime System (ERTS). It's intended to help those new to\nBEAM follow an upcoming series of posts about the JIT in OTP 24, leaving\nimplementation details for later.\n\nBEAM is often confused with ERTS and it's important to distinguish between the\ntwo; BEAM is just the virtual machine and it has no notion of processes, ports,\nETS tables, and so on. It merely executes instructions and while ERTS has\ninfluenced their design, it doesn't affect what they do when the code is\nrunning, so you don't need to understand ERTS to understand BEAM.\n\nBEAM is a register machine, where all instructions operate on named registers.\nEach register can contain any Erlang term such as an integer or a tuple, and it\nhelps to think of them as simple variables. The two most important kinds of\nregisters are:\n\n- `X`: these are used for temporary data and passing data between functions.\n  They don't require a stack frame and can be freely used in any function, but\n  there are certain limitations which we'll expand on later.\n- `Y`: these are local to each stack frame and have no special\n  limitations beyond needing a stack frame.\n\nControl flow is handled by instructions that test a certain condition and\neither move on to the next instruction or branch to its _fail label_, noted by\n`{f,Index}`. For example `{test,is_integer,{f,7},[{x,0}]}.` checks if `{x,0}`\ncontains an integer and jumps to label 7 if it doesn't.\n\nFunction arguments are passed from left to right in `X` registers, starting at\n`{x,0}`, and the result is returned in `{x,0}`.\n\nIt's easier to explain how this fits together through example, so let's walk\nthrough a few:\n\n```erlang\nsum_tail(List) -\u003e\n    sum_tail(List, 0).\n\nsum_tail([Head | Tail], Acc) -\u003e\n    sum_tail(Tail, Head + Acc);\nsum_tail([], Acc) -\u003e\n    Acc.\n```\n\nLet's use `erlc -S` to look at the instructions one by one:\n\n\u003cpre class=\"highlight\"\u003e\n\u003cem\u003e%% sum_tail/1, entry label is 2\u003c/em\u003e\n{function, sum_tail, 1, 2}.\n\n  \u003cem\u003e%% Marks a jump target with the label 1.\u003c/em\u003e\n  {label,1}.\n\n    \u003cem\u003e%% Special instruction that raises a function_clause\n    %% exception. Unused in this function.\u003c/em\u003e\n    {func_info,{atom,primer},{atom,sum_tail},1}.\n\n  {label,2}.\n    \u003cem\u003e%% The meat of the function starts here.\n    %%\n    %% Our only argument - \u003cb\u003eList\u003c/b\u003e - is in \u003cb\u003e{x,0}\u003c/b\u003e and\n    %% since sum_tail/2 expects it to be the first\n    %% argument we can leave it be. We'll pass the\n    %% integer 0 as the second argument in \u003cb\u003e{x,1}\u003c/b\u003e.\u003c/em\u003e\n    {move,{integer,0},{x,1}}.\n\n    \u003cem\u003e%% Tail call sum_tail/2, whose entry label is 4.\u003c/em\u003e\n    {call_only,2,{f,4}}.\n\n\u003cem\u003e%% sum_tail/2, entry label is 4\u003c/em\u003e\n{function, sum_tail, 2, 4}.\n  {label,3}.\n    {func_info,{atom,primer},{atom,sum_tail},2}.\n  {label,4}.\n\n    \u003cem\u003e%% Test whether we have a non-empty list, and jump to\n    %% the base case at label 5 if we don't.\u003c/em\u003e\n    {test,is_nonempty_list,{f,5},[{x,0}]}.\n\n    \u003cem\u003e%% Unpack the list in the first argument, placing the\n    %% head in \u003cb\u003e{x,2}\u003c/b\u003e and the tail in \u003cb\u003e{x,0}\u003c/b\u003e.\u003c/em\u003e\n    {get_list,{x,0},{x,2},{x,0}}.\n\n    \u003cem\u003e%% Add the head and our accumulator (remember that the\n    %% second function argument is in \u003cb\u003e{x,1}\u003c/b\u003e), and place\n    %% the result in \u003cb\u003e{x,1}\u003c/b\u003e.\n    %%\n    %% A fail label of 0 means that we want the\n    %% instruction to throw an exception on error, rather\n    %% than jump to a given label.\u003c/em\u003e\n    {gc_bif,'+',{f,0},3,[{x,2},{x,1}],{x,1}}.\n\n    \u003cem\u003e%% Tail-call ourselves to handle the rest of the list,\n    %% the arguments are already in the right registers.\u003c/em\u003e\n    {call_only,2,{f,4}}.\n\n  {label,5}.\n    \u003cem\u003e%% Test whether our argument was the empty list. If\n    %% not, we jump to label 3 to raise a function_clause\n    %% exception.\u003c/em\u003e\n    {test,is_nil,{f,3},[{x,0}]}.\n\n    \u003cem\u003e%% Return our accumulator.\u003c/em\u003e\n    {move,{x,1},{x,0}}.\n    return.\n\u003c/pre\u003e\n\nSimple enough, isn't it?\n\nI glossed over one little detail though; the mysterious number `3` in the\naddition instruction. This number tells us how many `X` registers hold live\ndata in case we need more memory, so they can be preserved while the rest are\ndiscarded as garbage. As a consequence, it's unsafe to refer to higher `X`\nregisters after this instruction as their contents may be invalid (in this case\n`{x,3}` and above).\n\nFunction calls are similar; we may schedule ourselves out whenever we call or\nreturn from a function, and we'll only preserve the function arguments/return\nvalue when we do so. This means that all `X` registers except for `{x,0}` are\ninvalid after a call even if you knew for certain that the called function\ndidn't touch a certain register.\n\nThis is where `Y` registers enter the picture. Let's take the previous example\nand make it body-recursive instead:\n\n```erlang\nsum_body([Head | Tail]) -\u003e\n    Head + sum_body(Tail);\nsum_body([]) -\u003e\n    0.\n```\n.\n\u003cpre class=\"highlight\"\u003e\n{function, sum_body, 1, 7}.\n  {label,6}.\n    {func_info,{atom,primer},{atom,sum_body},1}.\n  {label,7}.\n    {test,is_nonempty_list,{f,8},[{x,0}]}.\n\n    \u003cem\u003e%% Allocate a stack frame with a single Y register.\n    %% Since this instruction may need more memory, we\n    %% tell the garbage collector that we currently have\n    %% one live X register (our list argument in \u003cb\u003e{x,0}\u003c/b\u003e).\u003c/em\u003e\n    {allocate,1,1}.\n\n    \u003cem\u003e%% Unpack the list, placing the head in \u003cb\u003e{y,0}\u003c/b\u003e and\n    %% the tail in \u003cb\u003e{x,0}\u003c/b\u003e.\u003c/em\u003e\n    {get_list,{x,0},{y,0},{x,0}}.\n\n    \u003cem\u003e%% Body-call ourselves. Note that while this kills all\n    %% X registers, it leaves Y registers alone so our\n    %% head is still valid.\u003c/em\u003e\n    {call,1,{f,7}}.\n\n    \u003cem\u003e%% Add the head to our return value and store the\n    %% result in \u003cb\u003e{x,0}\u003c/b\u003e.\u003c/em\u003e\n    {gc_bif,'+',{f,0},1,[{y,0},{x,0}],{x,0}}.\n\n    \u003cem\u003e%% Deallocate our stack frame and return.\u003c/em\u003e\n    {deallocate,1}.\n    return.\n\n  {label,8}.\n    {test,is_nil,{f,6},[{x,0}]}.\n\n    \u003cem\u003e%% Return the integer 0.\u003c/em\u003e\n    {move,{integer,0},{x,0}}.\n    return.\n\u003c/pre\u003e\n\nNotice how the call instruction changed now that we're in a stack frame? There\nare three different call instructions:\n\n- `call`: ordinary call as in the example. Control flow will resume at\n  the next instruction when the called function returns.\n- `call_last`: tail call when there is a stack frame. The current frame will\n  be deallocated before the call.\n- `call_only`: tail call when there is no stack frame.\n\nEach of these have a variant for calling functions in other modules (e.g.\n`call_ext`), but they're otherwise identical.\n\nSo far we've only looked at using terms, but what about creating them? Let's\nhave a look:\n\n```erlang\ncreate_tuple(Term) -\u003e\n    {hello, Term}.\n```\n.\n\u003cpre class=\"highlight\"\u003e\n{function, create_tuple, 1, 10}.\n  {label,9}.\n    {func_info,{atom,primer},{atom,create_tuple},1}.\n  {label,10}.\n    \u003cem\u003e%% Allocate the three words needed for a 2-tuple, with\n    %% a liveness annotation of 1 indicating that \u003cb\u003e{x,0}\u003c/b\u003e\n    %% is alive in case we need to GC.\u003c/em\u003e\n    {test_heap,3,1}.\n\n    \u003cem\u003e%% Create the tuple and place the result in \u003cb\u003e{x,0}\u003c/b\u003e\u003c/em\u003e\n    {put_tuple2,{x,0},{list,[{atom,hello},{x,0}]}}.\n  \n    return.\n\u003c/pre\u003e\n\nThis is a bit magical in the sense that there's an unseen register for memory\nallocations, but allocation is rarely far apart from use and it's usually\npretty easy to follow. The same principle applies for lists ([consing]),\nfloats, and funs as well following [PR 2765].\n\nMore complicated types like maps, big integers, references, and so on are\ncreated by special instructions that may GC on their own (or allocate outside\nthe heap in a \"heap fragment\") as their size can't be statically determined in\nadvance.\n\nNow let's look at something more uncommon: exceptions.\n\n```erlang\nexception() -\u003e\n    try\n        external:call()\n    catch\n        throw:example -\u003e hello\n    end.\n```\n.\n\u003cpre class=\"highlight\"\u003e\n{function, exception, 0, 12}.\n  {label,11}.\n    {func_info,{atom,primer},{atom,exception},0}.\n  {label,12}.\n    {allocate,1,0}.\n  \n    \u003cem\u003e%% Place a catch tag in \u003cb\u003e{y,0}\u003c/b\u003e. If an exception is\n    %% raised while this tag is the most current one,\n    %% the control flow will resume at \u003cb\u003e{f,13}\u003c/b\u003e in this\n    %% stack frame.\u003c/em\u003e\n    {'try',{y,0},{f,13}}.\n\n    {call_ext,0,{extfunc,external,call,0}}.\n\n    \u003cem\u003e%% Deactivate the catch tag before returning with the\n    %% result from the call.\u003c/em\u003e\n    {try_end,{y,0}}.\n\n    {deallocate,1}.\n    return.\n\n  {label,13}.\n    \u003cem\u003e%% Uh oh, we've got an exception. Kill the catch tag\n    %% and place the exception class in \u003cb\u003e{x,0}\u003c/b\u003e, the error\n    %% reason/thrown value in \u003cb\u003e{x,1}\u003c/b\u003e, and the stack trace\n    %% in \u003cb\u003e{x,2}\u003c/b\u003e.\u003c/em\u003e\n    {try_case,{y,0}}.\n\n    \u003cem\u003e%% Return 'hello' if the user threw 'example'\u003c/em\u003e\n    {test,is_eq_exact,{f,14},[{x,0},{atom,throw}]}.\n    {test,is_eq_exact,{f,14},[{x,1},{atom,example}]}.\n    {move,{atom,hello},{x,0}}.\n    {deallocate,1}.\n    return.\n\n  {label,14}.\n    \u003cem\u003e%% Otherwise, rethrow the exception since no catch\n    %% clause matched.\u003c/em\u003e\n    {bif,raise,{f,0},[{x,2},{x,1}],{x,0}}.\n\u003c/pre\u003e\n\nBy now you've probably noticed how the control flow only moves forward; just\nlike Erlang itself the only way to loop is through recursion. The one exception\nto this is the receive construct, which may loop until a matching message has\nbeen received:\n\n```erlang\nselective_receive(Ref) -\u003e\n    receive\n        {Ref, Result} -\u003e Result\n    end.\n```\n.\n\u003cpre class=\"highlight\"\u003e\n{function, selective_receive, 1, 16}.\n  {label,15}.\n    {func_info,{atom,primer},{atom,selective_receive},1}.\n  {label,16}.\n    {allocate,1,1}.\n\n    \u003cem\u003e%% We may be scheduled out while waiting for a\n    %% message, so we'll preserve our \u003cb\u003eRef\u003c/b\u003e in \u003cb\u003e{y,0}\u003c/b\u003e.\u003c/em\u003e\n    {move,{x,0},{y,0}}.\n\n  {label,17}.\n    \u003cem\u003e%% Pick the next message from the process' message box\n    %% and place it in \u003cb\u003e{x,0}\u003c/b\u003e, jumping to label 19 if the\n    %% message box is empty.\u003c/em\u003e\n    {loop_rec,{f,19},{x,0}}.\n  \n    \u003cem\u003e%% Does it match our pattern? If not, jump to label 18\n    %% and try the next message.\u003c/em\u003e\n    {test,is_tuple,{f,18},[{x,0}]}.\n    {test,test_arity,{f,18},[{x,0},2]}.\n    {get_tuple_element,{x,0},0,{x,1}}.\n    {test,is_eq_exact,{f,18},[{x,1},{y,0}]}.\n\n    \u003cem\u003e%% We've got a match, extract the result and remove\n    %% the message from the mailbox.\u003c/em\u003e\n    {get_tuple_element,{x,0},1,{x,0}}.\n    remove_message.\n    {deallocate,1}.\n    return.\n\n  {label,18}.\n    \u003cem\u003e%% The message didn't match, loop back to handle our\n    %% next message. Note that the current message remains\n    %% in the inbox since a different receive may be\n    %% interested in it.\u003c/em\u003e\n    {loop_rec_end,{f,17}}.\n\n  {label,19}.\n    \u003cem\u003e%% Wait until the next message arrives, returning to\n    %% the start of the loop when it does. If there's a\n    %% timeout involved, it will be handled here.\u003c/em\u003e\n    {wait,{f,17}}.\n\u003c/pre\u003e\n\nThere's not much more to it, and if you feel comfortable following the examples\nabove you should have no problems with the JIT series.\n\nIf you're curious about which instructions there are, you can find a brief\ndescription of every instruction in [genop.tab].\n\n[genop.tab]: https://github.com/erlang/otp/blob/master/lib/compiler/src/genop.tab\n[consing]: https://en.wikipedia.org/wiki/Cons\n[PR 2765]: https://github.com/erlang/otp/pull/2765\n"},{"id":"the-new-scalable-ets-ordered_set","title":"The New Scalable ETS ordered_set","author":"Kjell Winblad","excerpt":"\nThe scalability of ETS tables of type `ordered_set` with the\n`write_concurrency` option is substantially better in Erlang/OTP 22\nthan earlier releases. In some extreme cases, you can expect\nmore than 100 times better throughput in Erlang/OTP 22 compared to\nErlang/OTP 21. The cause of this improvement is a new data structure\ncalled [the contention adapting search tree][jpdc_ca_tree] (CA tree\nfor short). This blog post will give you insights into how the CA tree\nworks and show you benchmark results comparing the performance of ETS\n`ordered_set` tables in OTP 21 and OTP 22.","article_date":1597795200000,"tags":["ETS ordered_set scalability CA tree"],"frontmatter":{"layout":"post","title":"The New Scalable ETS ordered_set","tags":"ETS ordered_set scalability CA tree","author":"Kjell Winblad"},"content":"\nThe scalability of ETS tables of type `ordered_set` with the\n`write_concurrency` option is substantially better in Erlang/OTP 22\nthan earlier releases. In some extreme cases, you can expect\nmore than 100 times better throughput in Erlang/OTP 22 compared to\nErlang/OTP 21. The cause of this improvement is a new data structure\ncalled [the contention adapting search tree][jpdc_ca_tree] (CA tree\nfor short). This blog post will give you insights into how the CA tree\nworks and show you benchmark results comparing the performance of ETS\n`ordered_set` tables in OTP 21 and OTP 22.\n\n## Try it Out!\n\n[This escript](/code/insert_disjoint_ranges.erl) makes it convenient for you\nto try the new `ordered_set` implementation on your own machine with\nErlang/OTP 22+ installed.\n\nThe escript measures the time it takes for `P` Erlang processes to\ninsert `N` integers into an `ordered_set` ETS table, where `P` and `N`\nare parameters to the escript. The CA tree is only utilized when the\nETS table options `ordered_set` and `{write_concurrency, true}` are\nactive. One can, therefore, easily compare the new data structure's\nperformance with the old one (an [AVL tree][AVLTree] protected by a\nsingle readers-writer lock). The `write_concurrency` option had no\neffect on `ordered_set` tables before the release of Erlang/OTP 22.\n\nWe get the following results when running the escript on a developer laptop with\ntwo cores (Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz):\n\n{% highlight console %}\n\n$ escript insert_disjoint_ranges.erl old 1 10000000\nTime: 3.352332 seconds\n$ escript insert_disjoint_ranges.erl old 2 10000000\nTime: 3.961732 seconds\n$ escript insert_disjoint_ranges.erl old 4 10000000\nTime: 6.382199 seconds\n$ escript insert_disjoint_ranges.erl new 1 10000000\nTime: 3.832119 seconds\n$ escript insert_disjoint_ranges.erl new 2 10000000\nTime: 2.109476 seconds\n$ escript insert_disjoint_ranges.erl new 4 10000000\nTime: 1.66509 seconds\n\n{% endhighlight %}\n\nWe see that in this particular benchmark, the CA tree has superior\nscalability to the old data structure. The benchmark ran about twice\nas fast with the new data structure and four processes as with the old\ndata structure and one process (the machine only has two\ncores). We will look at the performance and scalability of the new CA\ntree-based implementation in greater detail later after describing how\nthe CA tree works.\n\n## The Contention Adapting Search Tree in a Nutshell\n\nThe key feature that distinguishes the CA tree from other concurrent\ndata structures is that the CA tree dynamically changes its\nsynchronization granularity based on how much contention is detected\ninside the data structure. This way, the CA tree can avoid the\nperformance and memory overheads that come from using many unnecessary\nlocks without sacrificing performance when many operations happen in\nparallel. For example, let us imagine a scenario where the CA tree is\ninitially populated from many threads in parallel, and then it is only\nused from a single thread. In this scenario, the CA tree will adapt to\nuse fine-grained synchronization in the population phase (when\nfine-grained synchronization reduces contention). The CA tree will then change\nto use coarse-grained synchronization in the single-threaded phase\n(when coarse-grained synchronization reduces the locking and memory\noverheads).\n\nThe structure of a CA tree is illustrated in the following\npicture:\n\n![alt text](/images/ca_tree/ca_tree_9.png \"Contention Adapting Search Tree Structure\")\n\nThe actual items stored in the CA tree are located in\nsequential data structures in the bottom layer. These\nsequential data structures are protected by the locks in the base\nnodes in the middle layer. The base node locks have counters\nassociated with them. The counter of a base node lock is increased when\ncontention is detected in the base node lock and decreased when no\nsuch contention is detected. The value of this base node lock counter\ndecides if a split or a join should happen after an operation has been\nperformed in a base node. The routing nodes at the top of the picture\nabove form a binary search tree that directs the search for a\nparticular item. A routing node also contains a lock and a flag. These\nare used when joining base nodes. The details of how splitting and\njoining work will not be described in this article, but\nthe interested reader can find a detailed description in this [CA tree\npaper][jpdc_ca_tree] ([preprint PDF][jpdc_ca_tree_preprint]). We will now\nillustrate how the CA tree changes its synchronization granularity by\ngoing through an example:\n\n1. Initially, a CA tree only consists of a single base node with a\n   sequential data structure as is depicted in the picture below:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_1.png \"Initial Contention Adapting Search Tree\")\n2. If parallel threads access the CA tree, the value of a base node's\n   counter may eventually reach the threshold that indicates that the\n   base node should be split. A base node split divides the items in a\n   base node between two new base nodes and replaces the original base\n   node with a routing node where the two new base nodes are\n   rooted. The following picture shows the CA tree after the base node\n   pointed to by the tree's root has been split:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_2.png \"First Split Contention Adapting Search Tree\")\n3. The process of base node splitting will continue as long as there\n   is enough contention in base node locks or until the max depth of the\n   routing layer is reached. The following picture shows how the CA\n   tree looks like after another split:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_3.png \"Second Split Contention Adapting Search Tree\")\n4. The synchronization granularity may differ in different parts of a\n   CA tree if, for example, a particular part of a CA tree is accessed\n   more frequently in parallel than the rest. The following picture\n   shows the CA tree after yet another split:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_4.png \"Third Split Contention Adapting Search Tree\")\n5. The following picture shows the CA tree after the fourth split:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_5.png \"Fourth Split Contention Adapting Search Tree\")\n6. The following picture shows the CA tree after the fifth split:\n   \n   \n   ![alt text](/images/ca_tree/ca_tree_6.png \"Fifth Split Contention Adapting Search Tree\")\n7. Two base nodes holding adjacent ranges of items can be joined. Such\n   a join will be triggered after an operation sees that a base\n   node counter's value is below a certain threshold. Remember that a\n   base node's counter is decreased if a thread does not experience\n   contention when acquiring the base node's lock.\n   \u003c!--The conters The likelihood that\n   a join will be triggered in a certain base node gets higher when\n   the probablity of contention that does not detect contention in the\n   base node lock is high. The likelihood that two base nodes are\n   joined is also increased if operations that require both base nodes\n   happens often enough (to reduce the overhead of acquiring locks).--\u003e\n   \n   ![alt text](/images/ca_tree/ca_tree_7.png \"Join of two base nodes in a  Contention Adapting Search Tree\")\n8. As you might have noticed from the illustrations above, splitting\n   and joining results in that old base nodes and\n   routing nodes gets spliced-out from the tree. The memory that these\n   nodes occupy needs to be reclaimed, but this can not happen directly\n   after they have got spliced-out as some threads might still be\n   reading them. The Erlang run-time system has a mechanism called\n   [thread progress](https://github.com/erlang/otp/blob/d6285b0a347b9489ce939511ee9a979acd868f71/erts/emulator/internal_doc/ThreadProgress.md),\n   which the ETS CA tree implementation uses to reclaim these nodes\n   safely.\n   \n   ![alt text](/images/ca_tree/ca_tree_8.png \"Spliced-out base nodes and routing nodes have been reclaimed.\")\n\n[Click here](/images/ca_tree/ca_tree_ani.gif) to see an animation of the example.\n\n## Benchmark\n\nThe performance of the new CA tree-based ETS `ordered_set`\nimplementation has been evaluated in a benchmark that measures the\nthroughput (operations per second) in many scenarios. The\nbenchmark lets a configurable number of Erlang processes perform a\nconfigurable distribution of operations on a single ETS table. The\ncurious reader can find the source code of the benchmark in the [test\nsuite for\nETS](https://github.com/erlang/otp/blob/ba2c374d3d6fcba479bb542eb6ecd5d8216ce84b/lib/stdlib/test/ets_SUITE.erl#L7623).\n\nThe following figures show results from this benchmark on a machine\nwith two Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz (32 cores in total\nwith hyper-threading). The average set size in all scenarios was\nabout 500K. More details about the benchmark machine and configuration\ncan be found on [this\npage](http://blog.erlang.org/bench/ets_ord_set_21_vs_22/21_vs_22.html).\n\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_1.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_2.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_3.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_7.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_8.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_5.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_6.png \"benchmark results\")\n\n![alt text](/bench/ets_ord_set_21_vs_22/plot_4.png \"benchmark results\")\n\nWe see that the throughput of the CA tree-based `ordered_set` (OTP-22)\nimproves when we add cores all the way up to 64 cores, while the old\nimplementation's (OTP-21) throughput often gets worse when more\nprocesses are added. The old implementation's write operations are\nserialized as the data structure is protected by a single\nreaders-writer lock. The slowdown of the old version when adding more\ncores is mainly caused by increased communication overhead when more\ncores try to acquire the same lock and by the fact that the competing\ncores frequently invalidate each other's cache lines.\n\nThe graph for the 100% lookups scenario (the last graph in the list of\ngraphs above) looks a bit strange at first sight. Why does the CA tree\nscale so much better than the old implementation in this scenario? The\nanswer is almost impossible to guess without knowing the\nimplementation details of the `ordered_set` table type. First of all,\nthe CA tree uses the same readers-writer lock implementation\nfor its base node locks as the old implementation uses to protect the whole\ntable. The difference is thus not due to any lock differences. The\ndefault `ordered_set` implementation (the one that is active when\n`write_concurrency` is off) has an optimization that mainly improves\nusage scenarios where a single process iterates over items of the\ntable, for example, with a sequence of calls to the `ets:next/2`\nfunction. This optimization keeps a static stack per table. Some\noperations use this stack to reduce the number of tree nodes that need\nto be traversed. For example, the `ets:next/2` operation does not need\nto recreate the stack, if the top of the stack contains the same key\nas the one passed to the operation (see\n[here][ets_next_stack_opt]). As there is only one static stack per\ntable and potentially many readers (due to the readers-writer lock),\nthe static stack has to be reserved by the thread that is currently\nusing it. Unfortunately, the static stack handling is a scalability\nbottleneck in scenarios like the one with 100% lookups above. The CA\ntree implementation does not have this type of optimization, so it\ndoes not suffer from this scalability bottleneck. However, this also\nmeans that the old implementation may perform better than the new one\nwhen the table is mainly sequentially accessed. One example of when\nthe old implementation (that still can be used by setting the\n`write_concurrency` option to false) performs better is the single\nprocess case of the 10% `insert`, 10% `delete`, 40% `lookup` and 40%\n`nextseq1000` (a sequence of 1000 `ets:next/2` calls) scenario (the\nsecond last graph in the list of graphs above).\n\nTherefore, we can conclude that that turning on `write_concurrency`\nfor an `ordered_set` table is probably a good idea if the table is\naccessed from multiple processes in parallel. Still, turning off\n`write_concurrency` might be better if you mainly access the table\nsequentially.\n\n## A Note on Decentralized Counters\n\nThe CA tree implementation was not the only optimization introduced in\nErlang/OTP 22, affecting the scalability of `ordered_set` with\n`write_concurrency`. An optimization that decentralized counters in\n`ordered_set` tables with `write_concurrency` turned on was also\nintroduced in Erlang/OTP 22 (see [here][decent_ctrs_pull1]).  An\noption to enable the same optimization in all table types was\nintroduced in Erlang/OTP 23 (see [here][decent_ctrs_pull2]). You can\nfind benchmark results comparing the scalability of the tables with\nand without decentralized counters [here][decent_ctrs_bench].\n\n## Further Reading\n\n\nThe following paper describes the CA tree and some optimizations (of which some have not been applied to the ETS CA tree yet) in much more detail than this blog post. The paper also includes an experimental comparison with related data structures.\n\n* *[A Contention Adapting Approach to Concurrent Ordered Sets][jpdc_ca_tree] ([preprint][jpdc_ca_tree_preprint]). Journal of Parallel and Distributed Computing, 2018. Konstantinos Sagonas and Kjell Winblad*\n\nThere is also a lock-free variant of the CA tree that is described in the following paper. The lock-free CA tree uses immutable data structures in its base nodes to substantially reduce the amount of time range queries, and similar operations can conflict with other operations.\n\n* *[Lock-free Contention Adapting Search Trees][lfca_tree] ([preprint][lfca_tree_preprint]). In the proceedings of the 30th Symposium on Parallelism in Algorithms and Architectures (SPAA 2018). Kjell Winblad, Konstantinos Sagonas, and Bengt Jonsson.*\n\nThe following paper, which discusses and evaluates a prototypical CA tree implementation for ETS, was the first CA tree-related paper.\n\n* *[More Scalable Ordered Set for ETS Using Adaptation][erlang_workshop] ([preprint][erlang_workshop_preprint]). In Thirteenth ACM SIGPLAN workshop on Erlang (2014). Konstantinos Sagonas and Kjell Winblad*\n\nYou can look directly at the [ETS CA tree source\ncode][ets_ca_tree_code] if you are interested in specific\nimplementation details. Finally, it might also be interesting to look\nat the [author's Ph.D. thesis][kjell_phd_thesis] if you want to get\nmore links to related work or want to know more about the motivation\nfor concurrent data structures that adapt to contention.\n\n## Conclusion\n\nThe Erlang/OTP 22 release introduced a new ETS `ordered_set`\nimplementation that is active when the `write_concurrency` option is\nturned on. This data structure (a contention adapting search tree) has\nsuperior scalability to the old data structure in many different\nscenarios and a design that gives it excellent performance in a variety\nof scenarios that benefit from different synchronization\ngranularities.\n\n\n[jpdc_ca_tree]: https://doi.org/10.1016/j.jpdc.2017.11.007\n[jpdc_ca_tree_preprint]: http://winsh.me/papers/catree_jpdc_paper.pdf\n[lfca_tree]: https://doi.org/10.1145/3210377.3210413\n[lfca_tree_preprint]: http://winsh.me/papers/spaa2018lfcatree.pdf\n[erlang_workshop]: http://dl.acm.org/citation.cfm?id=2633455\n[erlang_workshop_preprint]: http://winsh.me/papers/erlang_workshop_2014.pdf\n[AVLTree]: https://en.wikipedia.org/wiki/AVL_tree\n[kjell_phd_thesis]: http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1220366\u0026dswid=6575\n[ets_lookup_stack_opt]: https://github.com/erlang/otp/blob/4ca912b859f779d6d9b235ea0cf6fb7662edcc59/erts/emulator/beam/erl_db_tree.c#L3306\n[decent_ctrs_pull1]: https://github.com/erlang/otp/pull/2190\n[decent_ctrs_pull2]: https://github.com/erlang/otp/pull/2229\n[decent_ctrs_bench]: http://winsh.me/ets_catree_benchmark/azure_D64s_decent_ctrs/hash_decentralized_ctrs.html\n[ets_ca_tree_code]: https://github.com/erlang/otp/blob/4ca912b859f779d6d9b235ea0cf6fb7662edcc59/erts/emulator/beam/erl_db_catree.c\n[ets_next_stack_opt]: https://github.com/erlang/otp/blob/master/erts/emulator/beam/erl_db_tree.c#L3084\n"},{"id":"OTP-23-Highlights","title":"OTP 23 Highlights","author":"Kenneth Lundin","excerpt":"\nOTP 23 has just been released (May 13:th 2020).\nIt has been a long process with three release\ncandidates in February, March and April before the final release.\nWe are very thankful for the feedback we have got regarding the release candidates,\nwhich has revealed some bugs and flaws that our internal testing did not find.","article_date":1589328000000,"tags":["otp 23 release"],"frontmatter":{"layout":"post","title":"OTP 23 Highlights","tags":"otp 23 release","author":"Kenneth Lundin"},"content":"\nOTP 23 has just been released (May 13:th 2020).\nIt has been a long process with three release\ncandidates in February, March and April before the final release.\nWe are very thankful for the feedback we have got regarding the release candidates,\nwhich has revealed some bugs and flaws that our internal testing did not find.\n\nThis blog post will describe some highlights of what is new in OTP 23.\n\nYou can download the readme describing the changes here:\n[OTP 23 Readme](http://erlang.org/download/otp_src_23.0.readme).\nOr, as always, look at the release notes of the application you are interested in.\nFor instance here: [OTP 23 Erts Release Notes](http://erlang.org/doc/apps/erts/notes.html).\n\n# Language\n\nIn OTP 23 we have added some new features to the language and compiler, one has been in the backlog since the bit syntax was introduced and the other is a suggestion from the community.\n\n## Matching syntax improvements​\n\n### Binary matching\nIn binary matching, the size of the segment to be matched is now allowed to be a guard expression. In the example below the variable `Size` is bound to the first 8 bits and then it is used in an expression `(Size-1)*8` for the size of the following binary.\n```erlang\nexample1(\u003c\u003cSize:8,Payload:((Size-1)*8)/binary,Rest/binary\u003e\u003e) -\u003e​\n    {Payload,Rest}.\n```\n\n### Matching on maps\nIn the current map matching syntax, the key in a map pattern must be a single value or a literal. This leads to unnatural code if the keys in a map are complex terms.​\n\nWith OTP 23 the keys in map matching can be guard expressions as you see in new_example2.​\n\nThe only limitation is that all variables used in a key expression must be previously bound. \n\nPreviously you had to do like this:\n\n```erlang\nexample2(M, X) -\u003e\n    Key = {tag,X},\n    #{Key := Value} = M,\n    Value.\n```\nAnd now you can do like this:\n```erlang\nnew_example2(M, X) -\u003e​\n    #{ {tag,X} := Value} = M,​\n    Value.​\n```\n\nBelow there is an illegal example showing that it is still not supported to use an unbound variable as part of the expression for the key-pattern. In this case Key is not bound and the requirement is that all variables used in a key expression must be previously bound. \n```erlang\nillegal_example(Key, #{Key := Value}) -\u003e Value.\n```\n\n## Numeric literals with underscore\nIt is now allowed to write numeric literals with underscore between the digits for the purpose of readability. But the placement of the underscores is not totally free there are some rules. See example of allowed use below:\n\n```\n305441741123_456\n1_2_3_4_5\n123_456.789_123\n1.0e1_23\n16#DEAD_BEEF\n2#1100_1010_0011\n```\nAnd in the following example we have some examples of disallowed placement of the underscore:\n\n```\n_123  % variable name\n123_\n123__456  % only single ‘_’\n123_.456\n123._456\n16#_1234\n16#1234_\n```\n\n# Distributed spawn and the new `erpc` module\n## Improved spawn\n\nThe spawn operation is improved regarding scalability and performance for the distributed case. That is when spawning a process on another node. \n\nNew features are also added, such as a distributed `spawn_monitor()` BIF. This function creates a new process and sets up a monitor atomically.\n\nThe `spawn_opt()` BIF will also support the monitor option for setting up a monitor atomically while creating a process on another node.\n\nWe have also added new [`spawn_request()`](http://erlang.org/doc/man/erlang.html#spawn_request-1) BIFs for asynchronous spawning of processes.\n`spawn_request()` supports all options that `spawn_opt()` already supports.\n\n\nThe spawn improvements described above can also be used to optimize and improve many of the functions in the `rpc` module but since the new functions will not be 100% compatible we decided to introduce a new module named `erpc` and will keep the old `rpc` module as well.\n\nThe `erpc` module implements an enhanced subset of the operations provided by the `rpc` module.\n\nEnhanced in the sense that it makes it possible to distinguish between returned value, raised exceptions, and other errors.​\n\n`erpc` also has better performance and scalability than the original `rpc` implementation. This by utilizing the newly introduced `spawn_request()` BIF.\n\nThe `rpc`module now share the same implementation as `erpc` and by that users of `rpc` will automatically benefit from the performance and scalability improvements made in `erpc`.\n\nThe pictures below illustrate the old and the new implementation of rpc:call() and shows why the new one is more efficient and scalable.\n\n\"old\" rpc:call implementation:\n![old rpc illustration](../images/rpc1.png)\n\nnew rcp:call implementation with support in the distribution protocol (spawn request)\n![new rpc illustration](../images/rpc2.png)\n\nAs you can see in the \"old\" implementation above the `rpc:call` relies on the `rex` process on the receiving node to spawn a temporary process to execute the called function. This will make `rex` to a bottleneck if there are many simultaneous `rpc:calls` towards the node. \n\nThe new solution does not use `rex` at all and let the spawned process decode the arguments of the call thus avoiding some unnecessary copying of data that occurs in the \"old\" implementation.\n\n# gen_tcp and the new socket module\n\nIn OTP 22 we introduced the new experimental [socket](http://erlang.org/doc/man/socket.html) API.\nThe idea behind this API is to have a stable intermediary API that can be used\nto create features that are not part of the higher-level `gen_*` APIs. \n\nWe have now come one step further in our plan to replace the inet driver by making it possible to use the `gen_tcp` API with `socket` as an optional back-end.\n\nTo make it easy to test with existing code using `gen_tcp` a new option `{inet_backend, socket | inet}` can be used to select the `socket` implementation instead of the default `inet` implementation. This option must be put first in the option list to the functions: `gen_tcp:listen`, `gen_tcp:connect` and `gen_tcp:fdopen`, which are all functions that create a socket. For example like this:\n```erlang\n{ok,Socket} = gen_tcp:connect(Addr,Port,[{inet_backend,socket}|OtherOpts])\n```\nThe returned `Socket` is a `'$inet'` tagged 3-tuple instead of a port, so all other API functions will use the right implementation for the socket.\n \nA more general override is to use the Kernel configuration variable `inet_backend` and set it to `socket` or `inet`.  For example on the `erl` command-line as \n```\nerl -kernel inet_backend socket\n```\nor set it with\n```\nERL_FLAGS=\"-kernel inet_backend socket\"\n```\n\n# Help in the shell\nWe have implemented [EEP 48](http://erlang.org/eeps/eep-0048.html) which specifies a storage format for API documentation to be used by BEAM languages. By standardizing how API documentation is stored, it will be possible to write tools that work across languages.\n\nThe ordinary doc build is extended with the generation of `.chunk` files for all OTP modules. You can run `make docs DOC_TARGETS=chunks` to build only the EEP 48 chunks. Running just `make docs` without setting the DOC_TARGETS variable will build all formats (html, man, pdf, chunks).\n\nBuilt on these new features we’ve added On-line help in the shell with the functions:\n```erlang\nh(Module) \nh(Module,Function), \nh(Module,Function,Arity)\n```\nThere are also corresponding functions `ht/1,2,3` and `hcb/1,2,3` to get help about types and callback functions\n\nWe have added a new module `shell_docs` in `stdlib` with functions for rendering documentation for a shell. This can be used for instance by Development Environments such as those based on the Language Server Protocol (LSP).\n\nThe `code` module also got a new function, `get_doc` which returns the doc chunk without loading the module.\n\nSee example below for getting documentation for `lists:sort/2`\n```\n4\u003e h(lists,sort,2).\n\n  -spec sort(Fun, List1) -\u003e List2\n                when\n                    Fun :: fun((A :: T, B :: T) -\u003e boolean()),\n                    List1 :: [T],\n                    List2 :: [T],\n                    T :: term().\n\n  Returns a list containing the sorted elements of List1,\n  according to the ordering function Fun. Fun(A, B) is to\n  return true if A compares less than or equal to B in the\n  ordering, otherwise false.\nok\n```\n## Improved tab-completion\nThe tab-completion in the shell is also improved. Previously the tab-completion for modules did only work for already loaded modules now this is extended to work for all modules available in the code path. The completion is also extended to work inside the \"help\" functions h, ht and hcb. You can for example press tab like the example below and get all modules beginning with `l`:\n```\n5\u003e h(l\nlcnt                      leex                      lists                     \nlocal_tcp                 local_udp                 log_mf_h                  \nlogger                    logger_backend            logger_config             \nlogger_disk_log_h         logger_filters            logger_formatter          \nlogger_h_common           logger_handler_watcher    logger_olp                \nlogger_proxy              logger_server             logger_simple_h           \nlogger_std_h              \nlogger_sup\n```\nOr complete all functions beginning with `s` in the `lists` module like this:\n```\n5\u003e h(lists,s\nsearch/2     seq/2        seq/3        sort/1       sort/2       split/2      \nsplitwith/2  sublist/2    sublist/3    subtract/2   suffix/2     sum/1        \n```\n\n# \"Container friendly\" features\n\n## Take CPU quotas into account\nCPU quotas are now taken into account when deciding the default number of online schedulers.\n\nThus, automatically making Erlang a good citizen in container environments where quotas are applied, such as docker with the `--cpus` flag.\n\n## EPMD independence\n\nIn a cloud and container based environment it might be interesting to run distributed Erlang nodes without use of `epmd` and use a hard coded port or an alternative service discovery. Because of this we introduce ways to make it easier to start and configure systems without `epmd`.\n\n### Handshake\nWe have improved the handshake during connection setup in the Erlang distribution protocol.\nIt is now possible to agree on protocol version without depending on `epmd` or other prior knowledge of peer node version.\n\n### Dynamic node name\n\nAnother feature introduced together with the new handshake is the dynamic node name. A dynamic node name is chosen by using the options `-name Name` or `-sname Name` and setting `Name` to `undefined`.\n\nThese options\nmakes the Erlang runtime system into a distributed node. These flags invokes all network servers necessary for a node to become distributed; see `net_kernel`. It is also ensured that `epmd` runs on the current host before Erlang is started; see epmd and the `-start_epmd` option.\n\n**The new feature in OTP 23** is that \n`Name` can be set to `undefined` and then the node will be started in a special mode optimized to be the **temporary client** of another node. When enabled the node will request a dynamic node name from the first node it connects to. In addition these distribution settings will be implied:\n```\nerl -dist_listen false -hidden -dist_auto_connect never\n```\nBecause `-dist_auto_connect` is set to `never`, the system will have to manually call `net_kernel:connect_node/1` in order to start the distribution. If the distribution channel is closed, when a node uses a dynamic node name, the node will stop the distribution and a new call to `net_kernel:connect_node/1` has to be made. Note that the node name may change if the distribution is dropped and then set up again.\n\n**Note!**\nThe dynamic node name feature is supported from OTP 23. Both the temporary client node and the first connected peer node (supplying the dynamic node name) must be at least OTP 23 for it to work.\n\n### New options to control the use of `epmd` \n\nTo give the user more control over the use of `epmd` some new options to the inet distribution has been added.\n\n- `-dist_listen false` Setup the distribution channel, but do not listen for incoming connection. This is useful when you want to use the current node to interact with another node on the same machine without it joining the entire cluster.\n\n- `-erl_epmd_port Port` Configure a default port that the built-in EPMD client should return. This allows the local node to know the port to connect to for any other node in the cluster.\n- `-remsh Node` Starts Erlang with a remote shell connected to `Node`.\n    If no `-name` or `-sname` is given the node will be started using `-sname undefined`. \n    If Node is using long names then you should give `-name undefined`.\n    If `Node` does not contain a hostname, one is automatically taken from the `-name` or `-sname` option.\n\n    **Note**\n    Before OTP-23 the user needed to supply a valid `-sname` or `-name` for `-remsh` to work. \n    This is still the case if the target node is not running OTP-23 or later.\n\n```bash\n# starting the E-node test\nerl -sname test@localhost \n\n# starting a temporary E-node (with dynamic name) as a remote shell to\n# the node test\nerl -remsh test@localhost \n```\n\nThe `erl_epmd` callback API has also been extended to allow returning -1 as the creation which means that a random creation will be created by the node.\n\nIn addition a new callback function called\n`listen_port_please` has been added that allows the callback to return which listen port the distribution should use. This can be used instead of `inet_dist_listen_min/max` if the listen port is to be fetched from an external service.\n\n## New option for `erl_call`\n\n`erl_call` is a C program originally bundled as an example inside the `erl_interface` application.\n`erl_interface` contains C-libraries for communicating with Erlang nodes and letting C programs behave as if they are Erlang nodes. They are then called C nodes. `erl_call` has become popular and is used in products mainly for administration of an Erlang node on the same host. In OTP 23 `erl_call` is installed under the same path as `erl` making available in the path without bothering about the `erl_interface` version. \nAnother new thing in `erl_call` is the `address` option, that can be used to connect directly to a node  without being dependent on `epmd` to resolve the node name.\n\nAFAIK `erl_call` is being used in the upcoming version of relx (used by rebar3) for the node_tool function.\n\n# TLS enhancements and changes\nTLS-1.3 is now supported (in OTP 22 we classed it as experimental) but not yet feature complete. \nKey features supported are:\n- session tickets\n- refreshing of session keys\n- RSASSA-PSS signatures\n- Middlebox compatibility.  \n\nThe \"early data\" feature is not yet supported. Early data is an optimization introduced in TLS 1.3 which allows a client to send data to a server in the first round trip of a connection, without waiting for the TLS handshake to complete if the client has spoken to the same server recently.\n\nIn OTP 23 TLS 1.3 is per default announced as the preferred protocol version by both client and server. Users who are not explicitly configuring the TLS versions should be aware of this since it can have impact on interoperability.  \n\nA new option `exclusive` is provided for `ssl:cipher_suites/2,3` and `ssl:versions` is extended to better reflect what versions of TLS that are available for the current  setup of Erlang/OTP.\n\nAlso note that we have removed support for the legacy TLS version SSL-3.0.\n\n# SSH\nTwo notable SSH features were provided as Pull Requests from open source users, namely support for fetching keys from ssh-agents and TCP/IP port forwarding.  Port forwarding is sometimes called tunneling or tcp-forward/direct-tcp. In the OpenSSH client, port forwarding corresponds to the options -L and -R.\n\nSsh agent stored keys improves the security while port forwarding is often used to get an encrypted tunnel between two hosts. In the area of key handling, the default key plugin `ssh_file.erl` is rewritten and extended with OpenSSH file format \"openssh-key-v1\".  A limitation so far is that keys in the new format cannot be encrypted The default plugin now also uses port numbers which increases the security.\n\nThe SSH application can now be configured in an Erlang config-file. This gives the possibility to for example change the supported algorithm set without code change.\n\n\n# Crypto\nA new crypto API was introduced in OTP-22.0.  The main reason for a new API was to use the OpenSSL libcrypto EVP API that enables HW acceleration, if the machine supports it.  The naming of crypto algorithms is also systemized and now follows the schema in OpenSSL.\n\nThere are parts of the Crypto app that are using very old APIs while other parts are using the latest one.\nIt turned out that using the old API in the new way, and still keeping it backwards compatible, was not possible.\n\nTherefore the old API is kept for now but it is implemented with new primitives.\nThe Old API is deprecated in OTP-23.0 and will be removed in OTP-24.0.\n"},{"id":"persistent_term","title":"Clever use of persistent_term","author":"Lukas Larsson","excerpt":"\nThis blog post will go through three different uses of [persistent_term](http://erlang.org/doc/man/persistent_term.html)\nthat I have used since its release and explain a bit why they work so well with\n[persistent\\_term](http://erlang.org/doc/man/persistent_term.html).","article_date":1567987200000,"tags":["persistent_term literal"],"frontmatter":{"layout":"post","title":"Clever use of persistent_term","tags":"persistent_term literal","author":"Lukas Larsson"},"content":"\nThis blog post will go through three different uses of [persistent_term](http://erlang.org/doc/man/persistent_term.html)\nthat I have used since its release and explain a bit why they work so well with\n[persistent\\_term](http://erlang.org/doc/man/persistent_term.html).\n\n# Global counters\n\nLet's say you want to have some global counters in your system. For example the number\nof times an http request has been made. If the system is very busy that counter\nwill be incremented many many times per second by many different processes. Before\nOTP-22 the best way that I know of to get the best performance is by using a striped\nets tables. i.e. something like the code below:\n\n{% raw %}\n```erlang\nincr(Counter) -\u003e\n  ets:update_counter(?MODULE,{Counter,erlang:system_info(scheduler_id)},1).\n\nread(Counter) -\u003e\n  lists:sum(ets:select(?MODULE,[{{{Counter,'_'},'$1'},[],['$1']}])).\n```\n{% endraw %}\n\nThe code above would make sure that there is very little contention on the ets table\nas each scheduler will get a separate slot in the table to update. This comes at the\ncost of more memory usage and that when reading the value you may not get an exact\nvalue.\n\nIn OTP-22 the same can be achieved by using [counters](http://erlang.org/doc/man/counters.html).\n[Counters](http://erlang.org/doc/man/counters.html) have built-in\nsupport for striping by using the `write_concurrency` option, so we don't have\nto write our own implementation for that. They are also faster and use less memory\nthan ets tables, so lots of wins.\n\nThe remaining problem then is finding the reference to the counter. We could put it\ninto ets and then do an [ets:lookup\\_element/3](http://erlang.org/doc/man/ets.html#lookup_element-3)\nwhen updating a counter.\n\n```erlang\ncnt_incr(Counter) -\u003e\n    counters:add(ets:lookup_element(?MODULE,Counter,2),1,1).\n\ncnt_read(Counter) -\u003e\n    counters:get(ets:lookup_element(?MODULE,Counter,2),1).\n```\n\nThis gives a performance degradation of about 20%, so not really what we want.\nHowever, if we place the counter in [persistent\\_term](http://erlang.org/doc/man/persistent_term.html)\nlike the code below we get a performance increase by about 140%, which is much\nmore in line with what we wanted.\n\n```erlang\ncnt_pt_incr(Counter) -\u003e\n    counters:add(persistent_term:get({?MODULE,Counter}),1,1).\n\ncnt_pt_read(Counter) -\u003e\n    counters:get(persistent_term:get({?MODULE,Counter}),1).\n```\n\nThe reason for this huge difference is because when the [counters](http://erlang.org/doc/man/counters.html)\nare placed into [persistent\\_term](http://erlang.org/doc/man/persistent_term.html)\nthey are placed there as literals which means that at each increment we not longer\nhave to make a copy of the [counters](http://erlang.org/doc/man/counters.html) reference.\nThis is good for two reasons:\n\n1) The amount of garbage will decrease. In my benchmarks the amount of garbage generated\nby `cnt_incr` is 6 words while both `ets_incr` and `cnt_pt_incr` create 3 words.\n\n2) No reference counts have to be modified. What I mean by this is that the\n[counters](http://erlang.org/doc/man/counters.html) reference\nis what is called a magic reference or nif resource. These references work much in the same\nway as reference counted binaries in that they are not copied when sent to different\nprocesses. Instead only a reference count is incremented at copy and then decremented later\nby the GC. This means that for `cnt_incr` we actually have 3 counters that are modified for\neach call. First we increment the reference count on the counter when copying from ets, then\nwe update the actual counter and then eventually we decrement the reference counter. If we\nuse [persistent\\_term](http://erlang.org/doc/man/persistent_term.html), the term is never\ncopied so we don't have to update any reference counters, instead we just have to update the\nactual counter.\n\nHowever, placing the counter in [persistent\\_term](http://erlang.org/doc/man/persistent_term.html)\nis not trouble free. In order to delete or replace the counter reference in\n[persistent\\_term](http://erlang.org/doc/man/persistent_term.html) we have to do a global\nGC which depending on the system could be very very expensive.\n\nSo this method is best to only be used by global persistent counters that will never be deleted.\n\nYou can find the code for all the above examples and the benchmark I ran\n[here](https://gist.github.com/garazdawi/17cdb5914b950f0acae21d9fcf7e8d41).\n\n# Logger level check\n\nIn [logger](http://erlang.org/doc/man/logger.html) there is a primary logging\nlevel that is the first test to be done for each potential log message to be generated.\nThis check can be done many times per second and needs to be very quick. At the\nmoment of writing (OTP-22) logger uses an ets table to keep all its configuration which\nincludes the primary logging level.\n\nThis is not really ideal as doing a lookup from the ets table means that we have to take\na read-lock to protect against parallel writes to the value. Taking such a read lock is not\nterribly expensive, but when done thousands of times per second it adds up.\n\nSo in [this PR](https://github.com/erlang/otp/pull/2356) I've used\n[persistent\\_term](http://erlang.org/doc/man/persistent_term.html) as\na cache for the primary logging level. Now when reading the value from the hot path\nlogger will instead use [persistent\\_term](http://erlang.org/doc/man/persistent_term.html).\nThis removes all locks from the hot path and we only need to do a lookup in the\n[persistent\\_term](http://erlang.org/doc/man/persistent_term.html) hash table.\n\nBut what if we need to update the primary logger level? Don't we force a global GC then?\nNo, because the small integer representing the primary logger level is an immediate.\nThis means that the value fits in one machine word and is always copied in its\nentirety to the calling process. Which in turn means that we don't have to do a global\nGC when replacing the value.\n\nWhen doing this we have to be very careful so that the value does not become a heap value\nas the cost of doing an update would explode. However, it works great for logger and\nhas reduced the overhead of a ?LOG_INFO call by about 65% when no logging should be done.\n\n# Large constant data\n\nWe use an internal tool here at the OTP-team called the \"ticket tool\". It basically\nmanages all of the OTP-XYZ tickets that you see in the release notes that comes with\neach release of Erlang/OTP. It is an ancient tool from late 90's or early 00's that\nno one really wants to touch.\n\nOne part of it is a server that contains a cache of all the 17000 or so tickets that\nhave been created through the years. In that server there is a single process that\nhas each ticket and its state in order to speed up searching in the tickets. The state\nof this process is quite large and when it is doing a GC it takes somewhere around 10\nseconds for it to finish. This means that about every 10 minutes the server freezes for\n10 seconds and we get to experience the joy of being Java programmers for a while.\n\nBeing a VM developer I've always thought the solution to this problem is to implement\neither an incremental GC or at least a mark and sweep GC for large heaps. However, the\nticket tool server has never been of high enough priority to make me spend a year or two\nrewriting the GC.\n\nSo, two weeks ago I decided to take a look and instead I used\n[persistent\\_term](http://erlang.org/doc/man/persistent_term.html)\nto move the data from the heap into the literal area instead. This was possible to do because\nI know that the majority of tickets are only searched and never changed, so they will\nremain in the literal area forever, while the tickets that do get edited move onto the\nheap of the ticket server. Basically my code change was this:\n\n```erlang\nhandle_info(timeout, State) -\u003e\n  persistent_term:put(?MODULE,State),\n  erlang:start_timer(60 * 60 * 1000, self(), timeout),\n  {noreply,persistent_term:get(?MODULE)}.\n```\n\nThis small change puts the entire gen_server state into the literal area and then\nany changes done to it will pull the data into the heap. This dropped the GC pauses\ndown to be non-noticeable and took considerable less time to implement than a new GC\nalgorithm.\n"},{"id":"OTP-22-Highlights","title":"OTP 22 Highlights","author":"Lukas Larsson","excerpt":"\nOTP 22 has just been released. It has been a long process with three release\ncandidates before the final release. We decided this year to try to get one month\nmore testing of the major release and I think that the extra time has paid off.\nWe've received many bug reports from the community about large and small bugs\nthat our internal tests did not find.","article_date":1557705600000,"tags":["otp 22 release"],"frontmatter":{"layout":"post","title":"OTP 22 Highlights","tags":"otp 22 release","author":"Lukas Larsson"},"content":"\nOTP 22 has just been released. It has been a long process with three release\ncandidates before the final release. We decided this year to try to get one month\nmore testing of the major release and I think that the extra time has paid off.\nWe've received many bug reports from the community about large and small bugs\nthat our internal tests did not find.\n\nThis blog post will describe some highlights of what is released in OTP 22\nand in OTP 21 maintenance patches.\n\nYou can download the readme describing the changes here:\n[OTP 22 Readme](http://erlang.org/download/otp_src_22.0.readme).\nOr, as always, look at the release notes of the application you are interested in.\nFor instance here: [OTP 22 Erts Release Notes](http://erlang.org/doc/apps/erts/notes.html).\n\n# Compiler\n\nIn OTP 22 we have completely re-implemented the lower levels of the Erlang compiler.\nBefore this change the Erlang compiler consisted of a number of\nIRs (intermediate representations):\n\n    Erlang AST -\u003e Core Erlang -\u003e Kernel Erlang -\u003e Beam Asm\n\nWhen compiling an Erlang module, the code is optimized and transformed between\nthese different IRs. In OTP 22 we have almost removed the `Kernel Erlang` IR and\nadded a new IR called `Beam SSA`. There are a series of blog posts describing this\nchange in greater details for those that are interested.\n\n  * [Introduction to SSA](http://blog.erlang.org/introducing-ssa/)\n  * [Digging deeper in SSA](http://blog.erlang.org/digging-deeper-in-ssa//)\n  * [SSA History](http://blog.erlang.org/ssa-history/)\n\nWith this change the compile pipeline now looks like this:\n\n    Erlang AST -\u003e Core Erlang -\u003e Kernel Erlang -\u003e Beam SSA -\u003e Beam Asm\n\nTogether with the SSA rewrite a number of new optimizations have been introduced. One such\nis [strengthening](https://github.com/erlang/otp/pull/1958) of the\n[bit syntax](http://erlang.org/doc/reference_manual/expressions.html#bit-syntax-expressions).\nBefore the change, you had to be very careful with how you wrote your binary matching in\norder for the binary match context optimization to work properly. There were also scenarios\nwhere it was impossible to get the optimization to trigger at all. One place in Erlang/OTP\nwhere this had a great effect was the internal [string:bin\\_search\\_inv\\_1](https://github.com/erlang/otp/blob/master/lib/stdlib/src/string.erl#L1638-L1671) function used by `string:lexemes/1`\nand other string functions. We can see the change in the benchmark graph below (where higher\nis better and \u003cspan style=\"color:#0c839c\"\u003ethe turquoise line\u003c/span\u003e in the OTP 22 branch):\n\n![String Lexemes OTP 22 benchmark](../images/bsm_opt_lexemes.png)\n\nYou can read more about this optimization in [PR1958](https://github.com/erlang/otp/pull/1958)\nand [Retiring old performance pitfalls](http://blog.erlang.org/retired-pitfalls-22/).\n\nAnother great optimization is [PR2100](https://github.com/erlang/otp/pull/2100) which\nmakes the compiler's type optimization pass work across functions within the same module.\nFor instance in the code below:\n\n```\n-record(myrecord, {value}).\n\nh(#myrecord{value=Val}) -\u003e\n    #myrecord{value=Val+1}.\n\ni(A) -\u003e\n    #myrecord{value=V} = h(#myrecord{value=A}),\n    V.\n```\n\nThe new compiler is able to detect the type of the term passed as an argument to\n`h/1` and also the return value of `h/1` so it can eliminate the record checks\ncompletely. Looking at the BEAM code (produced by `erlc -S`) of the `h/1` function we get:\n\nOTP 21:\n```\n    {test,is_tagged_tuple,{f,9},[{x,0},2,{atom,myrecord}]}.\n    {get_tuple_element,{x,0},0,{x,1}}.\n    {get_tuple_element,{x,0},1,{x,2}}.\n    {gc_bif,'+',{f,0},3,[{x,2},{integer,1}],{x,0}}.\n    {test_heap,3,1}.\n```\n\nOTP 22:\n```\n    {get_tuple_element,{x,0},1,{x,0}}.\n    {gc_bif,'+',{f,0},1,[{x,0},{integer,1}],{x,0}}.\n    {test_heap,3,1}.\n```\n\nThe `is_tagged_tuple` instruction has been completely eliminated and as an added bonus\none `get_tuple_element` was also removed.\n\nHowever, this is only the start and we are already looking into making even\nbetter optimizations for OTP 23, building on top of the SSA rewrite.\n\n# Socket\n\nOTP 22 comes with a new experimental [socket](http://erlang.org/doc/man/socket.html) API.\nThe idea behind this API is to have a stable intermediary API that users can use\nto create features that are not part of the higher-level gen APIs. We will also be using\nthis API to re-implement the higher-level gen APIs in OTP 23.\n\nAnother aspect of the new socket API is that it can be used to greatly reduce the\noverhead that is inherent with using ports. I wrote this\n[microbenchmark](https://gist.github.com/garazdawi/cd8ea31acb3284bfc526ae4b1bcb67af)\ncalled gen\\_tcp2 to see what the difference could be.\n\n```\nErlang/OTP 22 [erts-10.4] [source] [64-bit]\n\nEshell V10.4  (abort with ^G)\n1\u003e gen_tcp2:run().\n              client             server\n gen_tcp:       12.4 ns/byte       12.4 ns/byte\ngen_tcp2:        7.3 ns/byte        7.3 ns/byte\n   ratio:       58.9 %             58.9%\nok\n```\n\nThe results seem promising. The socket implementation of gen\\_tcp uses roughly 40%\nless CPU to send the same amount of packets. Of course, gen\\_tcp does a lot more\nthan gen\\_tcp2 (dealing with lots of buffers, error cases and IPv6 to name a new),\nso it is not by any means a fair comparison. Though if an application can live\nwithout all the guarantees that come with gen_tcp, then using socket could be\nvery good for performance.\n\n# Write concurrency in `ordered_sets`\n\n[PR1952](https://github.com/erlang/otp/pull/1952) contributed by Kjell\nWinblad from Uppsala University makes it possible to do updates in\nparallel on `ets` tables of the type `ordered_set`. This together with\nother improvements by Kjell Winblad and Sverker Eriksson\n([PR1997](https://github.com/erlang/otp/pull/1997) and\n[PR2190](https://github.com/erlang/otp/pull/2190)) has greatly\nincreased the scalability of such ets tables that are the base for\nmany applications, for instance,\n[pg2](http://erlang.org/doc/man/pg2.html) and the default [ssl session\ncache](http://erlang.org/doc/man/ssl_session_cache_api.html).\n\n![Ordered Set Write Concurrency OTP 22 benchmark](../images/ordered_set_write_conc.png)\n\nIn the benchmark above we can see that on an `ordered_set` table the\noperations per seconds possible on a 64 core machine has increased\ndramatically between OTP 21 and OTP 22. You can see a description of\nthe benchmark and the results of many more benchmarks\n[here](/bench/ets_ord_set_21_vs_22/21_vs_22.html).\n\nThe data structure used to enable `write_concurrency` in the\n`ordered_set` is called contention adaptive search tree. In a\nnutshell, the data structure keeps a shadow tree that represents the\nlocks needed to read or write a term in the tree. When conflicts\nbetween multiple writers happen, the shadow tree is updated to have\nmore fine-grained locks for specific branches of the tree. You can\nread more about the details of the algorithm in [A Contention Adapting\nApproach to Concurrent Ordered\nSets](https://www.sciencedirect.com/science/article/pii/S0743731517303052)\n([PDF](http://www.it.uu.se/research/group/languages/software/ca_tree/catree_proofs.pdf)).\n\n# TLS Improvements\n\nIn OTP 21.3 the culmination of many optimizations in the ssl application was released.\nFor certain use-cases, the overhead of a using TLS has been significantly reduced. For\ninstance in this TLS distribution benchmark:\n\n![TLS Dist OTP 22 benchmark](../images/tls_dist_until_opt.png)\n\nThe bytes per second that the Erlang distribution over TLS is able to send has been\nincreased from 17K to about 80K, so more than 4 times as much data as before. The\nthroughput gain above is mostly due to better batching of distribution messages\nwhich makes it so that ssl does not have to add a lot of padding to each message\nsent. So it does not translate over to using ssl directly but is still a very nice\nperformance improvement.\n\nIn OTP 22 the [logging facility for ssl](http://blog.erlang.org/ssl-logging-in-otp-22/)\nhas been greatly improved and there is now basic server support for `TLSv1.3`. In order to\nwork with `TLSv1.3` you need to install an [OpenSSL](https://github.com/openssl/openssl)\nversion that supports `TLSv1.3` (for instance 1.1.1b), compile Erlang/OTP using\nthat OpenSSL version and generate the correct certificates. Then we can start a `TLSv1.3`\nserver like this:\n\n```\nLOpts = [{certfile, \"tls_server_cert.pem\"},\n\t     {keyfile, \"tls_server_key.pem\"},\n\t     {versions, ['tlsv1.3']},\n\t     {log_level, debug}\n\t    ],\n{ok, LSock} = ssl:listen(8443, LOpts),\n{ok, CSock} = ssl:transport_accept(LSock),\n{ok, S} = ssl:handshake(CSock).\n```\n\nAnd use the `OpenSSL` client to connect:\n\n    openssl s_client -debug -connect localhost:8443 \\\n      -CAfile tls_client_cacerts.pem \\\n      -tls1_3 -groups P-256:X25519\n\nThis will produce a huge amount of logs, but somewhere in there we can see this in Erlang:\n\n    \u003c\u003c\u003c TLS 1.3 Handshake, ClientHello\n\nand this in `OpenSSL`:\n\n    New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384\n\nwhich means that we have successfully created a new `TLSv1.3` connection. If you want to\nduplicate what I've done you can follow \n[these instructions](https://gist.github.com/garazdawi/062627973b2887e50e9c9bbc86740b63).\n\nNot all features of `TLSv1.3` have been implemented, you can see which parts of the RFCs\nthat are missing in the `ssl` application's [Standard Complience documentation](http://erlang.org/doc/apps/ssl/standards_compliance.html#tls-1.3).\n\n# Fragmented distribution messages\n\nIn order to deal with the [head of line blocking](https://en.wikipedia.org/wiki/Head-of-line_blocking)\ncaused by sending very large messages over Erlang Distribution, we have added\n[fragmentation of distribution messages](https://github.com/erlang/otp/pull/2133) in OTP 22.\nThis means that large messages will now be split up into smaller fragments\nallowing smaller messages to be sent without being blocked for a long time.\n\nIf we run the code below that does small rpc calls every 100ms millisecond and\nconcurrently sends many 1/2 GB terms.\n\n```\n1\u003e spawn(fun() -\u003e\n           (fun F(Max) -\u003e\n             {T, _} = timer:tc(fun() -\u003e\n                 rpc:call(RemoteNode, erlang, length, [[]])\n               end),\n             NewMax = lists:max([Max, T]),\n             [io:format(\"Max: ~p~n\",[NewMax]) || NewMax \u003e Max],\n             timer:sleep(100),\n             F(NewMax)\n           end)(0)\n         end).\n2\u003e D = lists:duplicate(100000000,100000000),\n   [{kjell, RemoteNode} ! D || _ \u003c- lists:seq(1,100)],\n   ok.\n```\n\nUsing two of our test machines I get a max latency of about 0.4 seconds on OTP 22,\nwhereas on OTP 21 the max latency is around 50 seconds. So with the network at our\ntest site the max latency is decreased by roughly 99%, which is a nice improvement.\n\n# Counter/Atomics and persistent_terms\n\nThree new modules,\n[`counters`](http://erlang.org/doc/man/counters.html),\n[`atomics`](http://erlang.org/doc/man/atomics.html), and\n[`persistent_term`](http://erlang.org/doc/man/persistent_term.html),\nwere added in OTP 21.2.\nThese modules make it possible for the user to access low-level primitives of the\nruntime to make some spectacular performance improvements.\n\nFor instance, the `cover` tool was recently re-written to use `counters` and `persistent_term`.\nPreviously it used a bunch of `ets` tables to keep the counters for when the code was executed,\nbut now it uses `counters` and the overhead of running `cover` has decreased by up to 80%.\n\n`persistent_term` is adding run-time support for\n[mochiglobal](https://github.com/mochi/mochiweb/blob/master/src/mochiglobal.erl)\nand [similar](https://github.com/discordapp/fastglobal) tools. It makes it possible to\nvery efficiently access data globally but at the cost of making updates very expensive.\nIn Erlang/OTP we so far use it to optimize [logger backends](https://github.com/erlang/otp/blob/9c8075413728e3be373d7dff2a7168b3983e0be3/lib/kernel/src/logger_proxy.erl#L45)\nbut the use cases are numerous.\n\nA fun (and possibly useful) use case for `atomics` is to create a\n[shared mutable bit-vector](https://gist.github.com/garazdawi/48f1284c0d533ab5a39eeac6f8ff99a0).\nSo, now we can spawn 100 processes and play flip that bit with each other:\n\n```\nBV = bit_vector:new(80),\n[spawn(fun F() -\u003e\n            bit_vector:flip(BV, rand:uniform(80)-1),\n            F()\n          end) || _ \u003c- lists:seq(1,100)],\ntimer:sleep(1000),\nbit_vector:print(BV).\n```\n\n# Documentation Changes\n\nIn OTP 21.3, the version when all functions and modules were\n[introduced](https://github.com/erlang/otp/pull/2044) was added to the documentation.\n\n![Documentation Version OTP 21.3](../images/otp_22_docs.png)\n\nSverker used some git magic to figure out when functions and modules were added\nand automatically updated all the reference manuals. So now it should be a lot easier\nto see when some functionality was introduced. Knowing when an option to functions was\nadded is still problematic, but we are trying to be better there as well.\n\nIn OTP 22 a new documentation top section called `Internal Documentation` has been added to\nthe [erts](http://erlang.org/doc/apps/erts/internal_docs.html) and\n[compiler](http://erlang.org/doc/apps/compiler/internal_docs.html) applications.\nThe sections contain the internal documentation that previously only has been\navailable on github so that it easier to access.\n\n# More Memory optimizations\n\nEach major OTP release wouldn't be complete without a set of memory allocator improvements\nand OTP 22 is no exception. The ones with the most potential to impact your\napplications are [PR2046](https://github.com/erlang/otp/pull/2046) and\n[PR1854](https://github.com/erlang/otp/pull/1854). Both of these optimizations\nshould allow systems to better utilize memory carriers in high memory\nsituations allowing your systems to handle more load.\n"},{"id":"ets-oddity","title":"ETS oddity","author":"Lukas Larsson","excerpt":"\nWhen working with the implementation of the new [scalable ordered_set](https://github.com/erlang/otp/pull/1952)\nwe came across a strangeness with the guarantees when iterating over a table\nwhile inserting elements in parallel.","article_date":1546819200000,"tags":["ets write_concurrency first next"],"frontmatter":{"layout":"post","title":"ETS oddity","tags":"ets write_concurrency first next","author":"Lukas Larsson"},"content":"\nWhen working with the implementation of the new [scalable ordered_set](https://github.com/erlang/otp/pull/1952)\nwe came across a strangeness with the guarantees when iterating over a table\nwhile inserting elements in parallel.\n\n### Scenario:\n\n```\n\u003e Tab = ets:new(test_table,\n                [set, public, {write_concurrency, true}]).\n#Ref\u003c0.1705802953.985792516.98626\u003e\n\u003e P1 = spawn(fun() -\u003e\n               ets:insert(Tab, {fir, 1}),\n               ets:insert(Tab, {sec, 2})\n             end).\n\u003e K1 = ets:first(Tab), K2 = ets:next(Tab, K1).\n```\n\nWhat are the theoretical possible values of `K1` and `K2`? Let us first list the obvious:\n\n* `K1 = fir`, `K2 = sec` - both values inserted and found in term order\n* `K1 = sec`, `K2 = fir` - since this is a `set`, the hash algorithm may put `sec` before `fir`\n* `K1 = fir`, `K2 = '$end_of_table'` - only `fir` had time to be inserted\n* `K1 = '$end_of_table'`, `K2 = badarg` - no elements were inserted\n\nHowever it is also possible to get:\n\n* `K1 = sec`, `K2 = '$end_of_table'`\n\nThis was, at first, very counter-intuitive to me. How can the `ets:first/1` find the\nsecond value inserted, but then when iterating not find the value inserted before it?\n\nThe answer can be found in the way that the `write_concurrency` functionality is\nimplemented. Imagine we have a [hash table](https://en.wikipedia.org/wiki/Hash_table)\nwhere each bucket is protected by a mutex. When inserting a new element the mutex for\nthe current bucket has to be taken and when iterating over the hash table we take\neach mutex in turn for the buckets we iterate through.\n\n### Initial Table:\n\n| Bucket #      | Values        |\n| ------------- | ------------- |\n| 1             | `[]`          |\n| 2             | `[]`          |\n| 3             | `[]`          |\n| 4             | `[]`          |\n\n### Finished Table:\n\n| Bucket #      | Values        |\n| ------------- | ------------- |\n| 1             | `[{fir,1}]`   |\n| 2             | `[]`          |\n| 3             | `[]`          |\n| 4             | `[{sec,2}]`   |\n\nSo, in the scenario that leads to the strange behaviour the following will happen:\n\n* `ets:first/1` is called when the table is empty and iterates to Bucket #2.\n\n| Bucket #      | Values        |\n| ------------- | ------------- |\n| 1             | `[]`          |\n| 2 (first)     | `[]`          |\n| 3             | `[]`          |\n| 4             | `[]`          |\n\n\n* The OS does a context switch and P1 is allowed to run.\n* P1 inserts both `{fir,1}` and `{sec,2}` and then exits.\n\n| Bucket #      | Values        |\n| ------------- | ------------- |\n| 1             | `[{fir,a}]`   |\n| 2 (first)     | `[]`          |\n| 3             | `[]`          |\n| 4             | `[{sec,b}]`   |\n\n* The `ets:first/1` call resumes and will only see `sec` and then `'$end_of_table'`.\n\nWhen spelled out like this it becomes more logical that it is possible to get only\nthe element inserted as the second element. This is not normally a problem for\ntables of type `set` which have an arbitrary iteration order that you can't depend on anyway.\n\nHowever, for `ordered_set` you may very well depend on the defined iteration order\nand expect `ets:first/1` to return a key that has at least been first in the table\nat some point in time. But for the same reasons as with `set`, that is not guaranteed\nif you need that guarantee you have to either not use `write_concurrency`,\nfind some other way to synchronize or rely on luck... these races are very rare, but in heavily\nused tables they will eventually happen.\n\nThe same oddity applies to all kinds of table iterations; `ets:next/1`,\n`ets:select/1-3`, `ets:match/1-3` and friends. They may all miss concurrently\ninserted keys and return a key that has never existed in the table ordered\ndirectly after the previously returned key.\n"},{"id":"retired-pitfalls-22","title":"Retiring old performance pitfalls","author":"John Högberg","excerpt":"\nErlang/OTP 22 will bring many performance improvements to the table, but most\nof them have a broad impact and don't affect the way you write efficient code.\nIn this post I'd like to highlight a few things that used to be surprisingly\nslow but no longer need to be avoided.","article_date":1541548800000,"tags":["erts compiler"],"frontmatter":{"layout":"post","title":"Retiring old performance pitfalls","tags":"erts compiler","author":"John Högberg"},"content":"\nErlang/OTP 22 will bring many performance improvements to the table, but most\nof them have a broad impact and don't affect the way you write efficient code.\nIn this post I'd like to highlight a few things that used to be surprisingly\nslow but no longer need to be avoided.\n\n### Named fun recursion\n\nNamed funs have a neat little feature that might not be obvious at a first\nglance; their name is a variable like any other and you're free to pass it to\nanother function or even return it.\n\n```erlang\ndeepfoldl(F, Acc0, L) -\u003e\n    (fun NamedFun([_|_]=Elem, Acc) -\u003e lists:foldl(NamedFun, Acc, Elem);\n         NamedFun([], Acc) -\u003e Acc;\n         NamedFun(Elem, Acc) -\u003e F(Elem, Acc)\n     end)(L, Acc0).\n```\n\nThis is cool but a bit of a headache for the compiler. To create a fun we pass\nits definition and free variables to a `make_fun2` instruction, but we can't\ninclude the fun itself as a free variable because it hasn't been created yet.\nPrior to OTP 22 we solved this by creating a new equivalent fun _inside_ the\nfun, but this made recursion surprisingly expensive both in terms of run-time\nand memory use.\n\nAs of OTP 22 we translate recursion to a direct function call instead which\navoids creating a new fun. Other cases still require recreating the fun, but\nthey're far less common.\n\n[Optimize named funs and fun-wrapped macros #1973](https://github.com/erlang/otp/pull/1973)\n\n### List subtraction with large operands (-- operator)\n\nWhile the Erlang VM appears to be pre-emptively scheduled to the programmer,\nit's [co-operatively scheduled](https://en.wikipedia.org/wiki/Computer_multitasking#Cooperative_multitasking)\ninternally. When a native function runs it monopolizes the scheduler until it\nreturns, so a long-running one can severely harm the responsiveness of the\nsystem. We've therefore written nearly all such functions in a style that\nbreaks the work into short units that complete quickly enough, but there's\na steadily shrinking list of functions that misbehave, and list subtraction\nwas one of these.\n\nIt's usually pretty straightforward to rewrite functions in this style, but\nthe [old algorithm](https://github.com/erlang/otp/blob/d9682b02b81fa6e23e554b6e017650eb89ecebed/erts/emulator/beam/erl_bif_lists.c#L195)\nprocessed the second list in a loop around the first list, which is problematic\nsince both lists can be very long and resuming work in nested loops is often\ntrickier than expected.\n\nIn this case it was easier to just get rid of the nested loop altogether. The\nnew algorithm starts out by building a red-black tree from the right-hand side\nbefore removing elements from the left-hand side. As all operations on the tree\nhave `log n` complexity we know that they will finish really quickly, so all we\nneed to care about is yielding in the outer loops.\n\nThis also had the nice side-effect of reducing the worst-case complexity from\n`O(n²)` to `O(n log n)` and let us remove some warnings from the reference\nmanual and [efficiency guide](http://erlang.org/documentation/doc-10.1/doc/efficiency_guide/commoncaveats.html#operator-----). It's worth noting\nthat the new implementation is always faster than the proposed workarounds, and\nthat it falls back to the old algorithm when it's faster to do so.\n\nThis change will be rolled out in OTP 21.2, big thanks to\nDmytro Lytovchenko (@kvakvs on GitHub) for writing the better half of it!\n\n[Optimize list subtraction (A -- B) and make it yield on large inputs #1998](https://github.com/erlang/otp/pull/1998)\n\n### Lookahead in bit-syntax matching\n\nThe optimization pass for bit-syntax matching was completely rewritten in OTP\n22 to take advantage of the new SSA-based intermediate format. It applies the\nsame optimizations as before so already well-optimized code is unlikely to see\nany benefit, but it manages to apply them in far more cases.\n\nFor those who aren't familiar, all bit-syntax matching operates on a\n[\"match context\"](http://erlang.org/doc/efficiency_guide/binaryhandling.html#matching-binaries)\ninternally, which is a mutable object that keeps track of the current\nmatch position. This helps a lot when matching complicated patterns as it can\nzip back and forth as required, saving us from having to match components more\nthan once.\n\nThis is great when matching several different patterns, but it comes in real\nhandy in loops like the following:\n\n```erlang\ntrim_zero(\u003c\u003c0,Tail/binary\u003e\u003e) -\u003e trim_zero(Tail);\ntrim_zero(B) when is_binary(B) -\u003e B.\n```\n\nAs the compiler can see that `Tail` is passed directly to `trim_zero`, which\npromptly begins with a bit-match, it can skip extracting `Tail` as a sub-binary\nand pass the match context instead. This is a pretty well-known optimization\ncalled \"match context reuse\" which greatly improves performance when applied,\nand a lot of code has been written with it in mind.\n\nThe catch of passing a match context like this is that we have to maintain the\nillusion that we're dealing with an immutable _binary_. Whenever it's used in\na non-matching expression we either need to convert the context to an\nequivalent binary, or admit defeat and skip the optimization.\n\nWhile the compiler did a pretty good job prior to OTP 22 it gave up a bit too\neasily in many cases, and the most trivial example is almost funny:\n\n```erlang\ncalls_wrapper(\u003c\u003c\"hello\",Tail/binary\u003e\u003e) -\u003e\n    count_ones(Tail).\n\n%% This simple wrapper prevents context reuse in the call above. :(\ncount_ones(Bin) -\u003e count_ones_1(Bin, 0).\n\ncount_ones_1(\u003c\u003c1, Tail/binary\u003e\u003e, Acc) -\u003e count_ones_1(Tail, Acc + 1);\ncount_ones_1(\u003c\u003c_, Tail/binary\u003e\u003e, Acc) -\u003e count_ones_1(Tail, Acc);\ncount_ones_1(\u003c\u003c\u003e\u003e, Acc) -\u003e Acc.\n```\n\nA trickier example can be found in the `string` module:\n\n```erlang\nbin_search_inv_1(\u003c\u003cCP1/utf8, BinRest/binary\u003e\u003e=Bin0, Cont, Sep) -\u003e\n    case BinRest of\n        %% 1\n        \u003c\u003cCP2/utf8, _/binary\u003e\u003e when ?ASCII_LIST(CP1, CP2) -\u003e\n            case CP1 of\n                Sep -\u003e\n                    %% 2\n                    bin_search_inv_1(BinRest, Cont, Sep);\n                _ -\u003e\n                    %% 3\n                    [Bin0|Cont]\n            end;\n        %% ... snip ...\n```\n\nWhat we're looking at is a fast-path for ASCII characters; when both `CP1` and\n`CP2` are ASCII we know that `CP1` is not a part of a grapheme cluster and we\ncan thus avoid a call to `unicode_util:gc/1`. It's not a particularly expensive\nfunction but calling it once per character adds up quickly.\n\nAt first glance it might seem safe to pass the context at `2`, but this is made\ndifficult by `Bin0` being returned at `3`. As contexts are mutable and change\ntheir position whenever a match succeeds, naively converting `Bin0` back to a\nbinary would give you what comes after `CP2` instead.\n\nNow, you might be wondering why we couldn't simply restore the position before\nconverting `Bin0` back to a binary. It's an obvious thing to do but before OTP\n22 the context tracked not only the current position but also previous ones\nneeded when backtracking. These were saved in per-context \"slots\" which were\nmutable and heavily reused, and the match at `1` clobbered the slot needed to\nrestore `Bin0`.\n\nThis also meant that a context couldn't be used again after being passed to\nanother function or entering a `try`/`catch`, which made it more or less\nimpossible to apply this optimization in code that requires looking ahead.\n\nAs of OTP 22 these positions are stored outside the context so there's no need\nto worry about them becoming invalid, making it possible to optimize the above\ncases.\n\n[Rewrite BSM optimizations in the new SSA-based intermediate format #1958](https://github.com/erlang/otp/pull/1958)\n"},{"id":"ssl-logging-in-otp-22","title":"TLS logging improvements in OTP 22","author":"Péter Dimitrov","excerpt":"Erlang/OTP 22 will be an important release for the ```ssl``` application. We are working on\nseveral new features and improvements such as support for TLS 1.3, some of those are already\non the master branch. This blog post presents the new ssl debug logging built on the new\nlogger API.","article_date":1538697600000,"tags":["ssl logger"],"frontmatter":{"layout":"post","title":"TLS logging improvements in OTP 22","tags":"ssl logger","author":"Péter Dimitrov"},"content":"Erlang/OTP 22 will be an important release for the ```ssl``` application. We are working on\nseveral new features and improvements such as support for TLS 1.3, some of those are already\non the master branch. This blog post presents the new ssl debug logging built on the new\nlogger API.\n\n## Usage\n\nAs the ```ssl``` application undergoes a lot of changes the release of the new logger API\npresented the opportunity to level up its debug logging capabilities to be on par with\nOpenSSL.\n\nWe have introduced a new option ```log_level``` that specifies the log level for the ```ssl```\napplication. It can take the following values (ordered by increasing verbosity level):\n```emergency```, ```alert```, ```critical```, ```error```, ```warning```, ```notice```,\n```info``` and ```debug```. At verbosity level ```notice``` and above error reports are\ndisplayed in TLS. The level ```debug``` triggers verbose logging of TLS protocol messages\nin a similar style as in OpenSSL.\n\nThe verbose debug logging can be turned on by two simple steps: the ```log_level``` shall\nbe set to ```debug``` and the logger shall be configured to enable ```debug``` logging\nfor the ssl application. The following code snippet is a sample module with a simple TLS server\nand client:\n\n```erlang\n-module(ssltest).\n\n-compile(export_all).\n\n-define(PORT, 11000).\n\nserver() -\u003e\n    application:load(ssl),\n    logger:set_application_level(ssl, debug),\n    {ok, _} = application:ensure_all_started(ssl),\n    Port = ?PORT,\n    LOpts = [{certfile, \"server.pem\"},\n             {keyfile, \"server.key\"},\n             {versions, ['tlsv1.2']},\n             {log_level, debug}\n            ],\n    {ok, LSock} = ssl:listen(Port, LOpts),\n    {ok, CSock} = ssl:transport_accept(LSock),\n    {ok, _} = ssl:handshake(CSock).\n\nclient() -\u003e\n    application:load(ssl),\n    logger:set_application_level(ssl, debug),\n    {ok, _} = application:ensure_all_started(ssl),\n    Port = ?PORT,\n    COpts = [{verify, verify_peer},\n             {cacertfile, \"ca.pem\"},\n             {versions, ['tlsv1.2']},\n             {log_level, debug}\n            ],\n    {ok, Sock} = ssl:connect(\"localhost\", Port, COpts).\n\n```\n\nStarting the server and client in their respective erlang shells produces the following\nverbose logging of TLS protocol messages:\n\n\n```\n1\u003e ssltest:server().\nreading (238 bytes) TLS 1.2 Record Protocol, handshake\n0000 - 16 03 03 00 e9 01 00 00  e5 03 03 5b ab 42 7a ee    ...........[.Bz.\n0010 - 91 23 df 70 30 fb 41 b9  c5 14 79 d7 02 48 74 c9    .#.p0.A...y..Ht.\n0020 - b9 a9 8f e0 e9 04 1a f9  a8 21 49 00 00 4a 00 ff    .........!I..J..\n0030 - c0 2c c0 30 c0 24 c0 28  c0 2e c0 32 c0 26 c0 2a    .,.0.$.(...2.\u0026.*\n0040 - 00 9f 00 a3 00 6b 00 6a  c0 2b c0 2f c0 23 c0 27    .....k.j.+./.#.'\n0050 - c0 2d c0 31 c0 25 c0 29  00 9e 00 a2 00 67 00 40    .-.1.%.).....g.@\n0060 - c0 0a c0 14 00 39 00 38  c0 05 c0 0f c0 09 c0 13    .....9.8........\n0070 - 00 33 00 32 c0 04 c0 0e  01 00 00 72 00 00 00 0e    .3.2.......r....\n0080 - 00 0c 00 00 09 6c 6f 63  61 6c 68 6f 73 74 00 0a    .....localhost..\n0090 - 00 3a 00 38 00 0e 00 0d  00 19 00 1c 00 0b 00 0c    .:.8............\n00a0 - 00 1b 00 18 00 09 00 0a  00 1a 00 16 00 17 00 08    ................\n00b0 - 00 06 00 07 00 14 00 15  00 04 00 05 00 12 00 13    ................\n00c0 - 00 01 00 02 00 03 00 0f  00 10 00 11 00 0b 00 02    ................\n00d0 - 01 00 00 0d 00 18 00 16  06 03 06 01 05 03 05 01    ................\n00e0 - 04 03 04 01 03 03 03 01  02 03 02 01 02 02          ..............\n\u003c\u003c\u003c TLS 1.2 Handshake, ClientHello\n[{client_version,{3,3}},\n {random,\n     \u003c\u003c91,171,66,122,238,145,35,223,112,48,251,65,185,197,20,121,215,2,72,116,\n       201,185,169,143,224,233,4,26,249,168,33,73\u003e\u003e},\n {session_id,\u003c\u003c\u003e\u003e},\n {cipher_suites,\n     [\u003c\u003c0,255\u003e\u003e,\n      \u003c\u003c\"À,\"\u003e\u003e,\u003c\u003c\"À0\"\u003e\u003e,\u003c\u003c\"À$\"\u003e\u003e,\u003c\u003c\"À(\"\u003e\u003e,\u003c\u003c\"À.\"\u003e\u003e,\u003c\u003c\"À2\"\u003e\u003e,\u003c\u003c\"À\u0026\"\u003e\u003e,\u003c\u003c\"À*\"\u003e\u003e,\n      \u003c\u003c0,159\u003e\u003e,\n      \u003c\u003c0,163\u003e\u003e,\n      \u003c\u003c0,107\u003e\u003e,\n      \u003c\u003c0,106\u003e\u003e,\n      \u003c\u003c\"À+\"\u003e\u003e,\u003c\u003c\"À/\"\u003e\u003e,\u003c\u003c\"À#\"\u003e\u003e,\u003c\u003c\"À'\"\u003e\u003e,\u003c\u003c\"À-\"\u003e\u003e,\u003c\u003c\"À1\"\u003e\u003e,\u003c\u003c\"À%\"\u003e\u003e,\u003c\u003c\"À)\"\u003e\u003e,\n      \u003c\u003c0,158\u003e\u003e,\n      \u003c\u003c0,162\u003e\u003e,\n      \u003c\u003c0,103\u003e\u003e,\n      \u003c\u003c0,64\u003e\u003e,\n      \u003c\u003c\"À\\n\"\u003e\u003e,\n      \u003c\u003c192,20\u003e\u003e,\n      \u003c\u003c0,57\u003e\u003e,\n      \u003c\u003c0,56\u003e\u003e,\n      \u003c\u003c192,5\u003e\u003e,\n      \u003c\u003c192,15\u003e\u003e,\n      \u003c\u003c\"À\\t\"\u003e\u003e,\n      \u003c\u003c192,19\u003e\u003e,\n      \u003c\u003c0,51\u003e\u003e,\n      \u003c\u003c0,50\u003e\u003e,\n      \u003c\u003c192,4\u003e\u003e,\n      \u003c\u003c192,14\u003e\u003e]},\n {compression_methods,[0]},\n...\n[Truncated for brevity]\n```\n\nThis is not the final format as there are many ways to further improve the representation\nof the handshake protocol messages such as converting the cipher suites to a human-readable\nerlang representation.\n\nAs a comparison this is the debug output from an OpenSSL server when the same erlang client\nconnects to it:\n\n\n```\n$ /usr/bin/openssl s_server -accept 11000 -tls1_2 -cert server.pem -key server.key -msg -debug\nUsing default temp DH parameters\nACCEPT\nread from 0x16f0040 [0x16f56b3] (5 bytes =\u003e 5 (0x5))\n0000 - 16 03 03 00 a1                                    .....\n\u003c\u003c\u003c ??? [length 0005]\n    16 03 03 00 a1\nread from 0x16f0040 [0x16f56b8] (161 bytes =\u003e 161 (0xA1))\n0000 - 01 00 00 9d 03 03 5b ac-a1 cc 20 4c 4d 52 d0 d4   ......[... LMR..\n0010 - c8 fc dd 95 b0 fa 65 97-57 9e 44 aa dd 0e 46 10   ......e.W.D...F.\n0020 - 6c 14 57 9c ce a0 00 00-04 00 ff c0 14 01 00 00   l.W.............\n0030 - 70 00 2b 00 06 00 04 03-04 03 03 00 00 00 0e 00   p.+.............\n0040 - 0c 00 00 09 6c 6f 63 61-6c 68 6f 73 74 00 0a 00   ....localhost...\n0050 - 3a 00 38 00 0e 00 0d 00-19 00 1c 00 0b 00 0c 00   :.8.............\n0060 - 1b 00 18 00 09 00 0a 00-1a 00 16 00 17 00 08 00   ................\n0070 - 06 00 07 00 14 00 15 00-04 00 05 00 12 00 13 00   ................\n0080 - 01 00 02 00 03 00 0f 00-10 00 11 00 0b 00 02 01   ................\n0090 - 00 00 32 00 04 00 02 02-03 00 0d 00 04 00 02 02   ..2.............\n00a0 - 01                                                .\n\u003c\u003c\u003c TLS 1.2 Handshake [length 00a1], ClientHello\n    01 00 00 9d 03 03 5b ac a1 cc 20 4c 4d 52 d0 d4\n    c8 fc dd 95 b0 fa 65 97 57 9e 44 aa dd 0e 46 10\n    6c 14 57 9c ce a0 00 00 04 00 ff c0 14 01 00 00\n    70 00 2b 00 06 00 04 03 04 03 03 00 00 00 0e 00\n    0c 00 00 09 6c 6f 63 61 6c 68 6f 73 74 00 0a 00\n    3a 00 38 00 0e 00 0d 00 19 00 1c 00 0b 00 0c 00\n    1b 00 18 00 09 00 0a 00 1a 00 16 00 17 00 08 00\n    06 00 07 00 14 00 15 00 04 00 05 00 12 00 13 00\n    01 00 02 00 03 00 0f 00 10 00 11 00 0b 00 02 01\n    00 00 32 00 04 00 02 02 03 00 0d 00 04 00 02 02\n    01\n...\n[Truncated for brevity]\n```\n\nThe verbose debug logging proved to be especially useful during the development of new\nextensions as previously we had to use wireshark captures to validate TLS protocol\nmessages.\n\n## Implementation\n\nIn the ```ssl``` application, we needed a way to handle two types of protocol\nmessages, tls_record and handshake, each with a custom formatter.\n\nThe most straightforward solution was to add a new handler instance to the logger with a\nspecial formatter function that filters out all the \"noise\" coming from other\nmodules of the system.\n\nThe handler itself could reuse the standard handler for logger, ```logger_std_h```, as it\ncould print logs to ```standard_io```. You can add multiple standard handler instances to\nlogger if your application requires it.\n\n```erlang\nlogger:add_handler(ssl_handler, logger_std_h, Config),\n```\n\nThe new ssl_handler is configured with a formatter that is implemented by the ```ssl_logger```\nmodule.\n\n```erlang\nConfig = #{level =\u003e debug,\n           filter_default =\u003e stop,\n           formatter =\u003e {ssl_logger, #{}}},\n```\n\nHandler filter level is set to ```debug``` with ```stop``` as the default filter action. We also\nneed a filter that lets the log events pass to the formatter if the source of the log event is the\nssl application. In other words, we need a domain filter with the action ```log``` on all sub-domains\nmatching ```[otp,ssl]```.\n\n```erlang\nFilter = {fun logger_filters:domain/2,{log,sub,[otp,ssl]}},\n```\n\nPutting it all together we get the following function.\n\n```erlang\nstart_logger() -\u003e\n    Config = #{level =\u003e debug,\n               filter_default =\u003e stop,\n               formatter =\u003e {ssl_logger, #{}}},\n    Filter = {fun logger_filters:domain/2,{log,sub,[otp,ssl]}},\n    logger:add_handler(ssl_handler, logger_std_h, Config),\n    logger:add_handler_filter(ssl_handler, filter_non_ssl, Filter).\n```\n\nThe function ```format``` is called in ssl_logger when an event gets through all the filters:\n\n```erlang\nformat(#{level:= _Level, msg:= {report, Msg}, meta:= _Meta},\n       _Config0) -\u003e\n     #{direction := Direction,\n       protocol := Protocol,\n       message := BinMsg0} = Msg,\n    case Protocol of\n        'tls_record' -\u003e\n            BinMsg = lists:flatten(BinMsg0),\n            format_tls_record(Direction, BinMsg);\n        'handshake' -\u003e\n            format_handshake(Direction, BinMsg0);\n        _Other -\u003e\n            []\n    end.\n```\n\nThere are two more helper functions that wrap around the logging macros. They were added in order\nto be able to set logging level per TLS session.\n\n```erlang\ndebug(Level, Report, Meta) -\u003e\n    case logger:compare_levels(Level, debug) of\n        lt -\u003e\n            ?LOG_DEBUG(Report, Meta);\n        eq -\u003e\n            ?LOG_DEBUG(Report, Meta);\n        _ -\u003e\n            ok\n    end.\n\nnotice(Level, Report) -\u003e\n    case logger:compare_levels(Level, notice) of\n        lt -\u003e\n            ?LOG_NOTICE(Report);\n        eq -\u003e\n            ?LOG_NOTICE(Report);\n        _ -\u003e\n            ok\n    end.\n```\n\nTo print a log event, the above functions are called with the configured ssl log level and\nthe domain parameter.\n\n```erlang\nssl_logger:debug(Opts#ssl_options.log_level,\n\t         Report,\n\t\t #{domain =\u003e [otp,ssl,handshake]}),\n```\n\nThose who are interested in the current state of development can already play with the\n```'tlsv1.3'``` atom in the ```versions``` option.\n"},{"id":"ssa-history","title":"SSA History","author":"Björn Gustavsson","excerpt":"\nThis blog post looks back on the development of\nthe [SSA-based intermediate representation][pr1935]\nfrom the beginning of this year to the end\nof August when the branch was merged.","article_date":1538092800000,"tags":["compiler BEAM"],"frontmatter":{"layout":"post","title":"SSA History","tags":"compiler BEAM","author":"Björn Gustavsson"},"content":"\nThis blog post looks back on the development of\nthe [SSA-based intermediate representation][pr1935]\nfrom the beginning of this year to the end\nof August when the branch was merged.\n\n## January 2018\n\nIn January this year we realized that we have reached\nthe limit of the optimizations that we could do working\non BEAM code.\n\n[John][john] had finished the work on extending [`beam_bsm`][beam_bsm]\n(a pass that attempts to [delay creation of sub\nbinaries][bin_matching]). The extended `beam_bsm` pass could apply the\noptimization in a few more cases than it could before, but the amount\nof code in `beam_bsm` to achieve that modest improvement of the\noptimization was insane.\n\nJohn, [Lukas][lukas], and I discussed what we should do about\nit. Clearly, we needed a better intermediate format. But what should\nit be? Could we use the existing BEAM code but with variables instead of\nBEAM registers and do register allocation later? That would solve\nsome of the problems but not all of them. The irregular nature of\nBEAM instructions makes it cumbersome to traverse and analyze\nBEAM code.\n\nSo we decided to do like most modern compilers and use an [SSA][ssa]-based\nintermediate format.\n\n### Rewrites are scary!\n\nIntroducing a new intermediate format would require rewriting at\nleast some parts of the compiler. The problem with rewrites is\nthat they always take longer time than expected and that they often\nget abandoned before they are finished.\n\nTo increase the odds that this rewrite would be successful, we come\nup with this plan to do the minimum amount of work to get something working\nas soon as possible:\n\n1. Write a new pass that translates from [Kernel Erlang][kernel] to SSA code.\n\n2. Write a new pass that translates from SSA code to BEAM code.\n\n3. Keep all existing optimization passes.\n\n4. Rewrite the optimization passes one at a time.\n\nIt didn't quite work out according to the plan, as will soon be evident.\n\n## February 2018\n\nI made the first the commit February 1 this year.\n\n### beam_kernel_to_ssa\n\nThe first pass I wrote was the translator from Kernel Erlang to SSA code.\nWe named it `beam_kernel_to_ssa`.\n\nMy first thought was to write the pass from scratch, as opposed to\nbase it on `v3_codegen`. After all, there are fundamental differences\nbetween BEAM code and SSA code. BEAM code is a flat list of instructions.\nSSA code consists of blocks of numbered blocks stored in a map, and there\nare also the phi nodes.\n\nOn the other hand, the input for both `v3_codegen` and\n`beam_kernel_to_ssa` was Kernel Erlang. There was nothing wrong with\nthe code that handled the Kernel Erlang records and I didn't want to\nrewrite that code from scratch.  Instead, I rewrote the part of the\ncode that produced a list of BEAM instructions to produce a list of\nSSA instructions.  I then wrote a simple pass (about 100 lines of\ncode) that [packaged the SSA instructions into blocks and added the\nphi nodes][beam_kernel_to_ssa_finalize].\n\n### Testing beam_kernel_to_ssa\n\nI prefer to test the code I write a soon as possible after writing it.\nIt is much easier to find and fix bugs in code that has been recently\nwritten.\n\nHow can one test `beam_kernel_to_ssa` before the code generator for\nBEAM code has been written?\n\nOne cannot, not completely, but there are ways to find major problems.\n\nOne such way is [smoke testing][software_smoke_testing]. I modified\nthe compiler so that it would first run `beam_kernel_to_ssa` but\ndiscard its output, then run `v3_codegen` and the rest of the compiler\npasses. That allowed me to run the entire compiler test suite, and\nif the `beam_kernel_to_ssa` pass crashed, I've had found a bug.\n\nAnother way was to write a validator or linter of the SSA code.\n[John][john] wrote the `beam_ssa_lint` pass (actually called\n`beam_ssa_validator` at that time and later renamed), which would\nverify that a variable was only defined once, that variables were\ndefined before they were used, that labels in terminators and phi\nnodes referred to defined blocks, and so on. It helped me find a\nfew bugs.\n\n[Dialyzer][dialyzer] also helped me find some bugs. I made sure\nthat I added types for all fields in all new records and\nspecifications for all exported functions. Dialyzer pointed out\nsome bugs when I ran it and thinking about the types when writing\nthe `-type` declarations was also useful.\n\n### Finishing beam_kernel_to_ssa\n\nI am not sure exactly how long time I spent on the initial\nimplementation of `beam_kernel_to_ssa`, but it was probably less\nthan two weeks. There were a few snags along the way, most of\nthem bugs in `v3_kernel` that did not cause any problems\nwith the old `v3_codegen`.\n\nHere is an example. I chose to fix it in OTP 21 even though it was\nharmless in that release:\n\n[v3_kernel: Stop ensuring one return value in #k_try{}][v3_kernel_bug]\n\n### beam_ssa_pre_codegen\n\nNext up was the translation from SSA code to BEAM code.\n\nI have already decided that the translation was sufficiently complicated\nthat to better be split into two major passes.\n\nThe first pass of those passes,\n[`beam_ssa_pre_codegen`][beam_ssa_pre_codegen], would work on the SSA\ncode, rewriting it, and adding annotations for another pass that would\ngenerate the BEAM code, but the output would still be valid SSA code\nso that `ssa_lint` could be used to validate the output. The pretty-printed\nSSA code also includes the annotations to facilitate debugging.\n\nThe `dprecg` option can be used to produce a pretty-printed listing of\nthe SSA code. The following command will create the file `blog.precodegen`:\n\n```\nerlc +dprecg blog.erl\n```\n\nThe next section will dig deeper into the workings of `beam_ssa_pre_codegen`.\nOn a first reading, you might want to skip that section and jump ahead to\nthe section about [beam_ssa_codegen](#beam_ssa_codegen).\n\n### Digging deeper in beam_ssa_pre_codegen\n\nTo provide some context for the description of `beam_ssa_pre_codegen`,\nwe will first look at some BEAM code and talk about stack frames and\nY registers.\n\nHere is the example in Erlang:\n\n```erlang\nfoo(C, L) -\u003e\n    Sum = lists:sum(L),\n    C + Sum.\n```\n\nThe BEAM code looks like this:\n\n    {allocate,1,2}.\n    {move,{x,0},{y,0}}.\n    {move,{x,1},{x,0}}.\n    {line,[{location,\"blog.erl\",5}]}.\n    {call_ext,1,{extfunc,lists,sum,1}}.\n    {line,[{location,\"blog.erl\",6}]}.\n    {gc_bif,'+',{f,0},1,[{y,0},{x,0}],{x,0}}.\n    {deallocate,1}.\n    return.\n\nAs usual, we will walk through the code one or a few lines at a time.\n\n    {allocate,1,2}.\n\nThe `allocate` instruction allocates a stack frame. The `1` operand\nmeans that there should be room for one slot in the stack frame for\nstoring one value.  The slots in the stack frame are called *Y\nregisters*.\n\nThe `2` operand means that two X registers (`{x,0}` and `{x,1}`) are\nlive and must be preserved if `allocate` needs to do a garbage\ncollection in order to allocate space for the stack frame.\n\n    {move,{x,0},{y,0}}.\n\nThe `C` argument for `foo/2` is in `{x,0}`. The `move` instruction\ncopies the value of `{x,0}` to `{y,0}`, which is the zeroth slot\nin the stack frame. The reason for doing this copy will soon become\nclear.\n\n    {move,{x,1},{x,0}}.\n\nPreparing for the call of `lists:sum/1`, the value of `L` in `{x,1}`\nis copied to `{x,0}`.\n\n    {line,[{location,\"blog.erl\",5}]}.\n    {call_ext,1,{extfunc,lists,sum,1}}.\n\nHere `lists:sum/1` is called. The argument is in `{x,0}`. The result\n(the sum of all numbers in the list) is returned in `{x,0}`. Also,\nthe contents of all other X registers are destroyed. That means that\nany value that is to be used after a function call must be saved to\na Y register.\n\n    {gc_bif,'+',{f,0},1,[{y,0},{x,0}],{x,0}}.\n\nThis instruction calculates the sum of `C` (in `{y,0}`) and `Sum` (in `{x,0}`),\nstoring the result in `{x,0}`.\n\n    {deallocate,1}.\n\nPreparing to return from the function, the `deallocate` instruction\nremoves the stack frame that `allocate` created.\n\n    return.\n\n`return` returns from the function. The return value is in `{x,0}`.\n\nHere is the SSA code for the function:\n\n```\nfunction blog:foo(_0, _1) {\n0:\n  %% blog.erl:5\n  _2 = call remote (literal lists):(literal sum)/1, _1\n\n  %% blog.erl:5\n  _3 = bif:'+' _0, _2\n  @ssa_bool = succeeded _3\n  br @ssa_bool, label 3, label 1\n\n3:\n  ret _3\n\n1:\n  @ssa_ret = call remote (literal erlang):(literal error)/1, literal badarg\n  ret @ssa_ret\n}\n```\n\nAfter running `beam_ssa_pre_codegen`, the SSA code looks like this:\n\n```\nfunction blog:foo(x0/_0, x1/_1) {\n  %% _0: 0..1\n  %% _1: 0..1 0..3\n%% #{frame_size =\u003e 1,yregs =\u003e [0]}\n0:\n  %% _0:4: 1..5\n  [1] y0/_0:4 = copy x0/_0\n\n  %% blog.erl:5\n  %% _2: 3..5\n  [3] x0/_2 = call remote (literal lists):(literal sum)/1, x1/_1\n\n  %% blog.erl:5\n  %% _3: 5..11\n  [5] x0/_3 = bif:'+' y0/_0:4, x0/_2\n\n  %% @ssa_bool: 7..9\n  [7] z0/@ssa_bool = succeeded x0/_3\n  [9] br z0/@ssa_bool, label 3, label 1\n\n3:\n  [11] ret x0/_3\n\n1:\n  %% @ssa_ret: 13..15\n  [13] x0/@ssa_ret = call remote (literal erlang):(literal error)/1, literal badarg\n  [15] ret x0/@ssa_ret\n}\n```\n\nWe will describe what the important (for this example) sub passes of\n`beam_ssa_pre_codegen` do, and point to the relevant part of code while\ndoing so.\n\nThe sub pass [place_frames] determines where stack frames should be allocated.\nIn the example, block 0 needs a stack frame.\n\nThe sub pass [find_yregs] determines which variables that are to be\nplaced in Y registers. The result will be a `yregs` annotation added\nto each block that allocates a stack frame. For this example, the\nannotation will look like:\n\n\u003cpre class=\"highlight\"\u003e\n    %% #{frame_size =\u003e 1,\u003cb\u003eyregs =\u003e [0]\u003c/b\u003e}\n\u003c/pre\u003e\n\nVariable `_0` is `C` from the Erlang code. It needs to be saved across the\ncall to `lists:sum/1`.\n\nThe sub pass [reserve_yregs] uses the `yregs` annotations and inserts `copy` instructions\nto copy each variable that needs saving to a new variable. For the example,\nthe following instruction will be added\n\n```\n  [1] y0/_0:4 = copy x0/_0\n```\nIt copies the value of `_0` to `_0:4`.\n\nThe sub pass [number_instructions] numbers all instructions as a preparation for register\nallocation. In the listing, those numbers are in brackets before each instruction:\n`[1]`, `[3]`, `[5]`, and so on.\n\nThe sub pass [live_intervals] calculates the intervals in which each variable is live.\nIn the listing, the live intervals are shown as comments before the definition\nof the variable:\n\n```\n  %% _0:4: 1..5\n  [1] y0/_0:4 = copy x0/_0\n```\n\nThe variable `_0:4` is live from instruction `[1]` (its definition) to\n`[5]` (its last use).\n\nThe sub pass [linear_scan] uses the [linear scan][linear_scan_polleto] algorithm\nto allocate registers for each variable. The result is saved as annotation\nfor the function. In the listing of the SSA code, the register will be added\nto the definition and each use of a variable. For example:\n\n```\n  [1] y0/_0:4 = copy x0/_0\n```\n\nVariable `_0` (the argument `L`) is in `{x,0}`. Its copy in `_0:4` is in\n`{y,0}`.\n\nBut what is `z0`?\n\n\u003cpre class=\"highlight\"\u003e\n      [7] \u003cb\u003ez0\u003c/b\u003e/@ssa_bool = succeeded x0/_3\n      [9] br \u003cb\u003ez0\u003c/b\u003e/@ssa_bool, label 3, label 1\n\u003c/pre\u003e\n\n`succeeded` is not a BEAM instruction. It will be combined with the previous\ninstruction (`bif:+` in this example) and the `br` instruction that follows it\nto the following BEAM instruction:\n\n    {gc_bif,'+',{f,0},1,[{y,0},{x,0}],{x,0}}.\n\nThus, the value `@ssa_bool` is never explicitly stored in a BEAM\nregister.  Before I invented Z registers, `@ssa_bool` would have been\nassigned to an X register.  That worked most of the time, but sometimes\nan X register would seem to be occupied when it was not, and prevent\nanother instruction from using that register.\n\nHere are the [references that I used when implementing linear scan][linear_scan_references].\n\nThe sub pass [frame_size] uses the information from the linear scan pass to calculate the size\nof each stack frame. The result is stored as an annotation:\n\n\u003cpre class=\"highlight\"\u003e\n    %% #{\u003cb\u003eframe_size =\u003e 1\u003c/b\u003e,yregs =\u003e [0]}\n\u003c/pre\u003e\n\n[place_frames]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L715\n\n[find_yregs]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L1114\n\n[reserve_yregs]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L1645\n\n[number_instructions]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L1487\n\n[live_intervals]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L1515\n\n[linear_scan]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L2118\n\n[frame_size]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L1741\n\n### beam_ssa_codegen\n\nThe [`beam_ssa_codegen`][beam_ssa_codegen] pass generates BEAM code\nfrom the annotated SSA code. Testing of this pass was easier, because\nI could compile some sample code and try to run it.\n\nOften I did not even have to run the code to know that it was wrong.\nThe compiler would tell me, loudly:\n\n{% raw %}\n```\nblog: function bar/2+4:\n  Internal consistency check failed - please report this bug.\n  Instruction: {test_heap,2,3}\n  Error:       {{x,2},not_live}:\n```\n{% endraw %}\n\nIt's time to introduce the `beam_validator` pass.\n\n#### beam_validator\n\nThe [`beam_validator`][beam_validator] pass was introduced in one of\nthe R10B releases (probably in 2006). It is run directly before the\nBEAM code is packaged into a binary and written to a BEAM file. The\npurpose of `beam_validator` is to find unsafe instructions that\ncould crash the runtime system or cause it to misbehave in other\nways.\n\nLet's look at a simple example:\n\n```erlang\nbar(H, T) -\u003e\n    [H|T].\n```\n\nHere is the BEAM code, but edited by me to contain an unsafe instruction:\n\n\u003cpre class=\"highlight\"\u003e\n      {label,4}.\n        {test_heap,2,\u003cb\u003e3\u003c/b\u003e}.\n        {put_list,{x,0},{x,1},{x,0}}.\n        return.\n\u003c/pre\u003e\n\nThe number of live registers is here given as `3` instead of `2`.\nThat means that `{x,0}`, `{x,1}`, and `{x,2}` are supposed to contain\nvalid Erlang terms. Because `bar/2` is only called with two arguments,\n`{x,2}` can contain any old garbage.\n\nWhen running this code, it could crash the runtime system, or it could\nbe completely harmless. It depends on whether there will be a garbage\ncollection during execution of the `test_heap` instruction, and on the\nexact nature of the garbage in `{x,2}`. For example, if the garbage\nhappens to be an atom nothing bad will happen. That means that this\ntype of compiler bug is difficult to reliably catch in a test case.\n\n`beam_validator` will find this bug immediately. It keeps track of\nwhich registers are initialized at any point in the function. If it\nfinds a reference to a register that is not initialized it will\ncomplain:\n\n{% raw %}\n```\nblog: function bar/2+4:\n  Internal consistency check failed - please report this bug.\n  Instruction: {test_heap,2,3}\n  Error:       {{x,2},not_live}:\n```\n{% endraw %}\n\n#### Friend and foe\n\nDuring the implementation of `beam_ssa_codegen`, the `beam_validator`\npass pointed out many bugs for me. It was my friend.\n\nIt was also my foe, sort of. It would complain that some perfectly safe\ncode was unsafe. When that happened, I had to thoroughly investigate the\ncode to make doubly sure it was safe, and then extend `beam_validator`\nto make it smarter so that it would understand that the code was safe.\n\nHere is one example where I had to make `beam_validator` smarter.\nConsider this code:\n\n    {move,{x,0},{y,0}}.\n    {test,is_map,{f,777},[{x,0}]}.\n    {put_map_assoc,{f,0},{y,0},...}.\n\nThe `move` instruction stores a copy of `{x,0}` in `{y,0}` (a location\non the stack). The following `test` instruction tests whether `{x,0}`\nis a map and branches to label 777 if not. The `put_map_assoc` instruction\nupdates the map in `{y,0}`.\n\nThe `put_map_assoc` instruction will crash if its source argument is\nnot a map.  Therefore, `beam_validator` complains if `put_map_assoc` is\nused with a source argument that is not a map. In this example,\n`beam_validator` had not seen a `test` instruction that ensured that\n`{y,0}` was a map, so it complained. It is obvious (for a human) that\n`{y,0}` is a map because it is a copy of `{x,0}`, which is a map.\n\n`v3_codegen` never generated such code; in fact, it explicitly\n[avoided generating such code][v3_codegen_kludge].\nI did not want to add similar kludges to the new code generator, so\n[`beam_validator` had to become smarter][beam_validator_maps].\n\n[v3_codegen_kludge]: https://github.com/erlang/otp/blob/64422fcac9c602641dcf24ef2d35e3491376304d/lib/compiler/src/v3_codegen.erl#L1600\n\n#### Unsafe optimization passes\n\nSome of the unsafe code that `beam_validator` found was really unsafe,\nbut it was not the fault of my new compiler passes, but of the\noptimization passes that optimized the generated BEAM code.\n\nThe problem was that some of the optimization passes had implicit\nassumptions of the kind of code that `v3_codegen` would generate\n(or, rather, would **not** generate). The new code generator broke\nthose assumptions.\n\nAt first, when I saw those bugs, I removed the broken part of the\noptimization pass. Making the optimizations safe would be non-trivial\nand ultimately wasted work because we intended to rewrite all those\noptimization passes to work on SSA code.\n\nWhen I have seen a few too many of those unsafe optimizations, [I ripped\nout all of the unsafe optimization passes][unsafe_passes].\n\nThat meant that we would have to re-implement all of the optimizations\nbefore the generated code would be as good as the code from the old\ncompiler. I had also noticed that the new BEAM code generator in a\nfew ways generated better code than the old one, but in other ways the\ncode was worse. For example, the generated code used more stack space and\ndid a lot of register shuffling. Eventually, that had to be addressed in\nsome way.\n\nMeanwhile, I had worse problems to worry about.\n\n## March 2018\n\nOn March 14 I presented my progress on the new compiler passes for\nthe OTP team. One of my slides had the following text:\n\n\u003e * Can compile all modules in OTP (and run many of them correctly)\n\nYes, I had finished the initial implementation of `beam_ssa_codegen`\nso that I could compile all code in OTP.\n\nThe problem that I only at hinted in the slide was that Erlang could\ncrash and dump core when running test suites. Not every time, and\nnever in the same test case twice. It only happened when I have\ncompiled OTP with the new compiler.\n\nThe crash didn't seem related to the test cases themselves, but to the\nwriting of log files. I soon narrowed it down to that the crash could\nhappen if [`file_io_server`][file_io_server] had been compiled with\nthe new compiler passes. However, that was not much help. The module\ncontains complicated code that uses the binary syntax, `try`/`catch`,\nand `receive`, all of which are complicated instructions that might\nnot be correctly translated by the new compiler passes.\n\n`beam_validator` was supposed to catch those kinds of bugs before\nthey can cause a crash. Either there was some kind of bug that\n`beam_validator` didn't look for, or there was a bug in the\nimplementation of some of the instructions in the BEAM interpreter.\n\nI ended up spending the rest of March trying to hunt down that bug.\n\n[file_io_server]: https://github.com/erlang/otp/blob/OTP-21.0.9/lib/kernel/src/file_io_server.erl\n\n## April 2018\n\nAt the beginning of April, the bug still eluded me. I had narrowed\nit down somewhat. I was pretty sure it had something to do with\n`receive`.\n\nThen [Rickard] gave me some information that I could connect to another\npiece of information that I had absorbed during my hunt for the bug.\n\n### The bug\n\nThis section is somewhat advanced, and if you wish you can\nskip to [the fix](#the-fix).\n\nReading about the [Erlang Garbage Collector][lukas_gc] can give some\nbackground to better understand this section.\n\nRickard reminded me about the `message_queue_data` option that\nhad been added to [`process_flag/2`][process_flag] in OTP 19. After\ncalling `process_flag(message_queue_data, off_heap)` all messages\nthat have not yet been received would be stored outside the process\nheap. Storing the messages outside the heap means that the garbage\ncollector doesn't have to spend time copying the unreceived messages\nduring garbage collection, which can be a huge win for processes that\nhave many messages in its message queue.\n\nThe implementations details of messages outside the heap are\ncrucial. Consider this selective `receive`:\n\n```erlang\nreceive\n    {tagged_message,Message} -\u003e Message\nend.\n```\n\nWhen the BEAM interpreter executes this code, it will retrieve a\nreference message from the external message queue and match it against\nthe tuple pattern. If the message does not match, the next message\nwill be processed in the same way, and so on.\n\nIf a message does not match, there **must not** be any remaining\nreferences to it stored on the stack. The reason is that if there is a\ngarbage collection, the garbage collector will copy the message (or\npart of the message) to the heap, and, even worse, it will destroy the\noriginal message during the copy operation. The message is still in\nthe external message queue, but it has now been corrupted by the\ngarbage collector. If the message is later matched out in a `receive`,\nit will likely cause a crash.\n\nWhen Rickard first implemented off-heap messages, he asked me whether\nthe compiler could ever store references to unreceived messages\non the stack. I assured him that it could not happen.\n\nYes, that was true, it could not happen because of the way\n`v3_codegen` generated the code for `receive`.\n\nWith the new compiler passes, [it **could** happen][bad_receive]. When\nI first discussed the bug with Richard in March, he did mention that\nit is forbidden to store references to off-heap messages on the\nstack. At that time, I was not aware that the compiler could store\nreferences to off-heap messages on the stack.\n\nWhen Rickard reminded me about that for the second time in April, I remember\nseeing during my bug hunt generated code that stored off-heap message\nreferences on the stack.\n\n[rickard]: https://github.com/rickard-green\n[process_flag]: http://erlang.org/doc/man/erlang.html#process_flag-2\n[lukas_gc]: https://github.com/erlang/otp/blob/OTP-21.0.9/erts/emulator/internal_doc/GarbageCollection.md\n[bad_receive]: https://github.com/erlang/otp/blob/333e4c5a1406cdeb9d1d5cf9bf4a4fadb232fca8/lib/compiler/test/beam_validator_SUITE_data/receive_stacked.S#L22\n\n### The fix\n\nAfter finding the reason for the bug, I first taught `beam_validator`\nto [complain about \"fragile references\" on the stack][beam_validator_fragile].\nI included that commit in OTP 21.\n\nI then added a sub pass to `beam_ssa_pre_codegen` to [rewrite `receive`][receive_fix].\nIt introduces new variables and `copy` instructions to ensure that\nany references to the message being matched are kept in X registers.\n\nWith no known bugs in the code generator, I could start rewriting the optimization\npasses I had removed.\n\n[receive_fix]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L919\n\n### More optimizations\n\n#### beam_ssa_recv\n\n[`beam_ssa_recv`][beam_ssa_recv] is a replacement for the unsafe\n[`beam_receive`][beam_receive] pass.  The purpose is to optimize a\n`receive` that can only match a newly created reference.  The\noptimization avoids scanning the messages that were placed in the\nmessage queue before the reference was created.\n\nI actually wrote `beam_ssa_recv` at the beginning of March as an\nexperiment to see how easy it would be to write optimizations of SSA code.\nIt turned out to be pretty easy. `beam_ssa_recv` can apply the optimization\nin more places than `beam_receive` could, using slightly less code.\n\nIn the old `beam_receive` pass, a lot of code is needed to handle\nthe many variants of BEAM instructions. For example, in\n[`opt_update_regs/3`][beam_ssa_recv_opt_update_regs] there are three\nclauses just to handle three variants of a `call` instruction (calling\na local function, calling an external function, and calling a fun).\n\nHere is an example of a function that `beam_receive` did not optimize, but\n[`beam_ssa_recv` can optimize][yes_14].\n\n[beam_receive]: https://github.com/erlang/otp/blob/OTP-21.0.9/lib/compiler/src/beam_receive.erl\n[beam_ssa_recv]: https://github.com/erlang/otp/blob/367f4a3fabb12cda3f2547e9908acbf28cb34e3a/lib/compiler/src/beam_ssa_recv.erl\n[beam_ssa_recv_opt_update_regs]: https://github.com/erlang/otp/blob/333e4c5a1406cdeb9d1d5cf9bf4a4fadb232fca8/lib/compiler/src/beam_receive.erl#L185\n[yes_14]: https://github.com/erlang/otp/blob/6bee2ac7d11668888d93ec4f93730bcae3e5fa79/lib/compiler/test/receive_SUITE_data/ref_opt/yes_14.erl\n\n#### beam_ssa_opt\n\nThe [`beam_ssa_opt`][beam_ssa_opt] pass runs a [number of\noptimizations][beam_ssa_opt_passes]. Many of the optimizations are replacements\nfor the optimizations I removed earlier.\n\n[beam_ssa_opt]: https://github.com/erlang/otp/blob/81d34181d391709e9d2c404fa730ee9b5c72b5e3/lib/compiler/src/beam_ssa_opt.erl\n\n[beam_ssa_opt_passes]: https://github.com/erlang/otp/blob/81d34181d391709e9d2c404fa730ee9b5c72b5e3/lib/compiler/src/beam_ssa_opt.erl#L49\n\n#### beam_ssa_type\n\n[`beam_ssa_type`][beam_ssa_type] replaces the unsafe [`beam_type`][beam_type] pass.\n\nThe `beam_type` pass did a local type analysis (basically for extended basic blocks),\nand tried to simplify the code, for example by removing unnecessary type tests.\n\nThe `beam_ssa_type` pass analyzes the types in an entire function and\nsimplifies the code, for example by removing unnecessary type\ntests. It finds many more opportunities for optimizations than\n`beam_type` did.\n\n[beam_ssa_type]: https://github.com/erlang/otp/blob/81d34181d391709e9d2c404fa730ee9b5c72b5e3/lib/compiler/src/beam_ssa_type.erl\n\n## May 2018\n\nAt the beginning of May, [John][john] started working on what was to become\nthis pull request:\n\n[#1958: Rewrite BSM optimizations in the new SSA-based intermediate format][pr1958]\n\nI continued to write optimizations and fix bugs that John found while\ndeveloping his optimizations.\n\n### Rethinking the binary matching instructions\n\nWhile working on his binary optimizations, John realized that the SSA\ninstructions for binary matching were difficult to optimize. The\nbinary match instructions I had designed were close to the semantics\nof the BEAM instructions. John suggested that the `bs_get` instruction\nshould be broken up into a `bs_match` instruction and a `bs_extract`\ninstruction to simplify optimizations.\n\nThe breaking up of the instructions meant that [`beam_ssa_pre_codegen`\nwould have to work harder to combine\nthem][beam_ssa_pre_codegen_fix_bs], but it vastly simplified John's\noptimizations. It turned out that it also enabled other optimizations:\nthe [liveness optimizations][beam_ssa_opt_live] could remove unused\ninstructions more aggressively.\n\n[beam_ssa_pre_codegen_fix_bs]: https://github.com/erlang/otp/blob/master/lib/compiler/src/beam_ssa_pre_codegen.erl#L209\n[beam_ssa_opt_live]: https://github.com/erlang/otp/blob/e6c3dd9f701d354c06b9b1b043a3d7e9cc050b1c/lib/compiler/src/beam_ssa_opt.erl#L777\n\nOn the first day of [Code BEAM STO 2018][code_beam_2018] May 31, I didn't\nknow of any bugs in the new compiler passes and my list of optimizations to\nre-implement was shrinking steadily. I met [Michał Muskała][michal]\n(a frequent contributor to Erlang/OTP and a member of the\n[Elixir Core Team][elixir_core_team]) there and told him about my work\non the compiler and that it was stable enough be tested outside OTP,\nfor example to compile Elixir code...\n\n[pr1958]: https://github.com/erlang/otp/pull/1958\n[code_beam_2018]: https://codesync.global/conferences/code-beam-sto-2018/\n[elixir_core_team]: https://elixirforum.com/groups/Elixir-Core-Team\n\n## June 2018\n\n### Michał's feedback\n\nI received an email from Michał in the middle of June. He had tried out\nmy compiler branch. He wrote:\n\n\u003e First impression is that it took a loooong time to compile Elixir's unicode module,\nso long that I had to shut it down after about 10 minutes.\n\nHe sent me an Erlangified version of [Elixir's unicode module][elixir_unicode].\nThe size of the Erlang source for the module was almost 82,000 lines or\nabout 3,700,000 bytes. Based on the size, compilation could be expected to be\na little bit slow, but not that slow. On his computer, OTP 21.0-RC2 finished\nthe compilation in 16 seconds.\n\nI compiled the module using the `time` option. The slowest pass was `beam_ssa_type`.\nAfter some further profiling, I found the bottleneck in the joining of two maps.\nHere is the [corrected code][beam_ssa_type_bottleneck]. The original code didn't compare\nthe size of maps and swap them as needed. I might have done some other improvements,\ntoo. Anyway, that took care of that bottleneck. Now `beam_ssa_pre_codegen` was the\nslowest pass.\n\nI fixed several bottlenecks in the [linear scan sub pass][linear_scan], and after\nthat some other bottleneck in `beam_ssa_pre_codegen`. I think that reduced the\ncompilation time to well under one minute.\n\n[beam_ssa_type_bottleneck]: https://github.com/erlang/otp/blob/81d34181d391709e9d2c404fa730ee9b5c72b5e3/lib/compiler/src/beam_ssa_type.erl#L944\n\n### Optimizing code generation\n\nAfter having finished the re-implementation of the last optimization\npass (I think it was the [optimization of floating point\noperations][beam_ssa_opt_float], as previously done by the unsafe\n[`beam_type`][beam_type] pass), I started to compare the code\ngenerated by OTP 21 with code from the new compiler passes.\n\nI used [scripts/diffable][diffable], which compiles about 1000\nmodules from OTP to BEAM code and massages the BEAM code to make it more friendly\nfor diffing. I then ran `diff -u old new` to compare the new code to\nthe old code.\n\nIn the last part of June and the first week of July, I then improved\n`beam_ssa_pre_codegen` and `beam_ssa_codegen` to address the issues that I\nnoticed when reading the diff.\n\n#### `beam_ssa_pre_codegen` improvements\n\nI did not change the [linear scan][linear_scan] sub pass of `beam_ssa_pre_codegen`\nitself. Instead I added transformations of the SSA code that would help\nlinear scan do a better job of allocating registers.\n\nThe most obvious issue I noticed was unnecessary `move` instructions.\nHere are two of the sub passes I added to address that issue:\n\n* [reserve_xregs] gives hints to the linear scan sub pass that a\ncertain X register should be used for a certain variable, if possible.\n\n* [opt_get_list] tries to eliminate the extra `move` instruction that\nis frequently added when matching out elements from a list.  See the\ncomments in the code for an example and an explanation.\n\nAnother frequent issue was that the code generated from the new code\ngenerator used more stack space because two variables that were not\nstrictly live at the same time were allocated different Y registers\n(slots on the stack) instead of re-using the same Y register. I addressed\nthat issue in [copy_retval]. See the comments in the code for an example.\n\n#### `beam_ssa_codegen` improvements\n\nMichał noticed that when a value was stored in both an X register and\na Y register (on the stack), instructions using the value would\nalways use the Y register. The old code generator would use the X\nregister.  The new code could be slower because the BEAM interpreter\nis generally optimized for operands being in X registers.\n\nI added [prefer_xregs] to address that issue. See the comments in the code\nfor examples.\n\n[reserve_xregs]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L1916\n[copy_retval]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L1222\n[opt_get_list]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L1422\n\n[prefer_xregs]: https://github.com/erlang/otp/blob/7fbb86c77fa99caddabedfb992f47ddeece80652/lib/compiler/src/beam_ssa_codegen.erl#L359\n\n[beam_type]: https://github.com/erlang/otp/blob/OTP-21.0.9/lib/compiler/src/beam_type.erl\n\n[beam_ssa_opt_float]: https://github.com/erlang/otp/blob/e6c3dd9f701d354c06b9b1b043a3d7e9cc050b1c/lib/compiler/src/beam_ssa_opt.erl#L517\n\n## July 2018\n\nVacation.\n\n## August 2018\n\nBefore I left for vacation, it seemed that the new compiler passes generally\ngenerated code at least as good as the old compiler passes. In some cases,\nthe code would be much better.\n\nBack after my vacation, I did some final polishing.\n\nOn Aug 17 I created a [pull request][pr1935].\n\nBefore merging the pull request, I sneaked in a few final optimizations.\n\nOn Aug 24 I [merged][merged_ssa] the pull request.\n\n[merged_ssa]: https://github.com/erlang/otp/commit/9facb02b91979ef90b47ac0a54d1eb71fdaa1ee1\n\n[beam_ssa_pre_codegen]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl\n[beam_ssa_codegen]: https://github.com/erlang/otp/blob/ec1f35c9f52be894ba295b9a48237020855e3c46/lib/compiler/src/beam_ssa_codegen.erl\n[beam_validator]: https://github.com/erlang/otp/blob/e2a939dc4d23d75a0588722d0a08aef129b4c0be/lib/compiler/src/beam_validator.erl\n\n[v3_kernel_bug]: https://github.com/erlang/otp/commit/c896f08f5c028b1e31290e6a5502597401acd39f\n[beam_validator_fragile]: https://github.com/erlang/otp/commit/90853d8e7b50be13a3b71f4a1ed6b0407e1f7c2f\n[unsafe_passes]: https://github.com/erlang/otp/commit/3fc40fd57fa01b097b4c363860c4d4762e13db8b\n[beam_validator_maps]: https://github.com/erlang/otp/commit/1f221b27f1336e747f7409692f260055dd3ddf79\n\n[john]: https://github.com/jhogberg\n[lukas]: https://github.com/garazdawi\n[michal]: https://github.com/michalmuskala\n\n[elixir_unicode]: https://github.com/elixir-lang/elixir/blob/54cb02c2407856f4063c75a440507dacb6a31dbc/lib/elixir/unicode/unicode.ex\n[diffable]: https://github.com/erlang/otp/blob/master/scripts/diffable\n\n[pr1935]: https://github.com/erlang/otp/pull/1935\n\n[bin_matching]: http://erlang.org/doc/efficiency_guide/binaryhandling.html#matching-binaries\n\n[ssa]: https://en.wikipedia.org/wiki/Static_single_assignment_form\n\n[kernel]: http://blog.erlang.org/beam-compiler-history#r6b-enter-kernel-erlang\n\n[beam_bsm]: https://github.com/erlang/otp/blob/2e40d8d1c51ad1c3d3750490ecac6b290233f085/lib/compiler/src/beam_bsm.erl\n\n[software_smoke_testing]: https://en.wikipedia.org/wiki/Smoke_testing_(software)\n\n[beam_kernel_to_ssa_finalize]: https://github.com/erlang/otp/blob/6bee2ac7d11668888d93ec4f93730bcae3e5fa79/lib/compiler/src/beam_kernel_to_ssa.erl#L1231\n\n[dialyzer]: http://erlang.org/doc/apps/dialyzer/index.html\n\n[linear_scan_polleto]: http://web.cs.ucla.edu/~palsberg/course/cs132/linearscan.pdf\n[linear_scan_references]: https://github.com/erlang/otp/blob/494cb3be4a98653c212d673008085bc3ea70dc7e/lib/compiler/src/beam_ssa_pre_codegen.erl#L52\n\n## The Future\n\nThe SSA-based intermediate representation provides a solid framework for\nfuture improvements of the compiler. After the merging of the pull request in\nAugust, several pull requests have already added further improvements:\n\n* [Introduce a put_tuple2 instruction][pr1947]\n\n* [Replace beam_dead with beam_ssa_dead][pr1955]\n\n* [Rewrite BSM optimizations in the new SSA-based intermediate format][pr1958]\n\n* [Clean up variable-related cruft in new SSA passes][pr1959]\n\n* [beam_validator: Use set_aliased_type in more operations][pr1960]\n\n* [Minor cleanups and bug fixes of the compiler][pr1965]\n\nHere is a list of possible further improvements that could be implemented either\nby OTP members or external contributors before OTP 22 is released:\n\n* Rewrite `sys_core_dsetel` to be SSA-based.\n\n* Rewrite the guard optimizing sub pass `guard_opt/2` in `v3_kernel`\n  to an SSA-based optimization pass.\n\n* Rewrite `beam_trim`. It would probably have to be a part of `beam_ssa_codegen`.\n\n* Optimize `switch` branches. If two branches jump to blocks\n  that do the same thing, let both branches jump to the same\n  block. `beam_jump` does this kind of optimization, but doing it\n  earlier in the SSA representation could speed up compilation of functions\n  with many clauses.\n\n* Get rid of the `beam_utils` module, especially the `is_killed()` and\n  `is_not_used()` family of functions. The functions in `beam_utils`\n  used by `beam_jump` could be moved into `beam_jump`.\n\n* Rewrite `beam_bs` to be SSA-based. This rewrite might not improve\n  the generated code, but it might speed up compilation of modules\n  with heavy use of the binary syntax.\n\n[pr1947]: https://github.com/erlang/otp/pull/1947\n[pr1955]: https://github.com/erlang/otp/pull/1955\n[pr1959]: https://github.com/erlang/otp/pull/1959\n[pr1960]: https://github.com/erlang/otp/pull/1960\n[pr1965]: https://github.com/erlang/otp/pull/1965\n"},{"id":"digging-deeper-in-ssa","title":"Digging deeper in SSA","author":"Björn Gustavsson","excerpt":"\nThis blog post continues the exploration of the [new SSA-based\nintermediate representation][pr1935] through multiple examples. Make\nsure to read the [Introduction to SSA][prev] if you missed it.","article_date":1537401600000,"tags":["compiler BEAM"],"frontmatter":{"layout":"post","title":"Digging deeper in SSA","tags":"compiler BEAM","author":"Björn Gustavsson"},"content":"\nThis blog post continues the exploration of the [new SSA-based\nintermediate representation][pr1935] through multiple examples. Make\nsure to read the [Introduction to SSA][prev] if you missed it.\n\n## Calling a BIF that may fail\n\nThe first example calls a guard BIF that may fail with\nan exception:\n\n```erlang\nelement_body(T) -\u003e\n    element(2, T).\n```\n\nThe (optimized) SSA code looks like this:\n\n```\nfunction blog:element_body(_0) {\n0:\n  %% blog.erl:5\n  _1 = bif:element literal 2, _0\n  @ssa_bool = succeeded _1\n  br @ssa_bool, label 3, label 1\n\n3:\n  ret _1\n\n1:\n  @ssa_ret = call remote (literal erlang):(literal error)/1, literal badarg\n  ret @ssa_ret\n}\n```\n\nLet's go through the code a few lines at a time:\n\n```\n  %% blog.erl:5\n  _1 = bif:element literal 2, _0\n  @ssa_bool = succeeded _1\n```\n\nThe `bif:element` instruction calls the guard BIF `element/2`, assigning\nthe value to the variable `_1` if the call is successful.\n\nWhat if the call is not successful?\n\nThe `succeeded _1` instruction tests whether the previous instruction\nassigning to `_1` was successful. `true` will be assigned to `@ssa_bool`\nif the second element of the tuple was successfully fetched from the tuple,\nand `false` will be assigned otherwise.\n\n```\n  br @ssa_bool, label 3, label 1\n```\n\nThe `br` instruction tests whether `@ssa_bool` is `true`. If `true`, execution\ncontinues at block 3, which returns the value of the second element from the\ntuple. If `false`, execution continues at block 1.\n\nIt was mentioned in the [previous blog post][prev] that block 1 is a\nspecial block that the SSA code generator always emits. In the\nprevious examples, it was never referenced and therefore removed by\none of the optimization passes.\n\nIn this example, it is used as the target when the call to `element/2` fails.\n\nThe BEAM code generator treats references to block 1 specially. Here follows the\nBEAM code for the function. As usual, I have omitted the function header.\n\n      %% Block 0.\n      {line,[{location,\"blog.erl\",5}]}.\n      {bif,element,{f,0},[{integer,2},{x,0}],{x,0}}.\n      return.\n\nNote that no code has been generated for block 1.\n\nThe `line` instructions gives the file name and line number of the source file.\nIt will be used in the stack backtrace if the following instruction fails.\n\nThe `bif` instruction calls the given guard BIF, `element/2` in this case.\nThe `{f,0}` operand gives the action to take if the `element/2` fails. The number\n`0` is a special case, meaning that a `badarg` exception should be raised if the\ncall of `element/2` fails.\n\n## A failing BIF call in a guard\n\nIn the next example, `element/2` is called in a guard:\n\n```erlang\nelement_guard(T) when element(2, T) =:= true -\u003e\n    ok;\nelement_guard(_) -\u003e\n    error.\n```\n\nThe SSA code looks like this:\n\n```\nfunction blog:element_guard(_0) {\n0:\n  %% blog.erl:7\n  _1 = bif:element literal 2, _0\n  @ssa_bool = succeeded _1\n  br @ssa_bool, label 4, label 3\n\n4:\n  @ssa_bool:5 = bif:'=:=' _1, literal true\n  br @ssa_bool:5, label 6, label 3\n\n6:\n  ret literal ok\n\n3:\n  ret literal error\n}\n```\n\nThe first two instructions in block 0 are the same as in the previous\nexample.  The `br` instruction has different labels, though. The\nfailure label refers to block 3, which returns the value `error`. The\nsuccess label continues execution at block 4.\n\n```\n4:\n  @ssa_bool:5 = bif:'=:=' _1, literal true\n  br @ssa_bool:5, label 6, label 3\n```\n\nBlock 4 is the translation of `=:= true` part of the Erlang code.\nIf the second element in the tuple is equal to `true`, execution\ncontinues at block 6, which returns the value `ok`. Otherwise\nexecution continues at block 3, which returns the value `error`.\n\nHere is the BEAM code:\n\n      {bif,element,{f,5},[{integer,2},{x,0}],{x,0}}.\n      {test,is_eq_exact,{f,5},[{x,0},{atom,true}]}.\n      {move,{atom,ok},{x,0}}.\n      return.\n    {label,5}.\n      {move,{atom,error},{x,0}}.\n      return.\n\nIn the `bif` instruction, `{f,5}` means that execution should\ncontinue at label 5 if the `element/2` call fails. Otherwise\nexecution will continue at the next instruction.\n\n## Our first case\n\nHere is the next example:\n\n```erlang\ncase1(X) -\u003e\n    case X of\n        1 -\u003e a;\n        2 -\u003e b;\n        _ -\u003e c\n    end.\n```\n\nTranslated to SSA code:\n\n```\nfunction blog:case1(_0) {\n0:\n  switch _0, label 3, [ { literal 2, label 5 }, { literal 1, label 4 } ]\n\n4:\n  ret literal a\n\n5:\n  ret literal b\n\n3:\n  ret literal c\n}\n```\n\nThe `switch` instruction is a multi-way branch to one of any number of\nother blocks, based on the value of a variable. In this example, it\nbranches based on the value of the variable `_0`. If `_0` is equal\nto 2, execution continues at block 5. If `_0` is equal to 1,\nexecution continues at block 4. If the value is not equal to any\nof the values in the switch list, execution continues at the block\nreferred to by the failure label, in this example block 3.\n\nThe BEAM code looks like this:\n\n      {select_val,{x,0},{f,10},{list,[{integer,2},{f,9},{integer,1},{f,8}]}}.\n    {label,8}.\n      {move,{atom,a},{x,0}}.\n      return.\n    {label,9}.\n      {move,{atom,b},{x,0}}.\n      return.\n    {label,10}.\n      {move,{atom,c},{x,0}}.\n      return.\n\n## Terminators\n\nAs mentioned in the [previous blog post][prev], the last instruction in a block is\ncalled a *terminator*. A terminator either returns from the function or transfers\ncontrol to another block. With the introduction of `switch`, the terminator story\nis complete. To summarize, a block can end in one of the following terminators:\n\n* `ret` to return a value from the function.\n\n* `br` to either branch to another block (one-way branch), or branch to one\nof two possible other blocks based on a variable (two-way branch).\n\n* `switch` to branch to one of any number of other blocks.\n\n## Another case\n\nHere is a slightly different example:\n\n```erlang\ncase2(X) -\u003e\n    case X of\n        1 -\u003e a;\n        2 -\u003e b;\n        3 -\u003e c\n    end.\n```\n\nIn this case, `X` must be one of the integers 1, 2, or 3. Otherwise, there will\nbe a `{case_clause,X}` exception. Here is the SSA code:\n\n```\nfunction blog:case2(_0) {\n0:\n  switch _0, label 3, [ { literal 3, label 6 }, { literal 2, label 5 }, { literal 1, label 4 } ]\n\n4:\n  ret literal a\n\n5:\n  ret literal b\n\n6:\n  ret literal c\n\n3:\n  _2 = put_tuple literal case_clause, _0\n\n  %% blog.erl:20\n  @ssa_ret:7 = call remote (literal erlang):(literal error)/1, _2\n  ret @ssa_ret:7\n}\n```\n\nThe failure label for the `switch` is 3. Block 3 builds the `{case_clause,X}`\ntuple and calls `erlang:error/1`.\n\nHere is the BEAM code:\n\n      {select_val,{x,0},\n                  {f,16},\n                  {list,[{integer,3},\n                         {f,15},\n                         {integer,2},\n                         {f,14},\n                         {integer,1},\n                         {f,13}]}}.\n    {label,13}.\n      {move,{atom,a},{x,0}}.\n      return.\n    {label,14}.\n      {move,{atom,b},{x,0}}.\n      return.\n    {label,15}.\n      {move,{atom,c},{x,0}}.\n      return.\n    {label,16}.\n      {line,[{location,\"blog.erl\",20}]}.\n      {case_end,{x,0}}.\n\nThe `case_end` instruction is an optimization to save space. It is shorter than\nthe equivalent:\n\n      {test_heap,3,1}.\n      {put_tuple2,{x,0},{list,[{atom,case_clause},{x,0}]}}.\n      {line,[{location,\"blog.erl\",20}]}.\n      {call_ext_only,1,{extfunc,erlang,error,1}}.\n\n(The `put_tuple2` instruction was introduced in\n[#1947: Introduce a put_tuple2 instruction][pr1947], which was recently merged\nto `master`.)\n\n## Our final case\n\nIt's time to address the kind of `case` similar to what was teased at the\nend of the previous blog post.\n\nIn this example, the variable `Y` will be assigned different values in\neach clause of the `case`:\n\n```erlang\ncase3a(X) -\u003e\n    case X of\n        zero -\u003e\n            Y = 0;\n        something -\u003e\n            Y = X;\n        _ -\u003e\n            Y = no_idea\n    end,\n    {ok,Y}.\n```\n\nPerhaps a more common way to write this `case` would be:\n\n```erlang\ncase3b(X) -\u003e\n    Y = case X of\n            zero -\u003e 0;\n            something -\u003e X;\n            _ -\u003e no_idea\n        end,\n    {ok,Y}.\n```\n\nIn either case, the problem remains. Static Single Assignment means that each\nvariable can only be given a value once. So how can this example be translated\nto SSA code?\n\nHere follows the SSA code for `case3a/1`. The SSA code for `case3b/1` is almost\nidentical except for variable naming.\n\n```\nfunction blog:case3a(_0) {\n0:\n  switch _0, label 4, [ { literal something, label 6 }, { literal zero, label 5 } ]\n\n5:\n  br label 3\n\n6:\n  br label 3\n\n4:\n  br label 3\n\n3:\n  Y = phi { literal no_idea, 4 }, { literal 0, 5 }, { _0, 6 }\n  _7 = put_tuple literal ok, Y\n  ret _7\n}\n```\n\nLet's jump right to the interesting (and confusing) part of the code:\n\n```\n3:\n  Y = phi { literal no_idea, 4 }, { literal 0, 5 }, { _0, 6 }\n```\n\nClearly, `Y` is only given a value once, so the SSA property is\npreserved.\n\nThat's good, but exactly what is the value that is being assigned?\n\nThe name of the instruction is `phi`, which is the name of the\nGreek letter [\u0026phi;][phi]. Having an unusual name, the instruction\ndeserves to have unusual operands, too. Each operand is a pair, the\nfirst element in the pair being a value and the second element a block\nnumber of a predecessor block. The value of the `phi` node will be one\nof the values from one the pairs. But from which pair? That depends on\nthe number of the previous block that branched to the `phi` instruction.\n\nTo make that somewhat clearer, let's look at all operands:\n\n* `{ literal no_idea, 4 }`: If the number of block that executed `br label 3`\nwas 4, the value of the `phi` instruction will be the value in this pair,\nthat is, the atom `no_idea`. The failure label for the `switch` instruction\nis 4, so this pair will be chosen when `_0` does not match any of the values\nin the switch list.\n\n* `{ literal 0, 5 }`: If the number of block that executed `br label 3`\nwas 5, the value of the `phi` instruction will be the integer 0. The\n`switch` instruction will transfer control to block 5 if the value of\n`_0` is the atom `zero`.\n\n* `{ _0, 6 }`: Finally, if `_0` is the atom `something`, the `switch`\nwill transfer control to block 6, which will transfer control to\nblock 3. The value of the `phi` instruction will be the value of the\nvariable `_0`.\n\nThe concept of `phi` instructions probably feels a bit strange at\nfirst sight (and at second sight), and one might also think they\nmust be terribly inefficient.\n\nLeaving the strangeness aside, let's talk about the efficiency. `phi`\ninstructions is a fiction convenient for representing and optimizing\nthe code. When translating to BEAM code, the `phi` instructions are\neliminated.\n\nHere follows an example that is **not** SSA code, because it assigns\nthe variable `Y` three times, but gives an idea how the `phi`\ninstruction is eliminated:\n\n```\n%% Not SSA code!\nfunction blog:case3a(_0) {\n0:\n  switch _0, label 4, [ { literal something, label 6 }, { literal zero, label 5 } ]\n\n5:\n  Y := literal 0\n  br label 3\n\n6:\n  Y := _0\n  br label 3\n\n4:\n  Y := no_idea\n  br label 3\n\n3:\n  _7 = put_tuple literal ok, Y\n  ret _7\n}\n```\n\nThe BEAM code generator (`beam_ssa_codegen`) does a similar rewrite\nduring code generation.\n\nHere is the unoptimized BEAM code, slightly edited for clarity:\n\n    %% Block 0.\n    {select_val,{x,0},\n                {f,53},\n                {list,[{atom,something},{f,55},{atom,zero},{f,57}]}}.\n\n    %% Block 5.\n    {label,57}.\n      {move,{integer,0},{x,0}}.\n      {jump,{f,59}}.\n\n    %% Block 6.\n    {label,55}.\n      %% The result is already in {x,0}.\n      {jump,{f,59}}.\n\n    %% Block 4.\n    {label,53}.\n      {move,{atom,no_idea},{x,0}}.\n      {jump,{f,59}}.\n\n    %% Block 3.\n    {label,59}.\n       {test_heap,3,1}.\n       {put_tuple2,{x,0},{list,[{atom,ok},{x,0}]}}.\n       return.\n\nHere is the final BEAM code after some more optimizations:\n\n    {label,18}.\n      {select_val,{x,0},\n                  {f,20},\n                  {list,[{atom,something},{f,21},{atom,zero},{f,19}]}}.\n    {label,19}.\n      {move,{integer,0},{x,0}}.\n      {jump,{f,21}}.\n    {label,20}.\n      {move,{atom,no_idea},{x,0}}.\n    {label,21}.\n      {test_heap,3,1}.\n      {put_tuple2,{x,0},{list,[{atom,ok},{x,0}]}}.\n      return.\n\n## The cold case\n\nHere is the example from the end of the previous blog post:\n\n```erlang\nbar(X) -\u003e\n    case X of\n        none -\u003e\n            Y = 0;\n        _ -\u003e\n            Y = X\n    end,\n    Y + 1.\n```\n\nAnd here is the SSA code:\n\n```\nfunction blog:bar(_0) {\n0:\n  @ssa_bool = bif:'=:=' _0, literal none\n  br @ssa_bool, label 5, label 4\n\n5:\n  br label 3\n\n4:\n  br label 3\n\n3:\n  Y = phi { _0, 4 }, { literal 0, 5 }\n\n  %% blog.erl:52\n  _6 = bif:'+' Y, literal 1\n  @ssa_bool:6 = succeeded _6\n  br @ssa_bool:6, label 7, label 1\n\n7:\n  ret _6\n\n1:\n  @ssa_ret = call remote (literal erlang):(literal error)/1, literal badarg\n  ret @ssa_ret\n}\n```\n\nIt is left as an exercise to the reader to read and understand the code.\n\nHere is the BEAM code:\n\n    {label,28}.\n      {test,is_eq_exact,{f,29},[{x,0},{atom,none}]}.\n      {move,{integer,0},{x,0}}.\n    {label,29}.\n      {line,[{location,\"blog.erl\",52}]}.\n      {gc_bif,'+',{f,0},1,[{x,0},{integer,1}],{x,0}}.\n      return.\n\nThe `gc_bif` instruction calls a guard BIF that might need to do a\ngarbage collection. Since integers can be of essentially unlimited\nsize in Erlang, the result of `+` might not fit in a word. The\n`1` following `{f,0}` is the number of registers that must be\npreserved; in this case, only `{x,0}`.\n\n[prev]: http://blog.erlang.org/introducing-ssa/\n[pr1935]: https://github.com/erlang/otp/pull/1935\n[pr1947]: https://github.com/erlang/otp/pull/1947\n[phi]: https://en.wikipedia.org/wiki/Phi\n"},{"id":"introducing-ssa","title":"Introduction to SSA","author":"Björn Gustavsson","excerpt":"\nThis blog post is an introduction to the [new SSA-based intermediate\nrepresentation][pr1935] that has recently been merged to the `master`\nbranch in the [Erlang/OTP repository][otp]. It uses the same\nexample as in the [previous blog post][prev], first looking at the\ngenerated SSA code, and then at some optimizations.","article_date":1536105600000,"tags":["compiler BEAM"],"frontmatter":{"layout":"post","title":"Introduction to SSA","tags":"compiler BEAM","author":"Björn Gustavsson"},"content":"\nThis blog post is an introduction to the [new SSA-based intermediate\nrepresentation][pr1935] that has recently been merged to the `master`\nbranch in the [Erlang/OTP repository][otp]. It uses the same\nexample as in the [previous blog post][prev], first looking at the\ngenerated SSA code, and then at some optimizations.\n\nHere again is the example function that does the kind of tuple matching\ntypically done when matching records:\n\n```erlang\nfoo({tag,A,_,_}) -\u003e\n    {ok,A}.\n```\n\nAt the end of this blog post there will be section on [how to generate\nlisting files](#generating_listings) to inspect the code from the\ncompiler passes.\n\n## A brief introduction to the SSA intermediate format\n\nSSA stands for [Static Single Assignment][ssa]. Strictly speaking, SSA\nis the property of an [intermediate representation][intermediate] where\neach variable is assigned exactly once, and where every variable is\ndefined before it is used. In this blog post, we will use the term\n*SSA code* to refer to the new intermediate representation in the\nErlang compiler.\n\nHere is the SSA code for the `foo/1` function:\n\n```\nfunction blog:foo(_0) {\n0:\n  @ssa_bool:6 = bif:is_tuple _0\n  br @ssa_bool:6, label 7, label 3\n\n7:\n  @ssa_arity = bif:tuple_size _0\n  @ssa_bool:8 = bif:'=:=' @ssa_arity, literal 4\n  br @ssa_bool:8, label 5, label 3\n\n5:\n  _8 = get_tuple_element _0, literal 0\n  _7 = get_tuple_element _0, literal 1\n  @ssa_bool = bif:'=:=' _8, literal tag\n  br @ssa_bool, label 4, label 3\n\n4:\n  _9 = put_tuple literal ok, _7\n  ret _9\n\n3:\n  _4 = put_list _0, literal []\n\n  %% blog.erl:4\n  @ssa_ret:9 = call remote (literal erlang):(literal error)/2, literal function_clause, _4\n  ret @ssa_ret:9\n\n%% Unreachable blocks\n\n1:\n  @ssa_ret = call remote (literal erlang):(literal error)/1, literal badarg\n  ret @ssa_ret\n}\n```\n\n### A deeper look at the example\n\nWe will go through the code a few lines at the time.\n\n```\nfunction blog:foo(_0) {\n```\n\nThis is the head of the function. It gives the module name (`blog`),\nfunction name (`foo`), and the arguments (the single variable _0).\n\nVariables named as `_` followed by an integer are inherited from\n[Core Erlang][core_erlang]. In OTP 22, variable names in Core Erlang\nare integers (to avoid filling the atom table when compiling huge\nfunctions).\n\n```\n0:\n```\n\nFollowing the function head is one or more *blocks* (sometimes called\n*nodes*).  A integer followed by a colon gives the number of the block\nthat follows.\n\nThe block number `0` is special.  It is the first block that will be\nexecuted in this function.\n\n```\n  @ssa_bool:6 = bif:is_tuple _0\n```\n\nHere is the first real instruction! All instructions have this\nbasic format. First there is a variable, followed by `=`, followed\nby the name of the instruction, followed by its operands.\n\nThe variable to the left, `@ssa_bool:6` in this example, will be\nassigned the value of the expression to right of the `=`.\n\nEach variable can only be assigned once, just as in Erlang. The name\nof this variable consists of two parts, the base part `@ssa_bool` and\nthe numeric suffix `6`. Whenever the base name itself is not unique,\nthe numeric suffix is added to make the name unique.\n\nThe instruction name is `bif:is_tuple`. This is one of the\ninstructions that use a two-part name.  The `bif` prefix means that\nthe second part must be the name of an Erlang guard BIF, in this case\n`is_tuple/1`.\n\nFollowing the name of the instruction is the operand `_0`, which is\nthe name of the function argument for the `foo/1` function.\n\nThus, this instruction will call `is_tuple/1` and assign the result\n(either `true` or `false`) to `ssa_bool:6`.\n\n```\n  br @ssa_bool:6, label 7, label 3\n```\n\nThis is the last instruction of block 0. Instructions at the end\nof a block are called *terminators* and they have a different format\ncompared to instructions in the interior of a block. Terminators\neither transfer control to another block or returns from the\nfunction.\n\n`br` transfers control to another block. The first operand is a\nvariable, whose value must be `true` or `false`. If the value of\n`ssa_bool:6` is `true`, the second operand (`label 7`) is used as the\nblock number for the block where execution will continue. In this\nexample: block 7. Similarly, if the value of `ssa_bool:6` is `false`,\nthe third operand (`label 3`) will be used to transfer control to\nblock 3.\n\n\n```\n7:\n  @ssa_arity = bif:tuple_size _0\n```\n\nThis is the beginning of block 7. This block will be executed\nif `_0` was found to be a tuple. `@ssa_arity` will be assigned\nthe value of the call `tuple_size(_0)`.\n\nNote that `@ssa_arity` does not have a numeric suffix, since there\nis no other variable in this function having the same base name.\n\n```\n  @ssa_bool:8 = bif:'=:=' @ssa_arity, literal 4\n```\n\nHere `bif:=:=` compares `@ssa_arity` and `4` and assigns the\nresult to `@ssa_bool:8`. (Note that `=:=` is a guard BIF\nin Erlang; it is allowed but unusual to write\n`erlang:'=:='(Arity, 4)` instead of `Arity =:= 4`.)\n\n```\n  br @ssa_bool:8, label 5, label 3\n```\n\nHere is another `br` instruction. It will transfer control to\nblock 5 if `@ssa_bool:8` is `true` (that is, if `@ssa_arity`\nis equal to 4), and to block 3 otherwise.\n\n```\n5:\n  _8 = get_tuple_element _0, literal 0\n  _7 = get_tuple_element _0, literal 1\n```\n\nBlock 5 is executed if `_0` has been found to be a tuple of\nsize 4. The `get_tuple_element` instruction extracts an element\nfrom a tuple at the given position. The position is zero-based.\n\nThe `get_tuple_element` instruction in SSA in named after the\nBEAM instruction with the same name:\n\n    {get_tuple_element,{x,0},0,{x,1}}.\n\nNotice the similarity between the SSA instruction and the BEAM\ninstruction. The SSA form uses variables instead of registers,\nand the destination variable is to the left of the `=` as in\nall SSA instructions.\n\n```\n  @ssa_bool = bif:'=:=' _8, literal tag\n  br @ssa_bool, label 4, label 3\n```\n\nHere comes the test that the first element of the tuple\nis equal to the atom `tag`. If the first element is `tag`,\nexecution continues at block 4, otherwise at block 3.\n\n```\n4:\n  _9 = put_tuple literal ok, _7\n```\n\nThis instruction constructs the `{ok,A}` tuple. The variable `_7`\ncontains the second element of the tuple.\n\nThe `put_tuple` instruction takes any number of operands and\nconstructs a tuple from them. The result is assigned to the \nvariable `_9`.\n\nIn this case, the `put_tuple` instruction in SSA does more than the\ncorresponding BEAM instruction. To construct the same tuple, three\nBEAM instructions are needed:\n\n    {put_tuple,2,{x,0}}.\n    {put,{atom,ok}}.\n    {put,{x,2}}.\n\nHaving a single instruction to construct a tuple instead of\nthe multiple BEAM instructions simplifies optimizations\nimmensely. Also note that SSA has no equivalent of the `test_heap`\ninstruction that caused so much trouble in the [previous blog post][prev].\n\n```\n  ret _9\n```\n\n`ret` is another terminator instruction. `ret` returns from the function\nwith the value of variable `_9` as the return value.\n\nThat concludes the successful path through the function.\n\n```\n3:\n  _4 = put_list _0, literal []\n\n  %% blog.erl:4\n  @ssa_ret:9 = call remote (literal erlang):(literal error)/2, literal function_clause, _4\n  ret @ssa_ret:9\n```\n\nThis block is executed if any of `br` instructions in the previous blocks\nwere given the value `false`, that is if the function argument was not a tuple or\nhad the wrong size or wrong first element.\n\nThe comment line (starting with `%%`) has been added by the pretty printer based on\nannotation in the `call` instruction.\n\nIt is left as an exercise to the reader to figure out exactly what the\ninstructions in the block do.  As a hint, here is the code for the\nblock translated back to Erlang code:\n\n```erlang\nerlang:error(function_clause, [_0]).\n```\n\nMoving on to the part of the function that is not executed at all:\n\n```\n%% Unreachable blocks\n\n1:\n  @ssa_ret = call remote (literal erlang):(literal error)/1, literal badarg\n  ret @ssa_ret\n```\n\nThe comment (`Unreachable blocks`) was added by the pretty printer to\nindicate that the blocks that follow can never be executed, because no\nblock will ever branch to them.\n\nWhy is there an unreachable block?\n\nBlock 1 is a special block. It generates a `badarg` exeception, just\nas a call to `error:error(badarg)`. The SSA code generator always\nincludes block 1 with the exact same instructions in every function,\neven if it never actually used.\n\nWe will not go into details about the purpose of this block in this\nblog post (but we will see how it is used in the next blog post).\n\n## Optimizing the code\n\nNow it's time to see how the SSA code can be optimized. The SSA\noptimizations follow the same idea as the [Core Erlang\noptimizations][sys_core_fold] of using many simple optimizations\nworking together rather than a few complicated optimizations.\n\nHere is the code for the function again as it looks after running a few preliminary\noptimization passes:\n\n```\nfunction blog:foo(_0) {\n0:\n  @ssa_bool:6 = bif:is_tuple _0\n  br @ssa_bool:6, label 7, label 3\n\n7:\n  @ssa_arity = bif:tuple_size _0\n  @ssa_bool:8 = bif:'=:=' @ssa_arity, literal 4\n  br @ssa_bool:8, label 5, label 3\n\n5:\n  _8 = get_tuple_element _0, literal 0\n  _7 = get_tuple_element _0, literal 1\n  @ssa_bool = bif:'=:=' _8, literal tag\n  br @ssa_bool, label 4, label 3\n\n4:\n  _9 = put_tuple literal ok, _7\n  ret _9\n\n3:\n  _4 = put_list _0, literal []\n  br label 10\n\n10:\n  %% blog.erl:4\n  @ssa_ret:9 = call remote (literal erlang):(literal error)/2, literal function_clause, _4\n  ret @ssa_ret:9\n}\n```\n\nThe unreachable block 1 has been deleted.\n\nA pass that [splits blocks][ssa_opt_split_blocks] before certain\ninstructions has also been run (in order to make the passes for\n[sinking `get_tuple_element` instructions][ssa_opt_sink] and [swapping\n`element/2` calls][ssa_opt_element] more effective). This pass has\nsplit block 3 into two blocks. At the end of block 3 there is a\nvariant of the `br` terminator that we have not seen before.  `br label\n10` unconditionally continues the execution at block 10.\n\nThe first interesting optimization for our example is the\n[ssa_opt_record] optimizations, which attempts to translate tuple\nmatching instructions with an `is_tagged_tuple` instruction.\nHere is the part of the code that will be optimized:\n\n\u003cpre class=\"highlight\"\u003e\n    0:\n      @ssa_bool:6 = bif:is_tuple _0\n      br @ssa_bool:6, label 7, \u003cb\u003elabel 3\u003c/b\u003e\n\n    7:\n      @ssa_arity = bif:tuple_size _0\n      @ssa_bool:8 = bif:'=:=' @ssa_arity, \u003cb\u003eliteral 4\u003c/b\u003e\n      br @ssa_bool:8, label 5, \u003cb\u003elabel 3\u003c/b\u003e\n\n    5:\n      _8 = get_tuple_element _0, literal 0\n      @ssa_bool = bif:'=:=' _8, \u003cb\u003eliteral tag\u003c/b\u003e\n      br @ssa_bool, label 4, \u003cb\u003elabel 3\u003c/b\u003e\n\u003c/pre\u003e\n\nThe optimization is done in two stages. First the code is analyzed to find out\nwhether the optimization is applicable. There must be a test for a tuple of\na certain size (4 in this example) and with a certain first element\n(`tag` in this example). Furthermore all failure labels must be the same.\n\nIf all conditions are fulfilled, the optimization is done in the second stage.\nHere is the code again, with the optimized part of the code highlighted:\n\n\u003cpre class=\"highlight\"\u003e\n    function blog:foo(_0) {\n    0:\n      @ssa_bool:6 = \u003cb\u003eis_tagged_tuple _0, literal 4, literal tag\u003c/b\u003e\n      br @ssa_bool:6, label 7, label 3\n\n    7:\n      @ssa_arity = bif:tuple_size _0\n      @ssa_bool:8 = bif:'=:=' @ssa_arity, literal 4\n      br @ssa_bool:8, label 5, label 3\n\n    5:\n      _8 = get_tuple_element _0, literal 0\n      @ssa_bool = bif:'=:=' _8, literal tag\n      br @ssa_bool, label 4, label 3\n\n    4:\n      _7 = get_tuple_element _0, literal 1\n      _9 = put_tuple literal ok, _7\n      ret _9\n\n    3:\n      _4 = put_list _0, literal []\n      br label 10\n\n    10:\n      %% blog.erl:4\n      @ssa_ret:9 = call remote (literal erlang):(literal error)/2, literal function_clause, _4\n      ret @ssa_ret:9\n    }\n\u003c/pre\u003e\n\nYes, it really is this simple, but so far it is more of a\n[pessimization] than an optimization, because the `bif:is_tuple`\ninstruction has been replaced with the more expensive\n`is_tagged_tuple` instruction.\n\nThe next optimization is a type analysis pass, which is implemented in\nthe module [beam_ssa_type]. Here is the code after running `beam_ssa_type`:\n\n\u003cpre class=\"highlight\"\u003e\n    function blog:foo(_0) {\n    0:\n      @ssa_bool:6 = is_tagged_tuple _0, literal 4, literal tag\n      br @ssa_bool:6, label 7, label 3\n\n    7:\n      @ssa_arity = bif:tuple_size _0\n      @ssa_bool:8 = bif:'=:=' \u003cb\u003eliteral 4, literal 4\u003c/b\u003e\n      \u003cb\u003ebr label 5\u003c/b\u003e\n\n    5:\n      _8 = get_tuple_element _0, literal 0\n      @ssa_bool = bif:'=:=' \u003cb\u003eliteral tag, literal tag\u003c/b\u003e\n      \u003cb\u003ebr label 4\u003c/b\u003e\n\n    4:\n      _7 = get_tuple_element _0, literal 1\n      _9 = put_tuple literal ok, _7\n      ret _9\n\n    3:\n      _4 = put_list _0, literal []\n      br label 10\n\n    10:\n      %% blog.erl:4\n      @ssa_ret:9 = call remote (literal erlang):(literal error)/2, literal function_clause, _4\n      ret @ssa_ret:9\n    }\n\u003c/pre\u003e\n\n`beam_ssa_type` analyzes the code in execution order, remembering the\ntype of each variable seen. Based on the types, `beam_ssa_type` replaces\nvariables with known values with the values themselves.\n\nTwo of the conditional branchs have been converted to unconditional\nbranches.\n\nThe next optimization is [liveness analysis][ssa_opt_live]. The code\nis scanned in reverse execution order, and if an expression is never\nused, and has no observable side effect, it can be deleted. The\nhighlighted instructions in the code that follows was identified by\nthe liveness analysis pass as unused:\n\n\u003cpre class=\"highlight\"\u003e\n    function blog:foo(_0) {\n    0:\n      @ssa_bool:6 = is_tagged_tuple _0, literal 4, literal tag\n      br @ssa_bool:6, label 7, label 3\n\n    7:\n      \u003cb\u003e@ssa_arity = bif:tuple_size _0\u003c/b\u003e\n      \u003cb\u003e@ssa_bool:8 = bif:'=:=' literal 4, literal 4\u003c/b\u003e\n      br label 5\n\n    5:\n      \u003cb\u003e_8 = get_tuple_element _0, literal 0\u003c/b\u003e\n      \u003cb\u003e@ssa_bool = bif:'=:=' literal tag, literal tag\u003c/b\u003e\n      br label 4\n\n    4:\n      _7 = get_tuple_element _0, literal 1\n      _9 = put_tuple literal ok, _7\n      ret _9\n\n    3:\n      _4 = put_list _0, literal []\n      br label 10\n\n    10:\n      %% blog.erl:4\n      @ssa_ret:9 = call remote (literal erlang):(literal error)/2, literal function_clause, _4\n      ret @ssa_ret:9\n    }\n\u003c/pre\u003e\n\nBecause those expressions don't have any side effects, they can be deleted:\n\n\u003cpre class=\"highlight\"\u003e\n    function blog:foo(_0) {\n    0:\n      @ssa_bool:6 = is_tagged_tuple _0, literal 4, literal tag\n      br @ssa_bool:6, label 7, label 3\n\n    7:\n      br label 5\n\n    5:\n      br label 4\n\n    4:\n      _7 = get_tuple_element _0, literal 1\n      _9 = put_tuple literal ok, _7\n      ret _9\n\n    3:\n      _4 = put_list _0, literal []\n      br label 10\n\n    10:\n      %% blog.erl:4\n      @ssa_ret:9 = call remote (literal erlang):(literal error)/2, literal function_clause, _4\n      ret @ssa_ret:9\n    }\n\u003c/pre\u003e\n\nAfter running a pass that [merges blocks][ssa_opt_merge_blocks], the final code\nlooks like this:\n\n```\nfunction blog:foo(_0) {\n0:\n  @ssa_bool:6 = is_tagged_tuple _0, literal 4, literal tag\n  br @ssa_bool:6, label 5, label 3\n\n5:\n  _7 = get_tuple_element _0, literal 1\n  _9 = put_tuple literal ok, _7\n  ret _9\n\n3:\n  _4 = put_list _0, literal []\n\n  %% blog.erl:4\n  @ssa_ret:9 = call remote (literal erlang):(literal error)/2, literal function_clause, _4\n  ret @ssa_ret:9\n}\n```\n\nNow it's time to look at the resulting BEAM code. Here is the successful part of the\nfunction:\n\n    %% Block 0.\n    {test,is_tagged_tuple,{f,1},[{x,0},4,{atom,tag}]}.\n\n    %% Block 5.\n    {test_heap,3,1}.\n    {get_tuple_element,{x,0},1,{x,0}}.\n    {put_tuple,2,{x,1}}.\n    {put,{atom,ok}}.\n    {put,{x,0}}.\n    {move,{x,1},{x,0}}.\n    return.\n\nSince register allocation was done after the `is_tagged_tuple`\noptimization, the `get_tuple_instruction` will extract the second\nelement of the tuple to the first available register, namely\n`{x,0}`. That avoids any potential problem of registers being\nundefined at a `test_heap` instruction. The `put_tuple` instruction\nwill put the built tuple into `{x,1}` since the following\n`{put,{x,0}}` instruction still needs the contents of `{x,0}`. To\nreturn the built tuple, the `{move,{x,1},{x,0}}` instruction just\nbefore the `return` instruction copies the contents of `{x,1}`\nto `{x,0}`.\n\nIt happens that for this particular example, the OTP 21 compiler will produce\nslightly better code:\n\n\u003cpre class=\"highlight\"\u003e\n    {test,is_tagged_tuple,{f,1},[{x,0},4,{atom,tag}]}.\n    {test_heap,3,1}.\n    {get_tuple_element,{x,0},1,\u003cb\u003e{x,2}\u003c/b\u003e}.\n    {put_tuple,2,\u003cb\u003e{x,0}\u003c/b\u003e}.\n    {put,{atom,ok}}.\n    {put,\u003cb\u003e{x,2}\u003c/b\u003e}.\n    return.\n\u003c/pre\u003e\n\n(The tuple can be built to `{x,0}` directly, avoiding the `move`\ninstruction before the `return`.)\n\n## Getting rid of the `move` instruction\n\nPerhaps I should have chosen another example to avoid revealing that\nthe SSA-based compiler sometimes produces worse code than the old\ncompiler.\n\nAnyway, now that the secret is out, let's see what can been done\nabout that extra `move` instruction.\n\nLet's look at another example:\n\n```erlang\nmake_tuple(A) -\u003e\n    {ok,A}.\n```\n\nThe BEAM code produced by either the compiler in OTP 21\nor the new SSA-based compiler looks like this:\n\n    {test_heap,3,1}.\n    {put_tuple,2,{x,1}}.\n    {put,{atom,ok}}.\n    {put,{x,0}}.\n    {move,{x,1},{x,0}}.\n    return.\n\nClearly, the way the tuple building instructions work, it would be\nimpossible to avoid the `move` instruction. When building a tuple, the\ndestination register for the built tuple must not be the same\nas one of the source registers. It seems that we will need\nbetter instructions for constructing tuples if we are to avoid\nthe `move` instruction.\n\nThe problem doesn't exist when building a list:\n\n    %% build_list(A) -\u003e [A].\n    {test_heap,2,1}.\n    {put_list,{x,0},nil,{x,0}}.\n    return.\n\nThe `put_list` instruction can safely place the built list into the\nsame register as either of the source registers.\n\nIntroducing a new `put_tuple2` instruction that builds a tuple in a\nsingle instruction, the `move` instruction can be eliminated:\n\n    {test_heap,3,1}.\n    {put_tuple2,{x,0},{list,[{atom,ok},{x,0}]}}.\n    return.\n\nAt the time of writing, the implementation of `put_tuple2` has not yet\nbeen merged to the `master` branch, but can be found in [#1947:\nIntroduce a put_tuple2 instruction][pr1947].\n\n## Next time\n\nAs we have seen, a variable in the SSA code can only be assigned\nonce (just as in Erlang). So how can the following code be\ntranslated to SSA code?\n\n```\nbar(X) -\u003e\n    case X of\n        none -\u003e\n            Y = 0;\n        _ -\u003e\n            Y = X\n    end,\n    Y + 1.\n```\n\n## \u003ca name=\"generating_listings\"\u003e\u003c/a\u003eHow to generate listing files\n\nTo generate the unoptimized SSA code for a module, use the `dssa` option:\n\n```\nerlc +dssa blog.erl\n```\n\nThe SSA code will be pretty printed into the file `blog.ssa`.\n\nUse the `dssaopt` option to generate the optimized SSA code, printing\nit to the file `blog.ssaopt`.\n\n```\nerlc +dssaopt blog.erl\n```\n\nTo see how the SSA code looked when not all optimization passes had been\nrun, I used variations of the following command line\n\n```\nerlc +dssaopt +no_ssa_opt_type +no_ssa_opt_live +no_ssa_opt_merge_blocks blog.erl\n```\n\nThose options are intentionally not documented. Skipping optimization\nis only intended for debugging or exploring how the optimization\npasses work. Skipping some optimizations passes that are actually\nmandatory will crash the compiler.\n\nTo find the names of the options for skipping passes, see the [list of\nsub passes of `beam_ssa_opt`][passes] and add `no_` to the name of the\npass.\n\nTo generate `blog.S` with the BEAM code, use the `-S` option:\n\n```\nerlc -S blog.erl\n```\n\nTo skip all SSA optimizations, use the `no_ssa_opt` option:\n\n```\nerlc +no_ssa_opt -S blog.erl\n```\n\n[pr1935]: https://github.com/erlang/otp/pull/1935\n[pr1947]: https://github.com/erlang/otp/pull/1947\n[otp]: https://github.com/erlang/otp\n[intermediate]: https://en.wikipedia.org/wiki/Intermediate_representation\n[ssa]: https://en.wikipedia.org/wiki/Static_single_assignment_form\n[prev]: http://blog.erlang.org/opt-traps-and-pitfalls/\n[sys_core_fold]: http://blog.erlang.org/core-erlang-optimizations/\n[pessimization]: https://stackoverflow.com/questions/32618848/what-is-pessimization\n[core_erlang]: http://blog.erlang.org/core-erlang-by-example/\n\n[passes]: https://github.com/erlang/otp/blob/869537a9bf799c8d12fc46c2b413e532d6e3b10c/lib/compiler/src/beam_ssa_opt.erl#L49\n[ssa_opt_split_blocks]: https://github.com/erlang/otp/blob/869537a9bf799c8d12fc46c2b413e532d6e3b10c/lib/compiler/src/beam_ssa_opt.erl#L100\n[ssa_opt_element]: https://github.com/erlang/otp/blob/869537a9bf799c8d12fc46c2b413e532d6e3b10c/lib/compiler/src/beam_ssa_opt.erl#L119\n[ssa_opt_record]: https://github.com/erlang/otp/blob/869537a9bf799c8d12fc46c2b413e532d6e3b10c/lib/compiler/src/beam_ssa_opt.erl#L194\n[ssa_opt_live]: https://github.com/erlang/otp/blob/869537a9bf799c8d12fc46c2b413e532d6e3b10c/lib/compiler/src/beam_ssa_opt.erl#L683\n[ssa_opt_merge_blocks]: https://github.com/erlang/otp/blob/869537a9bf799c8d12fc46c2b413e532d6e3b10c/lib/compiler/src/beam_ssa_opt.erl#L976\n[ssa_opt_sink]: https://github.com/erlang/otp/blob/869537a9bf799c8d12fc46c2b413e532d6e3b10c/lib/compiler/src/beam_ssa_opt.erl#L1025\n\n[beam_ssa_type]: https://github.com/erlang/otp/blob/869537a9bf799c8d12fc46c2b413e532d6e3b10c/lib/compiler/src/beam_ssa_type.erl\n"},{"id":"opt-traps-and-pitfalls","title":"Optimization Traps and Pitfalls","author":"Björn Gustavsson","excerpt":"\nBack after the summer holidays, this blog will now change tracks and\nstart a series of blog posts about Static Single Assignment (SSA).\nThis first installment will set the scene for the posts that follow by\nlooking at the traps and pitfalls one can fall into when trying to\noptimize BEAM assembly code.","article_date":1535068800000,"tags":["compiler BEAM"],"frontmatter":{"layout":"post","title":"Optimization Traps and Pitfalls","tags":"compiler BEAM","author":"Björn Gustavsson"},"content":"\nBack after the summer holidays, this blog will now change tracks and\nstart a series of blog posts about Static Single Assignment (SSA).\nThis first installment will set the scene for the posts that follow by\nlooking at the traps and pitfalls one can fall into when trying to\noptimize BEAM assembly code.\n\n## A brief introduction to BEAM assembly language\n\nWe will look at the BEAM code for the following function:\n\n```erlang\nfoo({tag,A,_,_}) -\u003e\n    {ok,A}.\n```\n\nThe (unoptimized) BEAM code looks like this:\n\n```\n{function, foo, 1, 2}.\n  {label,1}.\n    {line,[{location,\"ex1.erl\",4}]}.\n    {func_info,{atom,ex1},{atom,foo},1}.\n  {label,2}.\n    {test,is_tuple,{f,3},[{x,0}]}.\n    {test,test_arity,{f,3},[{x,0},4]}.\n    {get_tuple_element,{x,0},0,{x,1}}.\n    {get_tuple_element,{x,0},1,{x,2}}.\n    {test,is_eq_exact,{f,3},[{x,1},{atom,tag}]}.\n    {test_heap,3,3}.\n    {put_tuple,2,{x,0}}.\n    {put,{atom,ok}}.\n    {put,{x,2}}.\n    return.\n  {label,3}.\n    {test_heap,2,1}.\n    {put_list,{x,0},nil,{x,1}}.\n    {move,{atom,function_clause},{x,0}}.\n    {line,[{location,\"ex1.erl\",4}]}.\n    {call_ext_only,2,{extfunc,erlang,error,2}}.\n```\n\nWe will concentrate on the part of the code that does\nthe actual work:\n\n```\n    {test,is_tuple,{f,3},[{x,0}]}.\n    {test,test_arity,{f,3},[{x,0},4]}.\n    {get_tuple_element,{x,0},0,{x,1}}.\n    {get_tuple_element,{x,0},1,{x,2}}.\n    {test,is_eq_exact,{f,3},[{x,1},{atom,tag}]}.\n    {test_heap,3,3}.\n    {put_tuple,2,{x,0}}.\n    {put,{atom,ok}}.\n    {put,{x,2}}.\n    return.\n  {label,3}.\n    %% Cause a function_clause exception.\n```\n\nWe will now explain what each instruction does.\n\n```\n    {test,is_tuple,{f,3},[{x,0}]}.\n```\n\n`test` instructions test whether a condition is true. If it\nis, the next instruction will be executed. Otherwise,\nthere will be a branch to the failure label.\n\nThe condition tested by this instruction is `is_tuple`, that is\nwhether its operand is a tuple.  The operand is `{x,0}`, which is the\nregister for the first argument for the function. If `{x,0}` does not\ncontain a tuple, execution will continue at the failure label. `{f,3}`\nmeans that that the failure label is `3`. The code at label `3` will\ncause a `function_clause` exception.\n\n```\n    {test,test_arity,{f,3},[{x,0},4]}.\n```\n\nThe `test_arity` instruction tests whether the first operand (which\nmust be a tuple) has the size given by the second operand. The first\noperand is `{x,0}` and the second operand is `4`. The failure label is\nthe same as for the previous instruction.\n\n```\n    {get_tuple_element,{x,0},0,{x,1}}.\n    {get_tuple_element,{x,0},1,{x,2}}.\n```\n\nWhen those two instructions are executed, the previous instructions have\nestablished that `{x,0}` contains a tuple of arity 4.\n`get_tuple_element` takes three operands. The first is the source\ntuple, `{x,0}`, the second is the **zero-based** index into the tuple,\nand the third operand is the register into which the element from the\ntuple should be stored. Note that there is no failure label because it\ncannot fail.\n\nSo the first `get_tuple_element` instruction fetches the first element\nof the tuple and stores it in the `{x,1}` register, and the second\n`get_tuple_element` instruction fetches the second element and stores\nit into the `{x,2}` register.\n\n```\n    {test,is_eq_exact,{f,3},[{x,1},{atom,tag}]}.\n```\n\n`is_eq_exact` is again a `test` instruction. It tests\nwhether the contents of `{x,1}` is exactly equal (that is,\n`=:=`) to the atom `tag`. If not, execution will continue\nat the failure label `3`.\n\nThat concludes the function header. The next instruction is in the\nbody of the function that will build the `{ok,A}` tuple:\n\n```\n    {test_heap,3,3}.\n```\n\nThe `test_heap` instruction ensures that there is sufficient free\nspace on the heap to construct a term. The first operand (the first\n`3`) says that the following instructions will need 3 words on the\nheap. A tuple has a header word, followed by the elements, so a tuple\nwith 2 elements needs 3 heap words in total.\n\nIf there is not sufficient room on the heap, the `test_heap`\ninstruction will do a garbage collection to find some fresh heap\nspace. The second operand (the second `3`) is the number of `x`\nregisters that have values that must be preserved during garbage\ncollection. The `3` means that `{x,0}`, `{x,1}`, and `{x,2}` have live\nvalues.\n\n```\n    {put_tuple,2,{x,0}}.\n    {put,{atom,ok}}.\n    {put,{x,2}}.\n```\n\nThose three instructions build the tuple, putting a tagged\npointer to the tuple in `{x,0}`.\n\n```\n    return.\n```\n\n`return` returns from the function. The return value is the\nvalue in `{x,0}`.\n\n## Optimizing this code\n\nTesting that a term is a tuple of a certain size with a specific atom\nas the first element is a common operation (think records). Therefore\nthe BEAM machine has an `is_tagged_tuple` instruction that does the\nwork of 4 other instructions.\n\nUsing that instruction, this code:\n\n\u003cpre class=\"highlight\"\u003e\n    \u003cb\u003e{test,is_tuple,{f,3},[{x,0}]}.\u003c/b\u003e\n    \u003cb\u003e{test,test_arity,{f,3},[{x,0},4]}.\u003c/b\u003e\n    \u003cb\u003e{get_tuple_element,{x,0},0,{x,1}}.\u003c/b\u003e\n    {get_tuple_element,{x,0},1,{x,2}}.\n    \u003cb\u003e{test,is_eq_exact,{f,3},[{x,1},{atom,tag}]}.\u003c/b\u003e\n    {test_heap,3,3}.\n    {put_tuple,2,{x,0}}.\n    {put,{atom,ok}}.\n    {put,{x,2}}.\n    return.\n\u003c/pre\u003e\n\ncan be rewritten like this:\n\n\u003cpre class=\"highlight\"\u003e\n    \u003cb\u003e{test,is_tagged_tuple,{f,1},[{x,0},4,{atom,tag}]}.\u003c/b\u003e\n    {get_tuple_element,{x,0},1,{x,2}}.\n    {test_heap,3,3}.\n    {put_tuple,2,{x,0}}.\n    {put,{atom,ok}}.\n    {put,{x,2}}.\n    return.\n\u003c/pre\u003e\n\nThis is a nice reduction in code size and execution\ntime. However, this optimization is not safe.\n\nWhy?\n\nConsider the `{test_heap,3,3}` instruction. The second `3` says\nthat 3 `x` registers are live, namely `{x,0}`, `{x,1}`, and `{x,2}`.\nClearly, `{x,0}` and `{x,2}` are live, but what about `{x,1}`?\nWe removed the `get_tuple_element` instruction that assigned `{x,1}`\na value, so the value of `{x,1}` is undefined.\n\nPassing undefined register values to the garbage collector is the kind\nof bug that could take weeks to track down. In fact, there will\nprobably be a future blog post about that kind of bug and how two\ntools were born as result of that bug.\n\nReluctantly, in order to make the optimization safe, we must keep\nthe `get_tuple_element` instruction that assigns to `{x,1}`:\n\n\u003cpre class=\"highlight\"\u003e\n    {test,is_tagged_tuple,{f,1},[{x,0},4,{atom,tag}]}.\n    \u003cb\u003e{get_tuple_element,{x,0},0,{x,1}}.\u003c/b\u003e\n    {get_tuple_element,{x,0},1,{x,2}}.\n    {test_heap,3,3}.\n    {put_tuple,2,{x,0}}.\n    {put,{atom,ok}}.\n    {put,{x,2}}.\n    return.\n\u003c/pre\u003e\n\nAnother possibility in this case would be to assign an empty list\n(called `nil` in the BEAM assembly language) to `{x,1}`:\n\n\u003cpre class=\"highlight\"\u003e\n    {test,is_tagged_tuple,{f,1},[{x,0},4,{atom,tag}]}.\n    \u003cb\u003e{move,nil,{x,1}}.\u003c/b\u003e\n    {get_tuple_element,{x,0},1,{x,2}}.\n    {test_heap,3,3}.\n    {put_tuple,2,{x,0}}.\n    {put,{atom,ok}}.\n    {put,{x,2}}.\n    return.\n\u003c/pre\u003e\n\nHowever, in this very simple example, another optimization will\nactually allow the compiler to remove the assignment to `{x,1}`:\n\n\u003cpre class=\"highlight\"\u003e\n    {test,is_tagged_tuple,{f,1},[{x,0},4,{atom,tag}]}.\n    \u003cb\u003e{test_heap,3,1}.\u003c/b\u003e\n    \u003cb\u003e{get_tuple_element,{x,0},1,{x,2}}.\u003c/b\u003e\n    {put_tuple,2,{x,0}}.\n    {put,{atom,ok}}.\n    {put,{x,2}}.\n    return.\n\u003c/pre\u003e\n\nThe `test_heap` and `get_tuple_element` instructions have been swapped.\nNote that the number of live register have been adjusted in the `test_heap`\ninstruction. It is now `1` instead of `3`.\n\nIn general, though, the compiler might have to abandon an optimization\nor keep an instruction that assigns a register to avoiding feeding the\ngarbage collector undefined values.\n\n## The final straw\n\nDuring the development of OTP 21, we realized that we have reached the\nlimit for improving the optimizations that operates on the BEAM\nassembly language. In particular, we wanted to make the optimization\ncalled the [delayed sub binary creation][bin_matching] applicable in\nmore circumstances. It turned out that would it be hard or impossible\nto substantially improve the optimization by working on BEAM assembly\nlanguage.\n\n[bin_matching]: http://erlang.org/doc/efficiency_guide/binaryhandling.html#matching-binaries\n\nApart from the problem of leaving undefined registers, as illustrated\nin the previous optimization example, there is also the complexity of\ntraversing and analyzing BEAM instructions. The BEAM instruction set\nwas not designed to be optimizer-friendly.\n\n## Conclusion\n\nAs I have tried to show with the example above, one of the hardest\nparts of working with BEAM code is that register allocation has\nalready been done and that instructions that may do a garbage\ncollection (such as `test_heap`) have already been added.\n\nEarly this year (2018), we decided that we should introduce a new\nintermediate format to alleviate the problems with optimizing BEAM\ncode. It should be close enough to BEAM code to allow\nlow-level optimizations such as the `is_tagged_tuple` optimization\ndescribed in this blog post, but register allocation should not have\nbeen done, and `test_heap` and similar instructions should not have\nbeen added. It should also be more regular to make it easier to\ntraverse while doing optimizations.\n\nWe decided to make the new intermediate format [SSA-based][ssa].\nIn the next blog post, we will re-visit the example from this blog\npost and see what it looks like in the [new SSA-based intermediate\nformat][pr1935].\n\n[ssa]: https://en.wikipedia.org/wiki/Static_single_assignment_form\n[pr1935]: https://github.com/erlang/otp/pull/1935\n"},{"id":"beam-compiler-history","title":"A Brief History of the BEAM Compiler","author":"Björn Gustavsson","excerpt":"\nThis blog post is a brief history lesson about the Erlang compiler for\nthe BEAM machine. To provide some context, there will first be a quick\nlook at the abstract machines for Erlang.","article_date":1529280000000,"tags":["compiler BEAM"],"frontmatter":{"layout":"post","title":"A Brief History of the BEAM Compiler","tags":"compiler BEAM","author":"Björn Gustavsson"},"content":"\nThis blog post is a brief history lesson about the Erlang compiler for\nthe BEAM machine. To provide some context, there will first be a quick\nlook at the abstract machines for Erlang.\n\n## A brief overview of the early Erlang implementations\n\n### The Prolog interpreter\n\nThe first version of Erlang was implemented in\nProlog in 1986. That version of Erlang was used\nto find out which features of the languages were\nuseful and which were not. New languages features\ncould be added or deleted in a matter of hours\nor days.\n\n### JAM (Joe's Abstract Machine)\n\nIt soon became clear that Erlang needed to be at\nleast 40 times faster to be useful in real projects.\n\nIn 1989 JAM (Joe's Abstract Machine) was first\nimplemented. [Mike Williams][mike] wrote the runtime system\nin C, [Joe Armstrong][joe] wrote the compiler, and\n[Robert Virding][robert] wrote the libraries.\n\n[mike]: http://www.erlang-factory.com/conference/ErlangUserConference2013/speakers/MikeWilliams\n[joe]: https://github.com/joearms\n[robert]: https://github.com/rvirding\n\nJAM turned out be 70 times faster than the Prolog\ninterpreter. Success?\n\n### TEAM (Turbo Erlang Abstract Machine)\n\nIt soon became clear that Erlang still needed\nto be faster to be useful in real projects.\n\nTherefore Bogumil (\"Bogdan\") Hausman created TEAM (Turbo Erlang\nAbstract Machine). It compiled the Erlang code to C code, which was\nthen compiled to native code using GCC.\n\nIt was significantly faster than JAM for small projects.\nUnfortunately, compilation was very slow, and the code size of the\ncompiled code was too big to make it useful for large projects.\n\n### BEAM (Bogdan's Erlang Abstract Machine)\n\nBogumil Hausman next machine was called BEAM\n(Bogdan's Erlang Abstract Machine). It was a hybrid machine\nthat could execute both native code and [threaded code] with\nan [interpreter]. That allowed customers to compile their\ntime-critial modules to native code and all other modules to\nthreaded BEAM code. The threaded BEAM in itself was faster\nthan JAM code.\n\n[threaded code]: https://en.wikipedia.org/wiki/Threaded_code\n[interpreter]: https://en.wikipedia.org/wiki/Interpreter_(computing)\n\nBogdan's original compiler for BEAM shared the compiler front end with\nJAM. Essentially, the front end at that time did the same thing as the\nfront end in the current compiler as described in [Lost in Translation\n(Exploring the Compiler's Front End)][front end].\n\nI don't have the source code for Bodgan's original compiler,\nbut as far as I can determine it had three compiler passes that\ntranslated the abstract format to threaded BEAM code.\n\n* `beam_compile` - Translated the abstract format to BEAM instructions.\n\n* `beam_optimize` - Optimized the BEAM instructions. This pass was mandatory,\nsince it did some necessary transformations of the BEAM instructions.\n\n* `beam_asm` - Converted the symbolic BEAM assembly format to a binary\nBEAM module.\n\n[front end]: http://blog.erlang.org/compiler-lost-in-translation\n\n### VEE (Virding's Erlang Engine)\n\nHere we must mention VEE (Virding's Erlang Engine) for reasons that\nwill soon become clear.\n\nVEE was an experimental implementation with a different memory model\ncompared to JAM and BEAM. Instead of JAM's and BEAM's separate heaps\nfor each process, VEE used a single shared heap with a real-time\ngarbage collector.  That made message passing blindlingly fast\ncompared to JAM and BEAM.\n\nOverall, though, there was no speed gain compared to JAM. The reason\nwas probably that the single shared heap decreased the cache hit\nrate.\n\n## The maturation of BEAM\n\nThe OTP group and Erlang/OTP was created to industrialize Erlang and\nmake it suitable for huge real-world projects. The first release, OTP\nR1B, was released in 1996.\n\nThis is the point where the history lesson may become a little bit\nmore subjective.\n\nI joined the Erlang/OTP team at the end of 1996. My first small\ncode contributions to Erlang/OTP were included in OTP R1D.\n\nI worked in the ERTS (Erlang Run-Time System) team, which at that time\nwas lead by Kenneth Lundin. Initially I worked with the Erlang runtime\nsystem for Microsoft Windows. After some time (maybe a year or so),\nKenneth asked me to help stabilizing and improving BEAM. Gradually\nBEAM become my main responsibility, and when Bogdan left Ericsson, I\nbecome the main developer responsible for the BEAM [interpreter] and\ncompiler.\n\nThis blog post desperately tries to cover the history of the BEAM\n*compiler*, but I think that some more historical context is needed\nbefore we can approach the compiler.\n\nThe overall goal of the work on BEAM from OTP R1 up to OTP R5\nwas to make it stable enough and fast enough to be useful in real\nprojects.\n\nThere were two major obstacles to reaching that goal:\n\n* BEAM/C, that is, native code via C code.\n* The huge number of ever-changing BEAM instructions.\n\n### BEAM/C must die!\n\nIt soon became obvious that BEAM/C, the compiler passes that\ncompiled Erlang code to C code, had to die. At the time that\nI started working on BEAM, there were three distinct flavors of\nBEAM/C: one for GCC on Sparc, one for GCC on non-sparc CPUs (such\nas Intel x86), and one for other C compilers that did not support\nGCC's extension for taking the address of a label. Bugs not only showed\nup in the native code, but the mere existence of BEAM/C complicated and\ncaused bugs in the threaded BEAM interpreter.\n\nUnfortunately, early in my career of improving BEAM, I made some\noptimizations of the size of the C code generated by BEAM/C. That came\nback to bite me later when I suggested that we should remove\nBEAM/C. The size improvements made it possible to fit more Erlang code\ncompiled to native code into the system, and the native code was\nfaster than threaded BEAM code. Our customer at the time ([the AXD 301\nproject][axd301]) needed the extra speed improvements that BEAM/C gave\nthem and did not allow us to remove BEAM/C unless we could improve the\nperformance of threaded BEAM code to similar or better than BEAM/C\nperformance.\n\n[axd301]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.140.9122\u0026rep=rep1\u0026type=pdf\n\n### The ever-changing BEAM instructions\n\nAt that time, the BEAM interpreter had over [300\ninstructions][beam1997].  While JAM had a very simple loader that\nessentially only loaded the JAM files into memory, the loader for BEAM\nhad to translate every instruction from the byte format in the BEAM\nfiles to the threaded code format in memory.  The BEAM had\nhand-written code for the loading of every single instruction.\n\nTo make it worse, the instruction set was constantly evolving. Bug\nfixes and performance improvements needed new instructions, and those\ninstructions had to be implemented in the compiler, threaded code\ninterpreter (the `process_main()` function in `beam_emu.c`), and\nthe loader. In every minor and major release of Erlang/OTP, the\nusers of BEAM had to recompile all of their Erlang code\nbecause the instruction set had changed.\n\nThere must be a better way, I thought. I started to write a simple\nPerl script to a least automate the mapping from instruction name to\ninstruction number in the compiler, interpreter, and loader.\n[Tony Rogvall][tonyrog] suggested that I could be more ambitious and\ngenerate most of the code for for the loader using the Perl script.\nHe also suggested that operands for many instructions could be packed\ninto a single word. That would reduce load code size and also improve\nthe cache hit rate, improving execution speed.\n\nSo I started writing the first version of the [beam_makeops script][makeops]\nand rewriting the loader. I prefer to work incrementally, making minor changes\nto a code base that is always working. But I could not rewrite the loader\nincrementally, so I hacked away frantically for two or three days until\nI had a bare bones version of the new loader working. I could then relax\na little and somewhat more slowly add more features to `beam_makeops` and\nthe loader.\n\nThe new loader took over some tasks formerly done by the compiler.\n\nFor example, the BEAM machine has several specialized `move`\ninstructions.  There is one instruction for moving something into an X\nregister, another for moving an atom into an X register, and so\non. Before the new loader, the compiler knew about all those variants\nof `move` instructions and selected the appropriate one. With the new\nloader, there is only one `move` instruction that the compiler needs\nto care about, and the loader will select the appropriate specialized\n`move` instruction to use at load time.\n\nAnother minor optimization done by the compiler was combining of\ncommon instructions sequences. For example, a `move` instruction\nfollowed by a `call` instruction would be combined to a `move_call`\ninstruction. That optimization was also moved to the loader.\n\nAll those capabilities made it possible to significantly simplify and\nreduce the number of instructions known to the compiler. More\nimportantly, that made it possible to keep the instruction set stable\n(while still allowing minor optimizations and performance tuning by\ntweaking only the loader and interpreter), avoiding the need to\nrecompile all Erlang code every time there was a new release.\n\nIf my memory doesn't fail me, the new loader was introduced in OTP R4.\n\n[beam1997]: http://www.cs-lab.org/historical_beam_instruction_set.html\n[tonyrog]: https://github.com/tonyrog\n[makeops]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/erts/emulator/internal_doc/beam_makeops.md\n\n## OTP R5B: The \"new\" BEAM\n\nMoving forward to OTP R5.\n\nOTP R5 was the last release that supported JAM.\n\nOTP R5 can also be said to be first release that featured the \"new\"\nBEAM. In that release, the [modern BEAM file format][beam file format]\nwas introduced. The same file format is used today. At that time,\nthere were 78 BEAM instructions; in OTP 20, there are 159 instructions\n(actually, 129 active instructions and 30 obsoleted instructions no\nlonger used). While new instructions have been introduced when needed\nand obsolete instructions have been removed, it has always been\npossible to load BEAM files compiled from at least two major releases\nback.\n\nExecution of threaded BEAM had become fast enough, so that BEAM/C\ncould be dropped (already in R4, I think). But strangely enough,\nthe customers still wanted more speed.\n\nThe BEAM compiler in R5 was still Bogdan's original compiler. While\nit did more optimizations than the JAM ever did, we knew that more\noptimizations were possible.\n\n[beam file format]: http://www.erlang.se/~bjorn/beam_file_format.html\n\n## R6B: Enter Kernel Erlang\n\nMeanwhile, on the top floor Robert Virding was busy writing a\nnew compiler for his VEE machine. In that new compiler, Robert\nintroduced a new intermediate format that he called *Kernel Erlang*.\nThe idea was that more optimizations could be applied to the code\nin that format before generating code for the actual machine.\n\nAt that time, there was no actual interpreter that could execute the\ncode emitted by his new compiler (he had not updated the VEE machine\nyet). The machine he had in mind was a register machine. It was similar\nto BEAM, except that it did stack trimming.\n\nWe wanted the better performance that we could get from Robert's compiler,\nbut the question was: should we implement a new interpreter (or adapt\nBEAM) to execute the code from Robert's compiler, or should we adapt\nRobert's compiler to generate BEAM code?\n\nBecause we now for the first time had a stable implementation of BEAM,\nwe decided not to rock the boat again; thus, we decided that I should\nadapt the code generator part of Robert's compiler for BEAM.\n\nFor the most part, I used Robert's name for instructions. For example,\nthe instruction to load a term into a register was called `M` in the\noriginal BEAM, while Robert's compiler used the `move`. The more major\nchanges was in the handling of the stack.  Robert's compiler had stack\ntrimming, which I had to remove and rewrite to handle BEAM's fixed\nstack frame. (I reintroduced a limited form of stack trimming later.)\n\nSince JAM was not supported in OTP R6, all customers that had previously\nused JAM had to migrate to BEAM. To minimize the risk of the migration\nas much as possible, one of our customers requested that we made the\nbattle-tested original BEAM compiler available as an option in OTP R6.\n\nTherefore, we added options to choose which version of the compiler\nto use. To use the old compiler, one would write:\n\n```\n$ erlc +v1 some_module.erl\n```\n\nDefault was Robert's new compiler, which was called `v2`. There\nwas also an undocumented, unofficial compiler version called `v3`.\n\nAll compilers shared the front end and the `beam_asm` pass that\ncreated the final BEAM module.\n\n### The v1_compiler\n\nThe `v1` compiler had the following passes:\n\n* v1_adapt\n* v1_compile\n* v1_optimize\n* v1_cleanup\n\nThe `v1_compile` and `v1_optimize` passes were essentially\nthe `beam_compile` and `beam_optimize` passes from Bogdan's\ncompiler.\n\nThere had been some changes to the front end since R5, so\nthe `v1_adapt` pass was there to hide those changes for the\n`v1_compile` and `v1_optimize` passes. The `v1_cleanup` pass was\nan additional minor optimization pass; I think it was present\nin OTP R5 as well.\n\n### The v2_compiler\n\nThe `v2` compiler was Robert's new compiler. It had the following\npasses:\n\n* v2_kernel\n* v2_kernopt\n* v2_match\n* v2_life\n* v2_codegen\n\nThe `v2_kernel` pass translated the abstract format to Kernel Erlang.\n\n`v2_kernopt` did very basic optimizations of the Kernel Erlang code,\nessentially only [constant propagation and constant folding][folding].\n\n`v2_match` did pattern matching compilation. JAM would match clauses\nin function heads or `case` expressions sequentially. The old BEAM\ncompiler would do only a little bit better in that it could match\nmultiple integers or atoms in a single instruction. Robert's compiler\nwas the first Erlang compiler to properly compile pattern matching using\nthe algorithm described in\n[The Implementation of Functional Programming Languages][peytonjones]\nby Simon Peyton Jones.\n\n`v2_life` would calculate life-time information needed by the\n`v2_codegen` pass, and `v2_codegen` would generate the BEAM\nassembly code.\n\n[folding]: https://en.wikipedia.org/wiki/Constant_folding\n[peytonjones]: https://www.microsoft.com/en-us/research/publication/the-implementation-of-functional-programming-languages/\n\n## R7B: Enter Core Erlang\n\nMeanwhile, [Richard Carlsson][carlsson] and the [HiPE group][hipe]\nat Uppsala University come up with the idea for a new intermediate\nformat useful as an interchange format for different Erlang\nimplementations and for optimizing Erlang programs.\n\nThe new format was called [Core Erlang][core]. Robert liked the idea\nand started to implement Core Erlang in the compiler.  The undocumented\nimplementation of `v3` compiler in OTP R6 is based on a draft version\nof the Core Erlang specification.\n\nIn OTP R7B, the v1 and v2 compilers were removed, and the only\nremaining compiler was the `v3` compiler that used Core Erlang.\nIt had the following passes:\n\n* v3_core\n* v3_core_opt\n* v3_kernel\n* v3_life\n* v3_codegen\n\nThe `v3_core` pass translated the abstract format to Core Erlang.\n\nThe `v3_core_opt` pass essentially only called `sys_core_fold`, which\ndid [constant propagation and constant folding][folding]. `sys_core_fold`\nstill do those things, and [more][sys_core_fold].\n\nThe remaining passes do the same thing as today.\n\nThe `v3_kernel` pass translates from Core Erlang to Kernel Erlang,\nand also does pattern matching compilation (in the same way as in\n`v2_match`). The optimizations in `v2_kernopt` are now done in\n`sys_core_fold`.\n\nThe `v3_life` pass (despite its name) no longer calculates life-time\ninformation. The life-time information is instead calculated by\n`v3_kernel` and passed on as annotations.\n\nThe reason that `v3_life` still exists is that Robert had continued\nto work on his own version of `codegen` that did not have all\nmy changes in it to work for BEAM. While implementing the Core Erlang\npasses, he also did many improvements to `codegen`.\n\nWhen it was time to integrate our different versions of the compiler,\nRobert looked in horror at all my changes in `codegen`. To avoid\nhaving to reintroduce all my adapations and optimizations for BEAM\ninto his new version of `codegen`, Robert wrote an adapter pass\nthat translated from the new Kernel Erlang format to the old format\nso that my `codegen` would work. The adapter pass is called\n`v3_life`.\n\nThus, `v3_codegen` is essentially `v2_codegen` with a new name.\n\nIn the upcoming OTP 21, `v3_life` has been combined with `v3_codegen`.\n\n[hipe]: https://www.it.uu.se/research/group/hipe/\n[carlsson]: https://github.com/richcarl\n[core]: https://www.it.uu.se/research/group/hipe/cerl/doc/core_erlang-1.0.3.pdf\n[sys_core_fold]: http://blog.erlang.org/core-erlang-optimizations\n\n## Learning Erlang from Robert\n\nIn the time period that Robert and I worked together on the compiler,\nI usually worked on `v3_codegen` and the passes below, while Robert\nworked on all passes above `v3_codegen`.\n\nOccasionally, I would add some optimizations to `sys_core_fold` and\ngive them to Robert to incorporate into his latest version of\n`sys_core_fold`.\n\nI would then look at what Robert had done with my code, and learn.\n\nUsually Robert had subtly improved my code, made it slightly\ncleaner and simpler. But one time I handed Robert an\noptimization of `case` clauses. The code I got back was very different.\nRobert had broken apart my optimization into several simpler\noptimizations that achieved the same purpose (and more) than my\nmore complicated optimization.\n"},{"id":"Interpreter-Optimizations","title":"Interpreter optimization","author":"Lukas Larsson","excerpt":"\nThe BEAM [interpreter] in erts has been completely re-written in OTP 21.\nMost of the instructions have remained the same, but the perl scripts used\nto generate the C code have a new implementation. This blog post will look at\nsome of the optimizations that were possible because of those changes.","article_date":1528675200000,"tags":["interpreter BEAM"],"frontmatter":{"layout":"post","title":"Interpreter optimization","tags":"interpreter BEAM","author":"Lukas Larsson"},"content":"\nThe BEAM [interpreter] in erts has been completely re-written in OTP 21.\nMost of the instructions have remained the same, but the perl scripts used\nto generate the C code have a new implementation. This blog post will look at\nsome of the optimizations that were possible because of those changes.\n\nFirst however, let's start with some basics about how the interpreter is\nbuilt. The BEAM interpreter is built using generated C code. Dispatching\nis done using [direct threading] which leverages the GCC extension [labels as values].\nIt is the job of the [beam_makeops] perl script to take the input files\nand create the C files.\n\nThere is also a set of transformation rules generated that is used by the\n[transform_engine] in the beam code loader to do several peephole optimization.\nThe optimization include, but are not limited to, instruction combining,\ninstruction specialization and dead code elimination.\n\nThere are three separate types of input files used.\n\n* [genop.tab] contains a listing of the instructions that the compiler emits.\n* [ops.tab] contains load time transformations done to the code.\n* [instrs.tab] contain the implementation of each instruction.\n\nThere is a description of the syntax and semantics of the different files\nin the [internal beam_makeops documentation]. The largest difference\nbetween the new and old way of doing the code generation is that\nin OTP 21 _all_ the instructions are now generated, instead of about 75%.\nThis allowed us to do architecture specific optimizations for all instructions\nwhen generating the code for the instructions.\n\n[interpreter]: https://en.wikipedia.org/wiki/Interpreter_(computing)\n[direct threading]: https://en.wikipedia.org/wiki/Threaded_code#Direct_threading\n[labels as values]: https://gcc.gnu.org/onlinedocs/gcc/Labels-as-Values.html\n[beam_makeops]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/erts/emulator/utils/beam_makeops\n[genop.tab]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/lib/compiler/src/genop.tab\n[ops.tab]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/erts/emulator/beam/ops.tab\n[instrs.tab]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/erts/emulator/beam/instrs.tab\n[internal beam_makeops documentation]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/erts/emulator/internal_doc/beam_makeops.md\n[transform_engine]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/erts/emulator/beam/beam_load.c#L5213\n\n## Packing the dispatch target address ##\n\nOn 64-bit machines all pointers are 8 byte large, this is also the case for\nthe pointer that you get when taking the address of a label. So for a small\ninstruction such as move_cx (move a constant to an x register), 3 words of\nmemory is needed.\n\n         +--------------------+--------------------+\n    I -\u003e |                            \u0026\u0026lb_move_cx |\n         +--------------------+--------------------+\n         |                        Tagged atom 'id' |\n         +--------------------+--------------------+\n         |                                      40 |\n         +--------------------+--------------------+\n\nOne word for the instruction, one word for the literal and then the target x\nregister actually only needs 2 bytes, but since all code is word aligned it\ngets its own word.\n\nHowever, on most machines, the linker uses what is called a small or medium code model.\nThis means that it will place all code in the lower 2 GB of the address space\nso that more efficient machine instructions can be used. It also works in the\nfavor of the beam interpreter as we now know that the upper 4 bytes of the\ninstruction word will always be 0.\n\n         +--------------------+--------------------+\n    I -\u003e |                 40 |       \u0026\u0026lb_move_cx |\n         +--------------------+--------------------+\n         |                        Tagged atom 'id' |\n         +--------------------+--------------------+\n\nSo instead of placing the x register to use in its own word we pack it into the\ninstruction word, thus saving one word of memory for this instruction.\n\nThe optimization is only possible to do on 64-bit platforms that place all code in the\nlower 4 GB of the address space, so that makes 32-bit and [position-independent executable]\nplatforms not use it.\nTo figure out which 64-bit platforms we can do it on we first started by writing\na configure script that looked at CFLAGS and LDFLAGS, asked the compiler what it would do\nby default etc etc. After tinkering for a while we came up with a simpler and so\nfar stable solution:\n\n    #include \u003cstdlib.h\u003e\n    int main() {\n      if ((unsigned long long)\u0026main \u003c (1ull \u003c\u003c 32)) {\n        exit(0);\n      }\n      exit(1);\n    }\n\nIt would seem that [position-independent executable] always places code in\nsegments \u003e 4GB so we can just check where it put main in a small test program.\n\nPacking arguments into the instruction word is possible for a significant amount\nof instructions which reduced loaded code size and in turn increased performance.\n\n## Smarter packing ##\n\nThe packing engine in [beam\\_makeops] uses several different heuristics to figure\nout which instruction argument(s) it should put in the instruction word. Because\nof alignment requirements and other things, only some types of argument are allowed\ntogether. When choosing how to pack arguments the packer first builds all different\nvariant possible and then chooses the instruction that takes the least amount of\nmemory. The logic for this can be found in the [do_pack_one] function in\n[beam_makeops]. Before OTP-21 this had to be done manually for all instructions,\nwhich meant that the implementer had to be extra careful when deciding which\norder the arguments should be placed in [ops.tab]. In addition the packing logic\nbefore OTP-21 did not pack argument of varying size into the same machine word.\n\nThe packer is not perfect however, so in some cases we needed to make it\npossible to override its decisions. That is why the [? type modifier] was introduced.\nThe [? type modifier] is used to determine whether an argument is likely to be used\nor not by the instruction.\n\nHow do you determine if an argument is likely to be used or not? For some instructions\nit is obvious, eg. the allocate instruction has an argument that is only used\nif a garbage collection is triggered, so it is very unlikely to be used. In other cases\nit is not so obvious, eg. the failure label of test instructions, is it likely to be\nused or not? For most test instructions it will be unlikely that it will be used,\nor at least more unlikely than the other arguments of the instruction as they are used\nin the actual test.\n\nWhy is packing less used instructions into the instruction word better? It seems like\nGCC will generate a little better code for most instructions if the arguments that\nare always used are in the same word, we've seen this by both looking at the assembly\nthat GCC produced and the total code size of the interpreter becomes smaller. We haven't\nbeen able to measure any differences in performance due to how the unlikely instructions\nare packed, but smaller code is always a good thing.\n\n[? type modifier]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/erts/emulator/internal_doc/beam_makeops.md#the--type-modifier\n[position-independent executable]: https://en.wikipedia.org/wiki/Position-independent_code#Position-independent_executables\n[do_pack_one]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/erts/emulator/utils/beam_makeops#L1801\n\n## Relative jump labels ##\n\nMany instruction have branches in them that will be taken depending on various conditions.\nOne of the simplest ones is the [is_eq_exact_immed] instruction. It will continue on\nto the next instruction if the value is equal to the immediate literal in the argument,\nor jump to the fail label if they are not equal. Before OTP-21 the instruction\nis\\_eq\\_exact\\_immed\\_frc would have this layout:\n\n         +--------------------+--------------------+\n    I -\u003e |              \u0026\u0026lb_is_eq_exact_immed_frc |\n         +--------------------+--------------------+\n         |                 Pointer To failure code |\n         +--------------------+--------------------+\n         |                       Tagged immed 'id' |\n         +--------------------+--------------------+\n\nThe C code for the instruction would look something like this (I've removed all macros\nthat beam\\_makeops uses):\n\n    if (reg[0] != I[2]) {\n      I = I[1];\n      goto *(void**)I;\n    }\n    I+=3;\n    goto *(void**)I;\n\nIn this example it does not help to use part of the instruction word as both arguments\nare 8 bytes large. We also cannot use the same trick as the instruction word to make\nthe fail label smaller as Erlang code can be allocated anywhere in the 64-bit address\nspace. One thing that we can rely on though is that the code for a single Erlang module\nwill be located in a contiguous memory area. So instead of using pointers to the code\nthat we want to jump to we can use relative addresses for jumps within the same module.\n\nThe problem with using relative local jumps is that we put a limit on how large a module\ncan be. For instance, the largest module in Erlang/OTP is the 'OTP-PUB-KEY' module,\nit is 61585 words large when loaded and the longest local jump is 5814 words. For this\nparticular module we could have used a 13-bit jump label, or 16-bits if we wanted to be\nsure that all functions could call each other. 16-bits would have been a perfect size as\nit can be packed with another register in the instruction word. So it would have been\npossible to fit a instruction address + jump label + register in 8 bytes. However, it is\nway too close for comfort, and there are sure to be larger modules than ours in other\nsystems, so we decided to use 32-bits for the jump labels. So in OTP-21 the max size of\na loaded module has gone from 2^32 GB to 32 GB, which should be enough for most use cases.\n\nUsing this new layout the is\\_eq\\_exact\\_immed\\_frc instruction can be re-written to use\nthe following layout instead:\n\n         +--------------------+--------------------+\n    I -\u003e | Offset to failure..| \u0026\u0026lb_is_eq_exac... |\n         +--------------------+--------------------+\n         |                       Tagged immed 'id' |\n         +--------------------+--------------------+\n\nAnd generate this code:\n\n    if (reg[0] != I[1]) {\n      I += I[0] \u003e\u003e 32;\n      goto *(void**)(Uint64)(Uint32)I;\n    }\n    I+=2;\n    goto *(void**)(Uint64)(Uint32)I;\n\nThe code ends up being a little bit more complicated, but the C compiler manages to optimize\nit into very efficient code. Basically for each jump label there is an extra add operation\nwhen compared to before. When profiling this extra code is not noticeable as it drowns in\nthe load of the value from (hopefully) the l1-cache.\n\nA lot of instructions benefit from this optimization as a lot of them control the control\nflow. This is especially noticeable on large [select_val_bins], as they have their memory\nusage reduced by 25%. Also as you may have noticed, it plays very well with the packing\nof the instruction word and the [? type modifier] in [beam_makeops].\n\n[is_eq_exact_immed]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/erts/emulator/beam/instrs.tab#L789\n[select_val_bins]: https://github.com/erlang/otp/blob/OTP-21.0-rc1/erts/emulator/beam/select_instrs.tab#L32-L76\n"},{"id":"core-erlang-wrapup","title":"Core Erlang Wrap Up","author":"Björn Gustavsson","excerpt":"\nThis blog post wraps up the exploration of Core Erlang started in the\nprevious two blog posts. The remaining default Core Erlang\npasses are described, followed by a look at how Core Erlang is\nrepresented internally in the compiler.","article_date":1527638400000,"tags":["compiler BEAM"],"frontmatter":{"layout":"post","title":"Core Erlang Wrap Up","tags":"compiler BEAM","author":"Björn Gustavsson"},"content":"\nThis blog post wraps up the exploration of Core Erlang started in the\nprevious two blog posts. The remaining default Core Erlang\npasses are described, followed by a look at how Core Erlang is\nrepresented internally in the compiler.\n\nHere are the Core Erlang passes that will be run when using\nOTP 21 RC1 or the `master` branch in the git repository:\n\n```\n$ erlc +time core_wrapup.erl\nCompiling \"core_wrapup\"\n     .\n     .\n     .\n core                          :      0.000 s      15.7 kB\n sys_core_fold                 :      0.000 s       9.0 kB\n sys_core_alias                :      0.000 s       9.0 kB\n core_transforms               :      0.000 s       9.0 kB\n sys_core_bsm                  :      0.000 s       9.0 kB\n sys_core_dsetel               :      0.000 s       9.0 kB\n     .\n     .\n     .\n```\n\nWe have covered `core` and `sys_core_fold` in the two previous\nblog posts about Core Erlang.\n\n## The other Core Erlang passes\n\n### sys_core_alias\n\nIn the upcoming OTP 21 release, there is a new `sys_core_alias` pass\ncontributed by [José Valim](https://github.com/josevalim).\n\nThe purpose of the pass is to avoid rebuilding terms that\nhave been matched, such as in this example:\n\n```erlang\nremove_even([{Key,Val}|T]) -\u003e\n    case Val rem 2 =:= 0 of\n        true -\u003e remove_even(T);\n        false -\u003e  [{Key,Val}|remove_even(T)]\n    end;\nremove_even([]) -\u003e [].\n```\n\nIn the function head, the pattern `{Key,Val}` binds two elements of a\ntuple to the variables `Key` and `Val`, but the original tuple is not\ncaptured. In the `false` clause of the `case`, a new tuple will be\nconstructed from `Key` and `Val`.\n\nIt is possible to avoid creating a new tuple by using the `=` operator\nto bind the complete tuple to a variable:\n\n```erlang\nremove_even([{Key,Val}=Tuple|T]) -\u003e\n    case Val rem 2 =:= 0 of\n        true -\u003e remove_even(T);\n        false -\u003e  [Tuple|remove_even(T)]\n    end;\nremove_even([]) -\u003e [].\n```\n\nEssentially, the new `sys_core_alias` pass does that transformation\nautomatically. Here is the Core Erlang code before applying this\noptimization:\n\n```\n'remove_even'/1 =\n    fun (_0) -\u003e\n\tcase _0 of\n\t  \u003c[{Key,Val}|T]\u003e when 'true' -\u003e\n\t      let \u003c_1\u003e =\n\t\t  call\n\t\t       'erlang':'rem'(Val, 2)\n\t      in\n\t\t  case \u003c\u003e of\n\t\t    \u003c\u003e\n\t\t\twhen call 'erlang':'=:='(_1, 0) -\u003e\n\t\t\t    apply 'remove_even'/1(T)\n\t\t    \u003c\u003e when 'true' -\u003e\n\t\t\tlet \u003c_2\u003e =\n\t\t\t    apply 'remove_even'/1(T)\n\t\t\tin\n                            [{Key,Val}|_2]      % BUILDING TUPLE\n\t\t  end\n\t  \u003c[]\u003e when 'true' -\u003e\n\t      []\n\t  \u003c_4\u003e when 'true' -\u003e\n\t\tprimop 'match_fail'({'function_clause',_4})\n\tend\n```\n\nHere is the code after running the `sys_core_alias` pass:\n\n```\n'remove_even'/1 =\n    fun (_0) -\u003e\n\tcase _0 of\n\t  \u003c[_@r0 = {Key,Val}|T]\u003e when 'true' -\u003e\n\t      let \u003c_1\u003e =\n\t\t  call 'erlang':'rem'(Val, 2)\n\t      in\n\t\t  case \u003c\u003e of\n\t\t    \u003c\u003e\n\t\t\twhen call 'erlang':'=:='(_1, 0) -\u003e\n\t\t\t    apply 'remove_even'/1(T)\n\t\t    \u003c\u003e when 'true' -\u003e\n\t\t\tlet \u003c_2\u003e =\n\t\t\t    apply 'remove_even'/1(T)\n\t\t\tin\n\t\t\t    [_@r0|_2]          % REUSING EXISTING TUPLE\n\t\t  end\n\t  \u003c[]\u003e when 'true' -\u003e\n\t      []\n\t  \u003c_4\u003e when 'true' -\u003e\n\t\tprimop 'match_fail'({'function_clause',_4})\n\tend\n```\n\n### core_transforms\n\nSimilar to parse transforms, the `core_transforms` pass makes it possible to\nadd compiler passes that transform the Core Erlang code without modifying\nthe compiler.\n\nAs an example, here is a simple core transform module:\n\n```erlang\n-module(my_core_transform).\n-export([core_transform/2]).\n\ncore_transform(Core, _Options) -\u003e\n    Module = cerl:concrete(cerl:module_name(Core)),\n    io:format(\"Module name: ~p\\n\", [Module]),\n    io:format(\"Number of nodes in Core Erlang tree: ~p\\n\",\n              [cerl_trees:size(Core)]),\n    Core.\n```\n\nBefore explaining the code, let's see it in action:\n\n```\n$ erlc my_core_transform\n$ erlc -pa . '+{core_transform,my_core_transform}' core_wrapup.erl\nModule name: core_wrapup\nNumber of nodes in Core Erlang tree: 220\n$\n```\n\nThe `{core_transform,Name}` option instructs the compiler to run a\ncore transformation. In this case, the core transform module is\n`my_core_transform`. After doing the standard optimizing passes,\nthe compiler will call `my_core_transform:core_transform/2`, passing\nthe Core Erlang code as the first argument and the compiler options\nas the second argument.\n\nThe first line in the `core_transform/2` functions calls\n`cerl:module_name(Core)` to retrieve the module name. The return value\nof `cerl:module_name/1` is a record representing any literal term. To\nretrieve the actual term (an atom in this case), `cerl:concrete/1`\nis called.\n\nIn the second `io:format/2` call, we call `cerl_trees:size/1` to\ncount the number of nodes in the tree that represents the Core Erlang\ncode for the module.\n\nThis core transform does not do any real transforming, since the last\nline returns the Core Erlang code without any modifications.\n\n### sys_core_bsm\n\n`sys_core_bsm` is the first of three passes that implement the delayed\nsub binary optimization described in the [Efficiency\nGuide][efficiency_guide].  `sys_core_bsm` adds annotations that are\nlater used by `v3_codegen` and `beam_bsm` to optimize matching of\nbinaries.\n\n[efficiency_guide]: http://erlang.org/doc/efficiency_guide/binaryhandling.html\n\n### sys_core_dsetel\n\nThe `sys_core_dsetel` pass will optimize chained or nested\napplications of `setelement/3` as in this example:\n\n```erlang\nupdate_tuple(T0) -\u003e\n    T = setelement(3, T0, y),\n    setelement(2, T, x).\n```\n\nTranslated to Core Erlang it looks like this:\n\n```\n'update_tuple'/1 =\n    fun (_0) -\u003e\n\tlet \u003cT\u003e =\n\t    call 'erlang':'setelement'(3, _0, 'y')\n\tin\n\t    call 'erlang':'setelement'(2, T, 'x')\n```\n\nThe `sys_core_dsetel` pass replaces the second call to `setelement/3`\nwith the primop `dsetelement/3`, which destructively updates a tuple:\n\n```\n'update_tuple'/1 =\n    fun (_0) -\u003e\n\tlet \u003cT\u003e =\n\t    call 'erlang':'setelement'(3, _0, 'y')\n\tin  do\n\t\tprimop 'dsetelement'(2, T, 'x')\n\t\tT\n```\n\n`do` evalutes two expressions in sequence, ignoring the value of the\nfirst expression. It is used here because the primop `dsetelement/3`\nupdates its tuple argument without returning a value.\n\nThe `sys_core_dsetel` pass is intentionally run as the very last\nCore Erlang pass. Doing other optimizations might render the optimization\nunsafe. For example, there must not occur a garbage collection between\nthe call to `setelement/3` and `dsetelement/3`.\n\nWhy is this optimization useful? Surely a sequence of `setelement/3`\ncalls must be rare?\n\nConsider this function that updates two elements in a record:\n\n```\n-record(rec, {a,b,c,d,e,f,g,h}).\n\nupdate_record(R) -\u003e\n    R#rec{a=x,b=y}.\n```\n\nIn [a previous blog post][lost in translation] we saw that the `-E` option\nwill produce an `.E` file with the code after records have been translated\nto tuple operations:\n\n[lost in translation]: http://blog.erlang.org/compiler-lost-in-translation/\n\n```\n$ erlc -E core_wrapup.erl\n```\n\nHere is the code for `update_record/1` after record translation:\n\n```\nupdate_record(R) -\u003e\n    begin\n        rec0 = R,\n        case rec0 of\n            {rec,_,_,_,_,_,_,_,_} -\u003e\n                setelement(2, setelement(3, rec0, y), x);\n            _ -\u003e\n                error({badrecord,rec})\n        end\n    end.\n```\n\nAfter verifying that `R` is indeed a record of the correct type (that is,\nthat the size and first element of the tuple are correct), nested calls\nto `setelement/3` is used to update two elements of the tuple.\n\nThe optimized Core Erlang code for `update_record/1` will look like\nthis:\n\n```\n'update_record'/1 =\n    fun (_0) -\u003e\n\tcase _0 of\n\t  \u003c{'rec',_5,_6,_7,_8,_9,_10,_11,_12}\u003e when 'true' -\u003e\n\t      let \u003c_2\u003e =\n\t\t  call 'erlang':'setelement'(3, _0, 'y')\n\t      in  do  primop 'dsetelement'(2, _2, 'x')\n\t\t      _2\n\t  \u003c_13\u003e when 'true' -\u003e\n\t\tcall 'erlang':'error'({'badrecord','rec'})\n\tend\n```\n\n## The representation of Core Erlang code\n\nSo far we have looked at the external (pretty-printed) representation of\nCore Erlang. Before leaving Core Erlang, we will take a brief look at\nthe internal representation of Core Erlang that the compiler uses.\n\nThere are ~~two~~ three ways to work with Core Erlang within an\noptimizer pass:\n\n* Using the API functions in the `cerl` module\n\n* Using the `c_*` records defined in `core_parse.hrl`\n\n* Mixing use of records with use of the API functions\n\n### Using the cerl module and friends\n\nThe `cerl` module provides API functions to construct, deconstruct, update,\nand query each of the constructs in Core Erlang.\n\nHere are some examples:\n\n* `cerl:c_var(Name)` constructs the Core Erlang representation\nof a variable with the name `Name`.\n\n* `cerl:is_c_var(Core)` returns `true` if the `Core` represents a Core\nErlang variable, and `false` otherwise.\n\n* `cerl:var_name(Core)` returns the name of a variable (and crashes if\n`Core` does not represent a Core Erlang variable).\n\nThere are also the `cerl_trees` and `cerl_clauses` modules that provide\nuseful utility functions for manipulating Core Erlang code.\n\n### Using the records\n\nIn `core_parse.hrl`, there is one record for each kind of Core Erlang\nconstruct. All record names start with the prefix `c_`.\n\nFor example, the record `#c_var{}` represents a variable, the record\n`#c_call{}` the `call` expression, the record `c_tuple{}` a tuple, and\nso on.\n\nAs a complete example, we can rewrite our previous core transform\nto use record matching instead of `cerl` to retrieve the module name:\n\n```\n-module(my_core_transform).\n-export([core_transform/2]).\n\n-include_lib(\"compiler/src/core_parse.hrl\").\n\ncore_transform(Core, _Options) -\u003e\n    #c_module{name=#c_literal{val=Module}} = Core,\n    io:format(\"Module name: ~p\\n\", [Module]),\n    io:format(\"Number of nodes in Core Erlang tree: ~p\\n\",\n              [cerl_trees:size(Core)]),\n    Core.\n```\n\n### Mixing the `cerl` API with records\n\nThe `cerl` module internally use the records in `core_parse.hrl`, so\nthe two approaches can be mixed. For example, `sys_core_fold` mostly\nuse the records, but sometimes uses `cerl` when it is more convenient.\n\n## Wrapping up the wrap up\n\nIt seems that there is enough material for several more blog posts\nabout Core Erlang. For instance, I haven't even mentioned the inliners\n(not a typo, there are *two* inliners). That means that there might be\nmore blog posts about Core Erlang in the future.\n\nBut in the very near future, it is time to explore the compiler passes\nthat follow Core Erlang, and perhaps answer the eternal question about\nthe `v3_` prefix. Was there ever a `v2_kernel` (spoiler: yes) or a\n`v1_kernel` (spoiler: no)?\n"},{"id":"core-erlang-optimizations","title":"Core Erlang Optimizations","author":"Björn Gustavsson","excerpt":"\nThis blog post continues the exploration of Core Erlang by\nlooking at some optimizations done by the `sys_core_fold`\ncompiler pass. The Core Erlang language was introduced in\nthe [previous blog post](http://blog.erlang.org/core-erlang-by-example/).","article_date":1526601600000,"tags":["compiler BEAM"],"frontmatter":{"layout":"post","title":"Core Erlang Optimizations","tags":"compiler BEAM","author":"Björn Gustavsson"},"content":"\nThis blog post continues the exploration of Core Erlang by\nlooking at some optimizations done by the `sys_core_fold`\ncompiler pass. The Core Erlang language was introduced in\nthe [previous blog post](http://blog.erlang.org/core-erlang-by-example/).\n\nTo prepare the examples in this blog post I used two\ncommands.\n\n```\n$ erlc +time +dcore core_fold_example.erl\nCompiling \"core_fold_example\"\n parse_module                  :      0.000 s       9.4 kB\n transform_module              :      0.000 s       9.4 kB\n lint_module                   :      0.005 s       9.4 kB\n expand_records                :      0.000 s       9.4 kB\n core                          :      0.000 s      59.3 kB\n listing                       :      0.003 s      59.3 kB\n```\n\nThe `dcore` option produces the file `core_fold_example.core`\ncontaining a listing of the Core Erlang code produced by the `core`\nparse (implemented by the module `v3_core`).\n\n```\n$ erlc +time +dcopt core_fold_example.erl\nCompiling \"core_fold_example\"\n parse_module                  :      0.000 s       9.4 kB\n transform_module              :      0.000 s       9.4 kB\n lint_module                   :      0.002 s       9.4 kB\n expand_records                :      0.000 s       9.4 kB\n core                          :      0.000 s      59.3 kB\n sys_core_fold                 :      0.000 s      25.3 kB\n core_transforms               :      0.000 s      25.3 kB\n listing                       :      0.002 s      25.3 kB\n```\n\nThe `dcopt` option produces the file `core_fold_example.copt`\ncontaining a listing of the Core Erlang code as it looks\nafter optimization by the `sys_core_fold` pass.\n\nAs was mentioned in my first blog post about the compiler,\n`compile:options()` will print most of the hidden options for\nthe compiler.\n\n## The most basic optimization\n\nThe most basic optimization done by `sys_core_fold` is constant propagation.\n\nConsider this Erlang function:\n\n```erlang\na() -\u003e\n    A = 42,\n    {ok,A}.\n```\n\nIt can be translated to Core Erlang like this:\n\n```\n'a'/0 =\n    fun () -\u003e\n       let \u003cA\u003e = 42\n       in {'ok',A}\n```\n\nThe variable `A` is bound to a constant (as opposed to an expression such\nas function call). We can replace all occurrences of the variable `A` with\nthe constant value `42` and eliminate the `let`:\n\n```\n'a'/0 =\n    fun () -\u003e\n\t{'ok',42}\n```\n\n## Optimizing `case` expressions\n\nActually, the first version of `a/0` that I showed was already\nslightly optimized by me.\n\nHere is the actual Core Erlang code (only slightly edited to\nremove annotations and unnecessary line breaks):\n\n```\n'a'/0 =\n    fun () -\u003e\n        case \u003c\u003e of\n\t  \u003c\u003e when 'true' -\u003e\n\t      let \u003cA\u003e = 42\n\t      in {'ok',A}\n\t  \u003c\u003e when 'true' -\u003e\n\t\tprimop 'match_fail'({'function_clause'})\n\tend\n```\n\nThe `let` has been wrapped in a useless outer `case`. The\n`case` would serve some purpose if there had been some function\narguments, but why complicate the code generator if `sys_core_fold` is\nperfectly capable of simplifying this code?\n\n`sys_core_fold` will simplify the code in several steps.\n\nFirst it will look at each clause. If a clause can't possibly\nbe executed (for example, it its guard is `false`) it will be\ndropped. If a clause will always match, all clauses following\nthe clause will be dropped.\n\nIn this case, the first clause will always match, because the\npattern is a list of no variables that can't fail to match, and\nthe guard is `true`. Thus the second clause is unreachable and\nis dropped:\n\n\n```\n'a'/0 =\n    fun () -\u003e\n        case \u003c\u003e of\n\t  \u003c\u003e when 'true' -\u003e\n\t      let \u003cA\u003e = 42\n\t      in {'ok',A}\n\tend\n```\n\nThe next step is to see if there is only one clause remaining.\nIf it is, the body of the clause can be kept and the `case`\neliminated:\n\n\n```\n'a'/0 =\n    fun () -\u003e\n       let \u003cA\u003e = 42\n       in {'ok',A}\n```\n\n## Another case example\n\nLet's see how a more complicated function can be optimized\nfollowing the steps just described. Consider this Erlang\nfunction:\n\n```erlang\naa() -\u003e\n    case {a,tuple} of\n\t[List] -\u003e List;\n\t{A,B} -\u003e  {tuple,A,B};\n\t_ -\u003e      something_else\n    end.\n```\n\nTranslated to Core Erlang code (with the outer `case` and\nannotations removed) it will look this:\n\n```\n'aa'/0 =\n    fun () -\u003e\n      case {'a','tuple'} of\n\t\u003c[List|[]]\u003e when 'true' -\u003e\n\t    List\n\t\u003c{A,B}\u003e when 'true' -\u003e\n\t    {'tuple',A,B}\n\t\u003c_@c1\u003e when 'true' -\u003e\n\t    'something_else'\n\t\u003c_@c0\u003e when 'true' -\u003e\n\t    primop 'match_fail'({'case_clause',_@c0})\n      end\n```\n\nLet's go through the clauses one by one:\n\n* The first clause will only match a list with exactly one element.\nThe `case` expression is a tuple, so the first clause can't\npossibly match. It will be dropped.\n\n* The second clause will match a tuple with (any) two elements.\nThe case expression is a tuple with two elements, so this clause\nwill always match.\n\n* There is no need to look at the remaining clauses, since the\nsecond clause will always match. The remaining clauses are dropped.\n\nWe now have:\n\n```\n'aa'/0 =\n    fun () -\u003e\n      case {'a','tuple'} of\n\t\u003c{A,B}\u003e when 'true' -\u003e\n\t    {'tuple',A,B}\n      end\n```\n\nThis is a `case` with just one clause, so we can keep\nthe body of the clause and remove the `case`. But there is\na problem if we do that naively:\n\n```\n'aa'/0 =\n    fun () -\u003e\n       {'tuple',A,B}\n```\n\nThe variables `A` and `B` are used, but they don't have\nany values bound to them. We must use a `let` to bind\nthe variables before they can be used:\n\n```\n'aa'/0 =\n    fun () -\u003e\n      let \u003cA,B\u003e = \u003c'a','tuple'\u003e\n      in {'tuple',A,B}\n```\n\nPropagating constants, the final code is:\n\n```\n'aa'/0 =\n    fun () -\u003e\n\t{'tuple','a','tuple'}\n```\n\n## Avoiding tuple building\n\nHere is an example of a common pattern of matching\nseveral expressions in parallel:\n\n```erlang\nb(A, B) -\u003e\n    case {A,B} of\n\t{true,false} -\u003e ok;\n\t{false,true} -\u003e not_ok;\n\t{_,_} -\u003e error\n    end.\n```\n\nThe unoptimized Core Erlang code looks like this:\n\n```\n'b'/2 =\n    fun (_@c1,_@c0) -\u003e\n\tcase \u003c_@c1,_@c0\u003e of\n\t  \u003cA,B\u003e when 'true' -\u003e\n\t      case {A,B} of\n\t\t\u003c{'true','false'}\u003e when 'true' -\u003e\n\t\t    'ok'\n\t\t\u003c{'false','true'}\u003e when 'true' -\u003e\n\t\t    'not_ok'\n\t\t\u003c{_@c5,_@c6}\u003e when 'true' -\u003e\n\t\t    'error'\n\t\t\u003c_@c2\u003e when 'true' -\u003e\n\t\t      primop 'match_fail'({'case_clause',_@c2})\n\t      end\n\tend\n```\n\nThe `case` expression is `{A,B}`. When executing the `case`\na tuple will built, and then almost immediately discarded.\nThat is wasteful. Therefore `sys_core_fold` rewrites the\ncode to eliminate the tuple building:\n\n```\n'b'/2 =\n    fun (_@c1,_@c0) -\u003e\n\tcase \u003c_@c1,_@c0\u003e of\n\t  \u003c'true','false'\u003e when 'true' -\u003e\n\t      'ok'\n\t  \u003c'false','true'\u003e when 'true' -\u003e\n\t      'not_ok'\n\t  \u003c_@c5,_@c6\u003e when 'true' -\u003e\n\t      'error'\n\tend\n```\n\nHere a value list is used instead of a tuple. (See\n[previous blog post](http://blog.erlang.org/core-erlang-by-example/)\nfor several examples of value lists.)\n\nAnother common pattern where tuples are built and immediately\ndiscarded is shown in this example:\n\n```erlang\nc(X) -\u003e\n    {A,B} = case X of\n\t\ta1 -\u003e {10,1};\n\t\tb2 -\u003e {20,2};\n\t\t_ -\u003e  {100,42}\n\t    end,\n    A+B.\n```\n\nThe unoptimized Core Erlang code looks like this:\n\n```\n'c'/1 =\n    fun (_@c0) -\u003e\n\tcase _@c0 of\n\t  \u003cX\u003e when 'true' -\u003e\n\t      let \u003c_@c2\u003e =\n\t\t  case X of\n\t\t    \u003c'a1'\u003e when 'true' -\u003e\n\t\t\t{10,1}\n\t\t    \u003c'b2'\u003e when 'true' -\u003e\n\t\t\t{20,2}\n\t\t    \u003c_@c5\u003e when 'true' -\u003e\n\t\t\t{100,42}\n\t\t    \u003c_@c1\u003e when 'true' -\u003e\n\t\t\t  primop 'match_fail'({'case_clause',_@c1})\n\t\t  end\n\t      in\n\t\t  case _@c2 of\n\t\t    \u003c{A,B}\u003e when 'true' -\u003e\n\t\t\tcall 'erlang':'+'(A, B)\n\t\t    \u003c_@c3\u003e when 'true' -\u003e\n\t\t\t  primop 'match_fail'({'badmatch',_@c3})\n\t\t  end\n\t  \u003c_@c4\u003e when 'true' -\u003e\n\t\t  primop 'match_fail'({'function_clause',_@c4})\n\tend\n```\n\nHere a tuple is built and assigned to `_@c2`. It is then matched\nin a `case`.\n\nFirst the code is optimized like this to eliminate the tuple building\nin each clause of the first `case`:\n\n```\n'c'/1 =\n    fun (_@c0) -\u003e\n\tlet \u003c_@f4,_@f5\u003e =\n\t    case _@c0 of\n\t      \u003c'a1'\u003e when 'true' -\u003e\n\t\t  \u003c10,1\u003e\n\t      \u003c'b2'\u003e when 'true' -\u003e\n\t\t  \u003c20,2\u003e\n\t      \u003c_@c5\u003e when 'true' -\u003e\n\t\t  \u003c100,42\u003e\n\t    end\n\tin\n            let \u003c_@c2\u003e = {_@f4,_@f5}\n            in\n\t          case _@c2 of\n\t\t    \u003c{A,B}\u003e when 'true' -\u003e\n\t\t\tcall 'erlang':'+'(A, B)\n\t\t    \u003c_@c3\u003e when 'true' -\u003e\n\t\t\t  primop 'match_fail'({'badmatch',_@c3})\n\t\t  end\n\tend\n```\n\nApplying all of the optimizations previously described,\nthe remaining tuple building and matching can be eliminated:\n\n```\n'c'/1 =\n    fun (_@c0) -\u003e\n\tlet \u003c_@f4,_@f5\u003e =\n\t    case _@c0 of\n\t      \u003c'a1'\u003e when 'true' -\u003e\n\t\t  \u003c10,1\u003e\n\t      \u003c'b2'\u003e when 'true' -\u003e\n\t\t  \u003c20,2\u003e\n\t      \u003c_@c5\u003e when 'true' -\u003e\n\t\t  \u003c100,42\u003e\n\t    end\n\tin\n\t    call 'erlang':'+'(_@f4, _@f5)\n```\n\n## Conclusion\n\nThat was a quick look at some of the optimizations done by\n`sys_core_fold`.\n\nSome of the optimizations are very simple. The power of the\n`sys_core_fold` pass comes from the combination of optimizations.  One\noptimization gives opportunities for other optimizations, as could be\nseen in the examples.\n\n## Points to Ponder\n\nWhy is the optimization pass called `sys_core_fold`?\n\nA hint can be found in the title of this Wikipedia article:\n[Constant folding](https://en.wikipedia.org/wiki/Constant_folding).\n\n"},{"id":"core-erlang-by-example","title":"Core Erlang by Example","author":"Björn Gustavsson","excerpt":"\nThis blog post is the first about the Core Erlang format. In this\nblog post, we introduce the Core Erlang format through examples\nthat compare Erlang code to the corresponding Core Erlang\ncode.","article_date":1525651200000,"tags":["compiler BEAM"],"frontmatter":{"layout":"post","title":"Core Erlang by Example","tags":"compiler BEAM","author":"Björn Gustavsson"},"content":"\nThis blog post is the first about the Core Erlang format. In this\nblog post, we introduce the Core Erlang format through examples\nthat compare Erlang code to the corresponding Core Erlang\ncode.\n\nI used the following command to translate my example module to\nCore Erlang code:\n\n```\n$ erlc +time +to_core core_example.erl\nCompiling \"core_example\"\n parse_module                  :      0.000 s      10.8 kB\n transform_module              :      0.000 s      10.8 kB\n lint_module                   :      0.003 s      10.8 kB\n expand_records                :      0.000 s      10.8 kB\n core                          :      0.000 s      89.9 kB\n sys_core_fold                 :      0.000 s      58.6 kB\n core_transforms               :      0.000 s      58.6 kB\n listing                       :      0.002 s      58.6 kB\n```\n\nThe [previous blog post](http://blog.erlang.org/compiler-lost-in-translation/)\nexplored the passes from `parse_module` to `expand_records`. The\n`core` passes translates from the abstract code to Core Erlang. We\nwill talk more about the Core Erlang passes in future blog posts.\n\nI have slightly edited the examples to make them somewhat easier to\nread. There will be an unedited example at the very end of this blog post.\n\nThere's a lot to cover, so let's get started!\n\n## The simplest function\n\nLet start with the simplest possible function, a function with no\narguments returning an atom:\n\n```erlang\nsimplest() -\u003e 'ok'.\n```\n\nIn Core Erlang, that will be:\n\n```\n'simplest'/0 =\n    fun () -\u003e\n\t'ok'\n```\n\nFrom that example, we can work out the following principles:\n\n* Atoms are always quoted.\n\n* Naming of the function has been separated from implementation\nof the function.\n\n* The body of a `fun` is not followed by an `end` as in Erlang.\n\n\n## Slightly less simple\n\nHere is as slightly more complicated function:\n\n```erlang\nid(I) -\u003e I.\n```\n\nIn Core Erlang:\n\n```\n'id'/1 =\n    fun (_@c0) -\u003e\n\t_@c0\n```\n\n**Note**: All examples were compiled with OTP 20. The name of the\ngenerated variables will be different in the upcoming OTP 21.\n\nEssentially, variables are named as in Erlang. In the translation\nto Core Erlang, the compiler generates new variable names for the\narguments in a function head. The following code is also valid\nCore Erlang:\n\n\n```\n'id'/1 =\n    fun (I) -\u003e\n\tI\n```\n\n## More than one clause\n\nHere is a function with more than one clause:\n\n```erlang\na(42) -\u003e ok;\na(_) -\u003e error.\n```\n\nIn Core Erlang:\n\n```\n'a'/1 =\n    fun (_@c0) -\u003e\n\tcase _@c0 of\n\t  \u003c42\u003e when 'true' -\u003e\n\t      'ok'\n\t  \u003c_@c2\u003e when 'true' -\u003e\n\t      'error'\n\tend\n```\n\n* A `fun` can only have a single clause.\n\n* Pattern matching must be done in a `case`, not in the `fun` head.\n\n* Guards are mandatory for each clause in a `case`.\n\n* `_` is **not** a valid variable name in Core Erlang. Uninteresting\nvalues must be bound to a new variable.\n\n* The `\u003c` and `\u003e` around the patterns will be explained soon.\n\nIn Erlang, multiple function clauses can also be written with a\n`case` like this:\n\n```erlang\nb(N) -\u003e\n    case N of\n        42 -\u003e ok;\n        _ -\u003e error\n    end.\n```\n\nThe Core Erlang code will be essentially the same as the Core Erlang\ncode for `a/1`:\n\n```\n'b'/1 =\n    fun (_@c0) -\u003e\n\tcase _@c0 of\n\t  \u003c42\u003e when 'true' -\u003e\n\t      'ok'\n\t  \u003c_@c3\u003e when 'true' -\u003e\n\t      'error'\n\tend\n```\n\n## Two clauses, three arguments\n\nLet's try multiple arguments:\n\n```erlang\nc(inc, Base, N) -\u003e\n    Base+N;\nc(_, Base, _) -\u003e\n    Base.\n```\n\nIn Core Erlang:\n\n```\n'c'/3 =\n    fun (_@c2,_@c1,_@c0) -\u003e\n\tcase \u003c_@c2,_@c1,_@c0\u003e of\n\t  \u003c'inc',Base,N\u003e when 'true' -\u003e\n\t      call 'erlang':'+'(Base, N)\n\t  \u003c_@c6,Base,_@c7\u003e when 'true' -\u003e\n\t      Base\n\tend\n```\n\n* `\u003c` and `\u003e` denote a **value list**. The patterns in each clause in\nthe `case` are always part of a value list. The `case` expression is\na value list unless there is only one expression.\n\n* Operators such as `+` are not part of the Core Erlang language,\nso the compiler has translated the use of `+` to a call to the\nBIF `erlang:'+'/2`.\n\n## If\n\nLet's see how `if` is implemented:\n\n```erlang\nd(A, B) -\u003e\n    if\n        A \u003e B -\u003e\n            greater;\n        true -\u003e\n            not_greater\n    end.\n```\n\nIn Core Erlang:\n\n```\n'd'/2 =\n    fun (_@c1,_@c0) -\u003e\n\tcase \u003c\u003e of\n\t  \u003c\u003e when call 'erlang':'\u003e'(_@c1, _@c0) -\u003e\n\t      'greater'\n\t  \u003c\u003e when 'true' -\u003e\n\t      'not_greater'\n\tend\n```\n\n* The `case` expression and the patterns are each value lists with\nzero elements. All the action is in the guards.\n\n## Repeated variables\n\nIn Erlang, a variable can be repeated in a clause or within a\npattern to indicate that the values must be the same:\n\n```erlang\ncmp(Same, Same) -\u003e same;\ncmp(_, _) -\u003e different.\n```\n\nCore Erlang does not allow repeating a variable:\n\n```\n'cmp'/2 =\n    fun (_@c1,_@c0) -\u003e\n\tcase \u003c_@c1,_@c0\u003e of\n\t  \u003cSame,_@c4\u003e when call 'erlang':'=:='(_@c4, Same) -\u003e\n\t      'same'\n\t  \u003c_@c5,_@c6\u003e when 'true' -\u003e\n\t      'different'\n\tend\n```\n\n* Here the second occurence of the variable `Same` has been renamed to\na new variable named `_@c4`, and a guard has been added to compare\n`Same` and `_@c4`.\n\n## Exceptions\n\nThis function will fail with a `function_clause` exception if it is called\nwith any other value than `42`:\n\n```erlang\ne(42) -\u003e ok.\n```\n\nIn Core Erlang:\n\n```\n'e'/1 =\n    fun (_@c0) -\u003e\n\tcase _@c0 of\n\t  \u003c42\u003e when 'true' -\u003e\n\t      'ok'\n\t  \u003c_@c1\u003e when 'true' -\u003e\n\t      primop 'match_fail'({'function_clause',_@c1})\n\tend\n```\n\n* A `case` in Core Erlang must not fall off at the end, that is,\nthere must always be a clause that will match.\n\n* In this example, the last clause with a variable pattern and\na `true` guard is guaranteed to match.\n\n* The body for the last clause calls a **primop** to generate\na function clause exception. Primops are primitive operations\nprovided by the Erlang implementation, but not specified in the\nCore Erlang language specification.\n\nHere is a similar function excepts that is uses `case` and therefore\nwill generate a `case_clause` exception if called with any other\nargument than `42`:\n\n```erlang\nf(N) -\u003e\n    case N of\n        42 -\u003e ok\n    end.\n```\n\nThe Core Erlang code is similar to the code for `e/1`:\n\n\n```\n'f'/1 =\n    fun (_@c0) -\u003e\n\tcase _@c0 of\n\t  \u003c42\u003e when 'true' -\u003e\n\t      'ok'\n\t  \u003c_@c1\u003e when 'true' -\u003e\n\t      primop 'match_fail'({'case_clause',_@c1})\n\tend\n```\n\n* The only difference is the argument for the `match_fail` primop.\n\nLet's rewrite this function one more time:\n\n```erlang\ng(N) -\u003e\n    42 = N,\n    ok.\n```\n\nIn Core Erlang:\n\n```\n'g'/1 =\n    fun (_@c0) -\u003e\n\tcase _@c0 of\n\t  \u003c42\u003e when 'true' -\u003e\n\t      'ok'\n\t  \u003c_@c1\u003e when 'true' -\u003e\n\t      primop 'match_fail'({'badmatch',_@c1})\n\tend\n```\n\n* Again, the only difference is the argument for the `match_fail` primop.\n\n## Binding variables using 'let'\n\nHere is a function that binds the variable `I`:\n\n```erlang\nh(A) -\u003e\n    I = id(A),\n    I + A.\n```\n\nIn Core Erlang:\n\n```\n'h'/1 =\n    fun (_@c0) -\u003e\n\tlet \u003cI\u003e =\n              apply 'id'/1(_@c0)\n\tin\n              call 'erlang':'+'(I, _@c0)\n```\n\n\n* `apply` calls a fun or local function.\n\n* The return value of the `apply` is bound to the variable `I`.\n\n* The variable `I` can only be used in the code that follows the\n`in` keyword.\n\n* The variable name is in a value list. That is because `let`\ncan bind several variables at once.\n\n## Binding more than one variable in a 'let'\n\nErlang has essentially no scoping. When a variable has been bound,\nit remains bound to the end of the function. For example, variables bound\nin a `case` can be used after the `case`:\n\n```erlang\ni(E) -\u003e\n    case E of\n        a -\u003e\n            X = 1,\n            Y = 10;\n        b -\u003e\n            X = 23,\n            Y = 17\n    end,\n    {X,Y}.\n```\n\nIn Core Erlang:\n\n```\n'i'/1 =\n    fun (_@c0) -\u003e\n\tlet \u003c_@c7,X,Y\u003e =\n\t    case _@c0 of\n\t      \u003c'a'\u003e when 'true' -\u003e\n\t\t  \u003c10,1,10\u003e\n\t      \u003c'b'\u003e when 'true' -\u003e\n\t\t  \u003c17,23,17\u003e\n\t      \u003c_@c5\u003e when 'true' -\u003e\n\t\t  primop 'match_fail'({'case_clause',_@c5})\n\t    end\n\tin\n\t    {X,Y}\n```\n\n* A `case` in Core Erlang does not export any variables. All variables\nthat are to be used after the `case` must be explicitly returned.\n\n* In this example, the first two clauses of the `case` return a\nvalue list with **three** values. The first value is the return value\nof the case, which in this case is ignored. The other two values are\nthe values assigned to the `X` and `Y` variables, respectively.\n\n* The values returned from the `case` is bound in the `let`. The ignored\nreturn value is bound to a new variable (`_@c7`), which is never used.\nThe exported values are bound to the `X` and `Y` variables.\n\n## The unedited Core Erlang code\n\nSo far all Core Erlang examples have been edited to make the points\nI am trying to make stand out clearer. Let's have a look at the unedited\nversion of a previous example:\n\n```\n'e'/1 =\n    %% Line 33\n    fun (_@c0) -\u003e\n\tcase _@c0 of\n\t  \u003c42\u003e when 'true' -\u003e\n\t      'ok'\n\t  ( \u003c_@c1\u003e when 'true' -\u003e\n\t\t( primop 'match_fail'\n\t\t      ({'function_clause',_@c1})\n\t\t  -| [{'function_name',{'e',1}}] )\n\t    -| ['compiler_generated'] )\n\tend\n```\n\n* The `-|` associates an annotation with a Core Erlang construct.\nThe meaning of an annotation is not specified in the Core Erlang\nlanguage specification.\n\n* The `compiler_generated` annotation associated with the last clause\nis a hint added by the compiler that subsequent optimization passes should\nnot generate a warning if the clause was found to never match and dropped.\n\n* The comment \"Line 33\" at the beginning is actually an annotation that\nthe pretty printer has turned into a comment to avoid rendering the\npretty-printed code unreadable.\n\n## Conclusion\n\nCore Erlang is less complicated than Erlang, and is therefore more\nsuited than the abstract format for code analyzing tools (such as\n[Dialyzer][dialyzer]) and optimizers.\n\n[dialyzer]: http://erlang.org/doc/apps/dialyzer/dialyzer_chapter.html\n\n## To learn more about Core Erlang\n\nAll details can be found in [Core Erlang 1.0.3 language specification][core].\n\n[core]: https://www.it.uu.se/research/group/hipe/cerl/doc/core_erlang-1.0.3.pdf\n"},{"id":"Memory-instrumentation-in-OTP-21","title":"Memory instrumentation in OTP 21","author":"John Högberg","excerpt":"\nThe memory instrumentation module was rewritten for Erlang/OTP 21 to make it\neasier to use. In this post I'll describe the rationale behind the new features\nand how to make use of them.","article_date":1525219200000,"tags":["erts memory instrumentation"],"frontmatter":{"layout":"post","title":"Memory instrumentation in OTP 21","tags":"erts memory instrumentation","author":"John Högberg"},"content":"\nThe memory instrumentation module was rewritten for Erlang/OTP 21 to make it\neasier to use. In this post I'll describe the rationale behind the new features\nand how to make use of them.\n\nOne of the most important features that a diagnostic tool can have is the\nability to work on the fly. If it requires a restart then the condition you're\ntrying to diagnose might vanish, and you can't use it to troubleshoot issues on\n\"production\" systems.\n\nThe previous implementation had a few major issues to this effect; you had to\nstart the Erlang VM with a certain flag, accept considerable overhead, and\nworst of all suspend the VM while it collected all its data.\n\nThe amount of data it collected was also quite problematic; with one entry for\nevery single allocation it was difficult to tell what was hiding in all that\ninformation, and since there was no way to tell whether a gap between two\nallocations was mapped or not it was needlessly difficult to use when trying\nto troubleshoot memory fragmentation.\n\nThe new implementation tackles these problems by scanning existing data\nstructures to lower its overhead to the point it can be turned on by default,\nand tries to collect information in a manner that doesn't harm the\nresponsiveness of the system.\n\n## Carriers and memory fragmentation\n\nThe VM allocates memory in large segments we call \"carriers\" and then allocates\nblocks within those. This has many benefits; since each carrier is completely\nseparate from the others it's easy to determine when they can be returned to\nthe operating system, and they scale very well since we can guarantee that\nthey're only modified by per-thread instances which makes allocation and\ndeallocation wait-free in most cases.\n\nThere are two types of carriers; single-block which always contain one large\nblock, and multi-block that can contain several smaller blocks. While both of\nthese rely on the operating system to minimize address space fragmentation, the\nlatter kind can also become internally fragmented which will result in new\ncarriers being created if no existing multi-block carrier can satisfy an\nallocation, even if the amount of unused memory exceeds the request.\n\nWhile you can glean some information about average carrier utilization from\n`erlang:system_info({allocator, Alloc})` and use `pmap` (or similar) to get an\nidea of how fragmented the address space is, it has always been a pain to get\ninformation about the individual carriers. Starting in OTP 21 you can ask the\nsystem for a list of all carriers without having to start the VM with any\nparticular flags. The list contains information about each carrier's total\nsize, combined allocation size, allocation count, whether it's in the migration\npool, and a histogram over free block sizes.\n\nWe chose to represent the free blocks with histograms (log2, starting at 512 by\ndefault) as they make it easy to tell at a glance whether a carrier has\nfragmentation issues; if there's a lot of free blocks clustered along the left\nside then it's pretty safe to say there's a problem.\n\nIn the example below, the `ll_alloc` carrier has no free blocks at all, the\n`binary_alloc` and `eheap_alloc` ones look healthy with a few very large\nblocks, and the `fix_alloc` carrier is somewhat fragmented with ~3KB free split\ninto 22 blocks smaller than 512 bytes (although this is not a problem for this\nallocator type).\n\n```erlang\n1\u003e instrument:carriers().\n{ok,{512,\n     [{ll_alloc,1048576,0,1048344,71,false,{0,0,0,0,0,0,0,0,0,0,0,0,0,0}},\n      {binary_alloc,1048576,0,324640,13,false,{3,0,0,1,0,0,0,2,0,0,0,0,0,0}},\n      {eheap_alloc,2097152,0,1037200,45,false,{2,1,1,3,4,3,2,2,0,0,0,0,0,0}},\n      {fix_alloc,32768,0,29544,82,false,{22,0,0,0,0,0,0,0,0,0,0,0,0,0}},\n      {...}|...]}}\n```\n\n(`instrument:carriers/1` can be used to tweak the histograms and which\nallocators to look in.)\n\n## Allocations and memory utilization\n\nThose who have used `erlang:memory()` are probably familiar with how annoyingly\ngeneral the `system` category can be. It's possible to get a bit more\ninformation by using `erlang:system_info({allocator, Alloc})` but the most it\nwill do is tell you that it's (say) `driver_alloc` that eats all that memory\nand leave you with no clue which one.\n\nWhile it's often easy to tell which driver or NIF is causing problems while\nyou're developing, it's not as easy when it's used in anger alongside half a\ndozen others. The new \"allocation tagging\" feature will help you figure out\nwhere the memory went at the cost of one word per allocation. The allocations\nare presented as block size histograms (similar to carrier information)\ngrouped by their origin and type:\n\n```erlang\n2\u003e instrument:allocations()\n{ok,{128,0,\n     #{udp_inet =\u003e\n           #{driver_event_state =\u003e {0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0}},\n       tty_sl =\u003e\n           #{io_queue =\u003e {0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},\n             drv_internal =\u003e {0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0}},\n       system =\u003e\n           #{db_segment =\u003e {0,0,0,0,0,18,0,0,1,0,0,0,0,0,0,0,0,0},\n             heap =\u003e {0,0,0,0,20,4,2,2,2,3,0,1,0,0,1,0,0,0},\n             thr_prgr_data =\u003e {38,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},\n             db_term =\u003e {271,3,1,52,80,1,0,0,0,0,0,0,0,0,0,0,0,0},\n             code =\u003e {0,0,0,5,3,6,11,22,19,20,10,2,1,0,0,0,0,0},\n             binary =\u003e {18,0,0,0,7,0,0,1,0,0,0,0,0,0,0,0,0,0},\n             atom_entry =\u003e {8681,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},\n             message =\u003e {0,40,78,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0},\n             ... }\n       spawn_forker =\u003e\n           #{driver_select_data_state =\u003e\n                 {1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}},\n       ram_file_drv =\u003e #{drv_binary =\u003e {0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0}},\n       prim_file =\u003e\n           #{process_specific_data =\u003e {2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},\n             nif_trap_export_entry =\u003e {0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},\n             monitor_extended =\u003e {0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},\n             drv_binary =\u003e {0,0,0,0,0,0,1,0,3,5,0,0,0,1,0,0,0,0},\n             binary =\u003e {0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}},\n       prim_buffer =\u003e\n           #{nif_internal =\u003e {0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},\n             binary =\u003e {0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}}}}}\n```\n\nThe above example was taken with allocation tagging turned on for all\nallocators (`+Muatags true` command line argument) to give you a better idea of\nwhat it can do. By default it will only be turned on for driver/NIF allocations\nand binaries since those are the most common culprits, and their allocations\nare generally so large that the overhead of a single word is a drop in the\nbucket.\n\n(As with carriers, `instrument:allocations/1` can be used to tweak the\nhistograms and which allocators to look in.)\n\n## Further reading\n\nFor those who'd like to know more about how our memory allocators work, Lukas\nLarsson's [talk at EUC 2014](https://erlangcentral.org/videos/euc-2014-lukas-larsson-memory-allocators-in-the-vm-memory-management-battle-stories/) is a good primer. Our internal\ndocumentation on [carrier migration](https://github.com/erlang/otp/blob/master/erts/emulator/internal_doc/CarrierMigration.md) and [inter-thread deallocation](https://github.com/erlang/otp/blob/master/erts/emulator/internal_doc/DelayedDealloc.md) may also be of interest.\n\nThe PR implementing this change can be found [here](https://github.com/erlang/otp/pull/1790), and the documentation for the old instrumentation module can be found [here](http://erlang.org/documentation/doc-9.3/lib/tools-2.11.2/doc/html/instrument.html).\n"},{"id":"My-OTP-21-Highlights","title":"My OTP 21 Highlights","author":"Lukas Larsson","excerpt":"\nOTP-21 Release Candidate 1 has just been released. I thought that I would go\nthrough the changes that I am the most excited about. Most likely this will\nmostly mean features in erts and the core libraries as those are the\nchanges that I am the most familiar with.","article_date":1525219200000,"tags":["otp 21 release"],"frontmatter":{"layout":"post","title":"My OTP 21 Highlights","tags":"otp 21 release","author":"Lukas Larsson"},"content":"\nOTP-21 Release Candidate 1 has just been released. I thought that I would go\nthrough the changes that I am the most excited about. Most likely this will\nmostly mean features in erts and the core libraries as those are the\nchanges that I am the most familiar with.\n\nYou can download the readme describing the changes here: [OTP 21-RC1 Readme](http://erlang.org/download/otp_src_21.0-rc1.readme).\nOr, as always, look at the release notes of the application you are interested in.\nFor instance here: [OTP 21-RC1 Erts Release Notes](http://erlang.org/doc/apps/erts/notes.html).\n\n# Compiler / Interpreter #\n\nBjörn Gustavsson has been doing a lot of work with the compiler and interpreter\nthe last year while I have been sitting next to him cheering. The largest changes\nis part of the OTP-14626 ticket. While working on the\n[BEAMJIT](https://www.youtube.com/watch?v=PtgD5WRzcy4) development\nI've been looking a lot at the [luajit](http://luajit.org/) project and what\nMike Pall has done both in the JIT but also in the interpreter. Inspired by\nthis and some other ideas that we got from the BEAMJIT project we decided it was time\nto do a major overhaul of the way that the BEAM interpreter is created. Most of the\nchanges done boil down to decreasing the size of beam code in memory, thus making more\ncode fit in the L1/L3 caches and in extension making code run faster. We've\ndecreased the loaded code size by about 20% using our optimizations. This has\ntranslated to about a 5% performance increase for most Erlang code\nwhich is quite amazing. Me or Björn will most likely write more about exactly\nwhat this has entailed in a future blogpost.\n\nAnother compiler change that has had quite a large impact (at least in our benchmarks)\nis OTP-14505 contributed by José Valim in [PR 1080](http://github.com/erlang/otp/pull/1080).\nThe change makes the compiler re-write:\n\n    example({ok, Val}) -\u003e {ok, Val}.\n\nto\n\n    example({ok, Val} = Tuple) -\u003e Tuple.\n\neliminating the extra creation of the tuple. As it turns out this is a quite\ncommon pattern in Erlang code so this will be good for all programs.\n\nAn example of this performance gain can be seen in the estone benchmarks SUITE\nbelow. OTP-14626 together with some other compiler and erts improvements have\nincreased the number of stones from 370000 in OTP-20.3 (the green line), to\n400000 in OTP-21 (the blue line). So about 7.5%.\n\n![Estone OTP-21 benchmark](../images/estone_otp21_benchmark.png)\n\n# Erlang run-time system #\n\nThere are many changes in the run-time system.\n\n## File handling ##\n\nAll file IO has traditionally been handled through a port. In OTP-21 all of the\nfile IO has been rewritten to use nifs instead, OTP-14256. This was mainly done\nin order to run file operation in the dirty IO schedulers. It also had the nice\nside-effect of significantly increasing throughput of certain operations.\n\n![File tiny reads OTP-21 benchmark](../images/file_tiny_reads_otp21_benchmark.png)\n\nFor instance in the tiny reads benchmark OTP-21 (the blue line) is about 2.8 times\nfaster than OTP-20.3 (the green line).\n\nAlso it is now possible to open device files using file:open, see OTP-11462.\n\n## I/O Polling ##\n\nThe entire underlying mechanism for checking for I/O on sockets has been rewritten\nand optimized for modern OS kernel polling features. See OTP-14346 and\n[I/O polling options in OTP 21]({{ site.baseurl }}/IO-Polling) for more details.\n\n## Distribution ##\n\nIt has always been possible to write your own distribution carrier if you want\nto if, for instance, you wanted to use [RFC-2549](https://tools.ietf.org/html/rfc2549)\nto send your distributed Erlang messages. However you have had to implement it as\na linked-in driver. With the introduction of OTP-14459 you can now use a process\nor port as the distribution carrier. So now you can use gen_pigeon instead of having\nto call the boost equivalent.\n\nThe ability to use processes as distribution carriers is now used by the TLS\ndistribution. This allows us to not have to jump through several hoops as was done\nbefore increasing the throughput of TLS distribution significantly.\n\n## Process signals ##\n\nWhen running benchmarks using cowboy and hammering it with connections that\ndo not use keep-alive, one of the SMP scalability bottlenecks that pop up\nis the link lock of the supervisor that supervises all the connections.\nThe reason why this lock pops up is because when you have a lot of linked\nprocesses, the rb-tree in which the links are stored becomes very large so\nthe insertion and deletion time increases. In OTP-14589 this has been\nchanged so that all link and monitor requests now are sent as messages\nfor the receiving process to take care of. This means that the lock has been\ncompletely removed. Now all signals (be they messages, links, monitors,\nprocess\\_info, group\\_leader etc) are handled through the same queue.\n\nIn addition, OTP-14901 now makes it so that monitor + send signals\nare merged into one signal. So the contention is reduced even further\nfor gen_server:call like functions.\n\n![GenStress OTP-21 benchmark](../images/genstress_otp21_benchmark.png)\n\nThe performance difference is quite significant. The genstress benchmark\nseen above OTP-21 (the blue line) has almost doubled in throughput\ncompared to OTP-20.3 (the green line).\n\n# Logger\n\nOTP-13295 adds a completely new logging framework for Erlang/OTP. It is\ninspired by the way that [lager](https://github.com/erlang-lager/lager),\nthe [Elixir Logger](https://hexdocs.pm/logger/Logger.html) and the [Python\nlogger](https://docs.python.org/3/howto/logging.html) works.\nWith logger the logging handlers can intercept the logging\ncall in the process that does the actual call instead of having to\nwait for a message. This opens up all sorts of possibilities of early\nrejection of log messages in case of an overload, see [Logger User's Guide](http://erlang.org/documentation/doc-10.0-rc1/lib/kernel-6.0/doc/html/logger_chapter.html#protecting-the-handler-from-overload)\nfor more details. The user can also add special purpose filters that are run\nbefore the handler is invoked in order to silence or amend log messages in the system.\n\n# Misc\n\nHiPE has finally been fixed by Magnus Lång to use the receive reference optimization\nthat beam has had for a long time, OTP-14785.\n\nThe ftp and tfpt parts of inets have been separated into their own applications\ninstead of being bundled, OTP-14113.\n\nThe rand module has seen a lot of work, adding new features. I'm not sure when or\nhow the difference is useful, but the theory around this is fascinating, OTP-13764.\n\nThe maps module now has an maps:iterator/0 and maps:next/1, OTP-14012.\n\nio_lib:format/3 has been added to limit the output of the functions. This is especially\nuseful when building logging frameworks as you may get arbitrarily large terms to\nformat and may want to cut them in order to not overwhelm the system, OTP-14983.\n\nAs a final note, I'm not sure if anyone noticed, but as of OTP-20.3, processes that\nare in the state GARBING when your system crashes now have stack traces in the\ncrash dump!!!\n"},{"id":"compiler-lost-in-translation","title":"Lost in Translation (Exploring the Compiler's Front End)","author":"Björn Gustavsson","excerpt":"\nIn this blog post, we will explore the compiler passes that make up\nthe compiler's front end.","article_date":1524700800000,"tags":["compiler BEAM"],"frontmatter":{"layout":"post","title":"Lost in Translation (Exploring the Compiler's Front End)","tags":"compiler BEAM","author":"Björn Gustavsson"},"content":"\nIn this blog post, we will explore the compiler passes that make up\nthe compiler's front end.\n\nIn the [previous blog post](http://blog.erlang.org/compiler-time-option/)\nwe showed how the `time` option shows information about the compiler passes\nbeing executed:\n\n\n```\n$ erlc +time trivial.erl\nCompiling \"trivial\"\n remove_file                   :      0.000 s       3.7 kB\n parse_module                  :      0.000 s       5.5 kB\n transform_module              :      0.000 s       5.5 kB\n lint_module                   :      0.002 s       5.5 kB\n expand_records                :      0.000 s       5.3 kB\n     .\n     .\n     .\n```\n\nWe explained what the `remove_file` pass does in the previous\nblog post. In today's blog post, we will discuss the other passes\nlisted in the output above.\n\nThose passes makes up the compiler's front end. The implementation\nmodules for those passes are not in the **compiler** application, but\nin **STDLIB**. The reason is that the Erlang shell also uses those\nmodules. That means that the shell will work in an embedded system\nthat does not include the **compiler** application.\n\nThe front end passes operate on the **abstract format**. The abstract\nformat is fairly close to the original Erlang source code. In fact, by\npretty-printing the abstract format, we can reconstruct the original\nsource code, albeit not perfectly.\n\n## Lost in translation ##\n\nTo see how much we will lose in translation, we will compile and\npretty-print this module:\n\n```\n-module(trivial).\n-export([example/4]).\n-record(rec, {mod,func,result}).\n\n%% Example to help explore the compiler front end.\nexample(A, B, C, D) -\u003e\n    #rec{mod=?MODULE,func=?FUNCTION_NAME,result=A + (B*C*(D+42))}.\n```\n\nWe use `-P` option to run the `parse_module` pass and produce\na listing of the result:\n\n```\n$ erlc -P +time trivial.erl\nCompiling \"trivial\"\n parse_module                  :      0.000 s       5.5 kB\n transform_module              :      0.000 s       5.5 kB\n lint_module                   :      0.003 s       5.5 kB\n listing                       :      0.001 s       5.5 kB\n```\n\nFor the moment, ignore the `transform_module` and `erl_lint` passes.\nThey don't change the abstract code for this module. The `listing`\npass pretty prints the abstract format, converting it back to Erlang\nsource code and creating the file `trivial.P`.\n\n\n```\n$ cat trivial.P\n-file(\"trivial.erl\", 1).\n\n-module(trivial).\n\n-export([example/4]).\n\n-record(rec,{mod,func,result}).\n\nexample(A, B, C, D) -\u003e\n    #rec{mod = trivial,func = example,result = A + B * C * (D + 42)}.\n```\n\nComparing the `trivial.P` file to the original, we can see what was\nlost in translation:\n\n* The `?MODULE` and `?FUNCTION_NAME` macro invocations have been\nreplaced with `trival` and `example`, respectively. That was done by\nthe preprocessor.\n\n* The comment has disappeared. There are also several differences in the\namount of whitespace surrounding variables and operators. The abstract format\ndoes not include whitepace or comments in its representation.\n\n* Also note that a redundant pair of parentheses has been omitted in the\nexpression `A + (B*C*(D+42))`. The parentheses around `D+42` are still there\nbecause otherwise the value of the expression would change. The abstract\nformat has no direct representation of parenheses.\n\n## Looking closer at the parse_module pass ##\n\nNow that we have seen what is lost in translation, we will take a\ncloser look at the abstract format.\n\nWe will use the expression `A+(B*C*(D+42))` as an example and\ntranslate it to the abstract format using the same modules that the\n`parse_module` pass uses to do its work.\n\n### Tokenizing using erl_scan ###\n\nThe first step in the translation from Erlang source code is to group\nthe characters into logical groups called **tokens**. This process is\ncalled **tokenization** or **scanning**, and is done by the `erl_scan`\nmodule.\n\nWe will use `erl_scan:string/1` to tokenize our example. (The\ncompiler will use other functions in `erl_scan`, but the principle\nis the same.)\n\n```\n1\u003e {ok,Tokens,_} = erl_scan:string(\"A + (B*C*(D+42)).\"), Tokens.\n[{var,1,'A'},\n {'+',1},\n {'(',1},\n {var,1,'B'},\n {'*',1},\n {var,1,'C'},\n {'*',1},\n {'(',1},\n {var,1,'D'},\n {'+',1},\n {integer,1,42},\n {')',1},\n {')',1},\n {dot,1}]\n```\n\nThe output is a list of tokens. The second element in each tuple\nis the line number. The first element is the category of the token.\nIf there is a third element, it is the symbol within that category.\n\nWe can see that whitespace has already been lost. Had there been\na comment, it would have been lost too.\n\nTo read more details about tokens, see [erl_scan:string/1].\n\n[erl_scan:string/1]: http://erlang.org/doc/man/erl_scan.html#string-1\n\n### Preprocessing the tokens ###\n\nIn the compiler, the next step would be to run the preprocessor\non the tokens. In this example, there are no macro invocations\nand thus nothing to preprocess, so we will skip to the next step.\n\n### Parsing using erl_parse ###\n\nThe next step is to **parse** the tokens to produce the abstract\nformat:\n\n```\n2\u003e {ok,Abstract} = erl_parse:parse_exprs(Tokens), Abstract.\n[{op,1,'+',\n     {var,1,'A'},\n     {op,1,'*',\n         {op,1,'*',{var,1,'B'},{var,1,'C'}},\n         {op,1,'+',{var,1,'D'},{integer,1,42}}}}]\n```\n\nThe result is a list with one expression. The expression is not a\nlist, but a **parse tree**. It can be visualized like this:\n\n![Abstract format visualized](../images/compiler-2018-04-26.svg)\n\nThe parentheses have been lost, because the structure of the tree\nmakes the evaluation order unambiguous.\n\nSee [The Abstract Format] for more details about the abstract format.\n\n[The Abstract Format]: http://erlang.org/doc/apps/erts/absform.html\n[parse tree]: ../images/compiler-2018-04-26.svg\n\n### Pretty-printing using erl_pp ###\n\nThe `listing` pass uses the [erl_pp] module to pretty print the\nabstract format to produce a listing file.\n\nWe can pretty print the abstract format of the example:\n\n```\n3\u003e lists:flatten(erl_pp:exprs(Abstract)).\n\"A + B * C * (D + 42)\"\n```\n\nHere the pretty printer has inserted one pair of parentheses, but the\nredundant pair of parentheses in the original expression has been lost.\nThe whitespace is also different from the original.\n\n[erl_pp]: http://erlang.org/doc/man/erl_pp.html\n\n### A quick look at the preprocessor ###\n\nA mentioned in passing, the preprocessor (the [epp] module) is run\nafter tokenization and before parsing.\n\nThe preprocessor goes through the tokens, looking for a question\nmark followed by a variable or atom. For example, `?MODULE` in\na source file would be tokenized like this by `erl_scan`:\n\n    [{'?',1},{var,1,'MODULE'}]\n\nAssuming that the module name is `trivial`, the preprocessor will\nreplace those tokens with the token:\n\n    [{atom,1,trivial}]\n\n[epp]: http://erlang.org/doc/man/epp.html\n\n\n## The other passes operating on the abstract format ##\n\nNow that `parse_module` has been explained, let's take quick look at the\nother passes in the front end.\n\n### The transform_module pass ###\n\nThe `transform_module` pass runs parse transforms, for example\nfor [QLC] or [ms_transform].\n\n[QLC]: http://erlang.org/doc/man/qlc.html\n[ms_transform]: http://erlang.org/doc/man/ms_transform.html\n\n### The lint_module pass ###\n\nThe `lint_module` pass verifies that the code is semantically\ncorrect. That is, variables must be bound before they are used,\nall clauses for a function must have the same number of arguments,\nand so on.\n\nWhen we compile a module with problems, [erl_lint] will print\nerror messages and terminate the compilation:\n\n```\n$ cat bug.erl\n-module(bug).\n-export([main/0]).\n\nmain() -\u003e\n    A+B.\n$ erlc +time bug.erl\nCompiling \"bug\"\n remove_file                   :      0.000 s       2.1 kB\n parse_module                  :      0.000 s       2.7 kB\n transform_module              :      0.000 s       2.7 kB\n lint_module                   :      0.004 s       2.4 kB\nbug.erl:5: variable 'A' is unbound\nbug.erl:5: variable 'B' is unbound\n$\n```\n\n[erl_lint]: http://erlang.org/doc/man/erl_lint.html\n\n### Translating records ###\n\nThe `expand_records` pass uses [erl_expand_records] to translate\nrecords:\n\n```\n$ erlc -E +time trivial.erl\nCompiling \"trivial\"\n parse_module                  :      0.000 s       5.5 kB\n transform_module              :      0.000 s       5.5 kB\n lint_module                   :      0.002 s       5.5 kB\n expand_records                :      0.000 s       5.3 kB\n listing                       :      0.001 s       5.3 kB\n$ cat trivial.E\n-file(\"trivial.erl\", 1).\n\n-module(trivial).\n\n-export([example/4]).\n\n-record(rec,{mod,func,result}).\n\nexample(A, B, C, D) -\u003e\n    {rec,trivial,example,A + B * C * (D + 42)}.\n```\n\nThe `-E` option produces a listing of the abstract format\nproduced by the `expand_records` pass.\n\nThe `-record()` declaration is still there, but the construction of\nthe record has been replaced with construction of a tuple. Similarly,\nmatching of records will be translated to matching of tuples.\n\n[erl_expand_records]: http://erlang.org/doc/man/erl_expand_records.html\n\n## Tip: Producing a single source file using -P ##\n\nThe `-P` option can be used to package a source file that includes\nmultiple include files into a single self-contained source file.\n\nHaving a self-contained source file is useful if you want to report\na compiler bug, but don't have the time to minimize the source code\nto a minimum example.\n\nHere is an example. The `compile.erl` file includes two header files.\nCompiling it directly like this will not work:\n\n```\n$ cd lib/compiler/src\n$ erlc compile.erl\ncompile.erl:36: can't find include file \"erl_compile.hrl\"\n   .\n   .\n   .\n$\n```\n\nWe must give the path to the `include` directories of both\nKernel and STDLIB:\n\n```\n$ erlc -I ../../kernel/include -I ../../stdlib/include compile.erl\n$\n```\n\nTo package the source from `compile.erl` as well as the contents\nof the header files, use the `-P` option to generate `compile.P`:\n\n\n```\n$ erlc -P -I ../../kernel/include -I ../../stdlib/include compile.erl\n```\n\n`compile.P` can be renamed to `compile.erl` and successfully\ncompiled without any additional options:\n\n```\n$ mv compile.P $HOME/compile.erl\n$ cd $HOME\n$ erlc compile.erl\n$\n```\n\n## Points to Ponder ##\n\nThe preprocessor is run after tokenization, before running the\nparser.\n\nSo how are the `?FUNCTION_NAME` and `?FUNCTION_ARITY` macros implemented?\n\nHere is an example of how tokens for a simple function looks like:\n\n```\n1\u003e {ok,T,_} = erl_scan:string(\"foo({tag,X,Y}) -\u003e ?FUNCTION_ARITY.\"), T.\n[{atom,1,foo},\n {'(',1},\n {'{',1},\n {atom,1,tag},\n {',',1},\n {var,1,'X'},\n {',',1},\n {var,1,'Y'},\n {'}',1},\n {')',1},\n {'-\u003e',1},\n {'?',1},\n {var,1,'FUNCTION_ARITY'},\n {dot,1}]\n```\n"},{"id":"compiler-time-option","title":"Exploring the Compiler Using the 'time' Option","author":"Björn Gustavsson","excerpt":"\nThis is the first of a series of blog posts about the compiler.  There\nwill be blog posts about how the compiler works now, how it might work\nin the future, and some historical notes to explain why some things\nare what they are. In this blog post I will talk about one of the most\nuseful options for exploring the compiler, namely the `time` option.","article_date":1524096000000,"tags":["compiler BEAM"],"frontmatter":{"layout":"post","title":"Exploring the Compiler Using the 'time' Option","tags":"compiler BEAM","author":"Björn Gustavsson"},"content":"\nThis is the first of a series of blog posts about the compiler.  There\nwill be blog posts about how the compiler works now, how it might work\nin the future, and some historical notes to explain why some things\nare what they are. In this blog post I will talk about one of the most\nuseful options for exploring the compiler, namely the `time` option.\n\nFirst let see `time` in action on a huge file with many functions\nand many variables so that the numbers get interesting:\n\n```\n$ erlc +time NBAP-PDU-Contents.erl\nCompiling \"NBAP-PDU-Contents\"\n remove_file                   :      0.000 s       6.5 kB\n parse_module                  :      0.709 s   25146.1 kB\n transform_module              :      0.000 s   25146.1 kB\n lint_module                   :      0.426 s   25146.1 kB\n expand_records                :      0.086 s   25993.7 kB\n core                          :      0.675 s  282518.3 kB\n sys_core_fold                 :      1.566 s  237885.4 kB\n core_transforms               :      0.000 s  237885.4 kB\n sys_core_bsm                  :      0.205 s  238982.3 kB\n sys_core_dsetel               :      0.108 s  238982.3 kB\n v3_kernel                     :      0.950 s  305320.5 kB\n v3_life                       :      0.453 s  221354.8 kB\n v3_codegen                    :      0.896 s   75801.0 kB\n beam_a                        :      0.080 s   75561.2 kB\n beam_reorder                  :      0.049 s   75561.2 kB\n beam_block                    :      0.361 s   87171.9 kB\n beam_except                   :      0.041 s   81557.7 kB\n beam_bs                       :      0.097 s   79929.2 kB\n beam_type                     :      0.502 s   77270.5 kB\n beam_split                    :      0.042 s   75004.5 kB\n beam_dead                     :      0.356 s   77566.7 kB\n beam_jump                     :      0.232 s   73347.9 kB\n beam_peep                     :      0.164 s   73346.0 kB\n beam_clean                    :      0.150 s   73081.0 kB\n beam_bsm                      :      0.092 s   75473.2 kB\n beam_receive                  :      0.020 s   75473.2 kB\n beam_record                   :      0.023 s   75471.4 kB\n beam_trim                     :      0.042 s   75471.4 kB\n beam_flatten                  :      0.071 s   66745.5 kB\n beam_z                        :      0.019 s   66442.2 kB\n beam_validator                :      0.401 s   66442.2 kB\n beam_asm                      :      0.236 s       6.5 kB\n save_binary                   :      0.000 s       6.5 kB\n```\n\nWhen the `time` option is given, the compiler will print a line after\nexecuting each compiler pass.  First on each line is the name of the\ncompiler pass. Often, but not always, the name is the name of the\nErlang module that implements the compiler pass.\n\nThe name is followed by the time (in seconds) that the compiler\nspent running that compiler pass. For smaller files, the time\nis usually zero or nearly zero. For this huge file, most of the\ntimes are non-zero. For example, the `sys_core_fold` pass needs\nabout one and a half second to do its work.\n\nThe time is followed by the amount of memory used by that compiler\npass.\n\nIn this blog post, I will just talk about a few of the compiler\npasses. There will be more about what the compiler passes do in later\nblog posts.\n\nThe `remove_file` pass is the very first pass run. It removes any\nexisting BEAM file so that there will not be an outdated BEAM file\nin case the compilation fails. The last pass is the `save_binary`\npass. It saves the binary with the BEAM code to the BEAM file.\n\nNow let's see how the output changes if we give the `-S` option:\n\n```\n$ erlc -S +time NBAP-PDU-Contents.erl\nCompiling \"NBAP-PDU-Contents\"\n parse_module                  :      0.718 s   25146.1 kB\n transform_module              :      0.000 s   25146.1 kB\n lint_module                   :      0.420 s   25146.1 kB\n expand_records                :      0.088 s   25993.8 kB\n core                          :      0.671 s  282518.3 kB\n sys_core_fold                 :      1.564 s  237885.4 kB\n core_transforms               :      0.000 s  237885.4 kB\n sys_core_bsm                  :      0.203 s  238982.3 kB\n sys_core_dsetel               :      0.104 s  238982.3 kB\n v3_kernel                     :      0.964 s  305320.5 kB\n v3_life                       :      0.375 s  221354.8 kB\n v3_codegen                    :      1.044 s   75801.0 kB\n beam_a                        :      0.091 s   75561.3 kB\n beam_reorder                  :      0.044 s   75561.3 kB\n beam_block                    :      0.276 s   87171.9 kB\n beam_except                   :      0.028 s   81557.8 kB\n beam_bs                       :      0.103 s   79929.3 kB\n beam_type                     :      0.518 s   77270.5 kB\n beam_split                    :      0.049 s   75004.6 kB\n beam_dead                     :      0.379 s   77566.8 kB\n beam_jump                     :      0.195 s   73347.9 kB\n beam_peep                     :      0.156 s   73346.0 kB\n beam_clean                    :      0.168 s   73081.0 kB\n beam_bsm                      :      0.070 s   75473.2 kB\n beam_receive                  :      0.044 s   75473.2 kB\n beam_record                   :      0.021 s   75471.5 kB\n beam_trim                     :      0.041 s   75471.5 kB\n beam_flatten                  :      0.045 s   66745.5 kB\n beam_z                        :      0.016 s   66442.2 kB\n listing                       :      1.503 s   66442.2 kB\n```\n\nWe can see how the list of passes has changed. The last pass run is\nnow `listing`, which produces a listing of the BEAM assembly code in a\n`.S` file. The `remove_file` pass in the beginning is not run because\nno BEAM file is being produced and any existing BEAM file should be\npreserved.\n\nLet's try one of the many undocumented debugging options:\n\n```\n$ erlc +no_postopt +time NBAP-PDU-Contents.erl\nCompiling \"NBAP-PDU-Contents\"\n remove_file                   :      0.000 s       6.5 kB\n parse_module                  :      0.706 s   25146.1 kB\n transform_module              :      0.000 s   25146.1 kB\n lint_module                   :      0.421 s   25146.1 kB\n expand_records                :      0.090 s   25993.8 kB\n core                          :      0.684 s  282518.3 kB\n sys_core_fold                 :      1.614 s  237885.4 kB\n core_transforms               :      0.000 s  237885.4 kB\n sys_core_bsm                  :      0.210 s  238982.3 kB\n sys_core_dsetel               :      0.105 s  238982.3 kB\n v3_kernel                     :      0.967 s  305320.5 kB\n v3_life                       :      0.353 s  221354.8 kB\n v3_codegen                    :      1.028 s   75801.0 kB\n beam_a                        :      0.091 s   75561.3 kB\n beam_clean                    :      0.201 s   73513.2 kB\n beam_z                        :      0.023 s   72897.9 kB\n beam_validator                :      0.467 s   72897.9 kB\n beam_asm                      :      0.396 s       6.6 kB\n save_binary                   :      0.001 s       6.5 kB\n```\n\nWe can see that far fewer passes were run. The `no_postopt` option\nturns off all optimizations run on the BEAM code (i.e. all optimizations\nafter `v3_codegen`).\n\n## So why is this `time` option useful?\n\n* When compilation of a module is very slow, `time` can show if any particular\npasses are bottlenecks (much slower than the other passes). In fact, a long time\nago the compiler needed several minutes to compile the `NBAP-PDU-Contents` module\nthat I have used an example in this blog post. The `time` option immediately pointed\nout the bottlenecks that I needed to fix.\n\n* If the compiler doesn't terminate when compiling a certain module, `time` will\nshow the last successfully run pass (the one before the culprit).\n\n* The compiler ignores options it doesn't recognize, so if you\nmisremember or misspell an option, the compiler will not do what you\nexpect. Adding the `time` option can help you verify that the expected\ncompiler passes are run.\n\n## Where are all those undocumented options documented?\n\nThere are many options meant for debugging that allow you skip certain optimization\npasses or to produce a listing of the code after a certain pass.\n\nMost of these options can be shown by running `compile:options/0` from the Erlang shell:\n\n```\n$ erl\nErlang/OTP 20 [erts-9.2] [source] [64-bit] [smp:8:8] [ds:8:8:10] [async-threads:10] [hipe] [kernel-poll:false]\n\nEshell V9.2  (abort with ^G)\n1\u003e compile:options().\ndpp - Generate .pp file\n'P' - Generate .P source listing file\ndabstr - Generate .abstr file\ndebug_info - Run save_abstract_code\ndexp - Generate .expand file\n'E' - Generate .E source listing file\ndcore - Generate .core file\nclint0 - Run core_lint_module\ndoldinline - Generate .oldinline file\ndcorefold - Generate .corefold file\ndinline - Generate .inline file\ndcopt - Generate .copt file\n.\n.\n.\n```\n\n## Points to Ponder\n\nWhy does the name of some compiler passes begin with `v3`? Follow this blog, and there might\nbe an answer in a future blog post.\n"},{"id":"IO-Polling","title":"I/O polling options in OTP 21","author":"Lukas Larsson","excerpt":"\nErlang/OTP 21 will introduce a completely new IO polling implementation.\nThis new implementation comes with a new set of tuneable parameters that\ncan be used to get the most out of your system. This blog post describes\nthe parameters and attempts to describe what they should be used for.","article_date":1523404800000,"tags":["erts polling tcp"],"frontmatter":{"layout":"post","title":"I/O polling options in OTP 21","tags":"erts polling tcp","author":"Lukas Larsson"},"content":"\nErlang/OTP 21 will introduce a completely new IO polling implementation.\nThis new implementation comes with a new set of tuneable parameters that\ncan be used to get the most out of your system. This blog post describes\nthe parameters and attempts to describe what they should be used for.\n\nThe I/O polling framework in erts is responsible for delivering events to\nports and processes that have subscribed to events on file descriptors.\nBefore OTP 21 it was the job of an Erlang scheduler thread to deliver these\nevents. In OTP 21 dedicated threads are used to deliver the events.\n\nFor information about how the new implementation works under the hood you can\nlook at Kenneth Lundin's presentation [Erlang VM News Regarding Dirty Schedulers and I/O](http://www.erlang-factory.com/euc2017/kenneth-lundin)\nfrom the EUC 2017.\n\n## Kernel-space vs User-space polling\n\nIn OTP 21 the `+K` option has been removed as it is not longer possible to\nchoose whether to use kernel-space poll or not at run-time. Instead the decision\nis made at compile time where kernel-space poll will be used by default. If you\nwant to use user-space poll instead you have to pass the `--disable-kernel-poll`\nflag to configure when compiling Erlang/OTP.\n\nBefore OTP 21 it made sense to run using user-space polling if the file\ndescriptors that was subscribed to tended to be removed quickly. For example\nif a HTTP server managed short-lived connection from only a handful other\nmachines, it could be beneficial to use user-space poll. However if the\nconnection start being long-lived, or the number of concurrent connection\ngo up, kernel-space poll becomes better.\n\nIn OTP 21, this is no longer true. Because the polling has been moved to another\nthread, it is almost always better to use kernel-space polling. The user-space\npolling implementation is left in place for platforms that do not support\nparallel update of the kernel-space pollset. Also user-space polling is used\nfor individual file descriptors when they cannot be put in a kernel-space pollset\nfor some reason.\n\n## Poll-threads and Poll-sets\n\nOTP 21 introduces two new configuration parameters: +IOt and +IOp.\n\n### Configure +IOt\n\n+IOt controls the number of threads that are used to deliver events. The default\nis 1 and it should be enough for most applications. However on very busy\nsystems with many concurrent connection it could be beneficial to increase this.\nOne way to get an indication of whether your system could benefit from it is\nby using [msacc](http://erlang.org/doc/man/msacc.html). If you turn it on briefly\nand when examining the `msacc:print()` output notice that sleep time\nof the the thread type `poll` is low, the system may benefit from increasing the\nnumber of polling threads.\n\n```\nEshell V9.3  (abort with ^G)\n1\u003e msacc:start(10000),msacc:print().\nAverage thread real-time    : 10000410 us\nAccumulated system run-time :      937 us\nAverage scheduler run-time  :      897 us\n\n        Thread      aux check_io emulator       gc    other     port    sleep\n\nStats per thread:\n     async( 0)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%\n       aux( 1)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%\ndirty_cpu_( 1)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%\ndirty_io_s( 1)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%\n      poll( 0)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%\n scheduler( 1)    0.00%    0.00%    0.00%    0.00%    0.01%    0.00%   99.99%\n\nStats per type:\n         async    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%\n           aux    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%\ndirty_cpu_sche    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%\ndirty_io_sched    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%\n          poll    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%\n     scheduler    0.00%    0.00%    0.00%    0.00%    0.01%    0.00%   99.99%\n```\n\nIn the example above the poll thread is sleeping for 100% of the time so no need to\nincrease the number of poll threads.\n\n### Configure +IOp\n\n+IOp controls the number of pollsets used to put the file descriptors in. This\noptions defaults to 1, and it should be very rare for any system to benefit\nfrom changing this. The only time so far that I have seen it to be beneficial is when the\nkernel-space poll implementation does not scale well when accessed in parallel\nby multiple threads. So if you run [perf top](http://man7.org/linux/man-pages/man1/perf-top.1.html)\n(or something similar) on your system and notice that a lot of time is spent\nlocking the kernel-space pollset, it would be a good idea to increase the\nnumber of pollsets used.\n"}]},"__N_SSG":true},"page":"/blog","query":{},"buildId":"BGhkIUi1bNl03TKudis6o","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"charSet":"utf-8"}],["title",{"children":"Erlang Programming Language"}],["meta",{"httpEquiv":"Content-Type","content":"text/html;charset=utf-8"}],["meta",{"name":"description","content":"Erlang Programming Language"}],["meta",{"name":"keywords","content":"erlang, functional, programming, fault-tolerant, distributed, multi-platform, portable, software, multi-core, smp, concurrency"}],["meta",{"name":"viewport","content":"width=device-width, initial-scale=1.0"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Overall RSS 2.0 Feed","href":"/rss"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"News RSS 2.0 Feed","href":"/rss/news"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Article RSS 2.0 Feed","href":"/rss/articles"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Events RSS 2.0 Feed","href":"/rss/event"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Downloads RSS 2.0 Feed","href":"/rss/download"}],["script",{"src":"https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"}],["script",{"src":"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js","integrity":"sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa","crossOrigin":"anonymous"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-8bebc7aacc0076174a73.js"></script><script src="/_next/static/chunks/main-a9a86200c2afaf3f233a.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" async=""></script><script src="/_next/static/chunks/commons.8b4ad2366e12f882e3d5.js" async=""></script><script src="/_next/static/chunks/pages/_app-79f3144d8877c67ce98b.js" async=""></script><script src="/_next/static/chunks/9f96d65d.6d2fb2f6923d41a412a8.js" async=""></script><script src="/_next/static/chunks/87b308f4a72b1b263da2fe072492c20d1199252a.8368050723e578161621.js" async=""></script><script src="/_next/static/chunks/2968c2e854f156f56dcf4f3a0f05db49ee39d399.938502fc97c89e1a18f7.js" async=""></script><script src="/_next/static/chunks/a9595808cf5ee3285d96b2a14ee913304b9362ca.20c663042d024f8cc93c.js" async=""></script><script src="/_next/static/chunks/202314b546da1167b19f69946a64c65ef91ce335.ea40b862c8a8db76db57.js" async=""></script><script src="/_next/static/chunks/pages/blog-ad17a6542c3c0ffa69b7.js" async=""></script><script src="/_next/static/BGhkIUi1bNl03TKudis6o/_buildManifest.js" async=""></script><script src="/_next/static/BGhkIUi1bNl03TKudis6o/_ssgManifest.js" async=""></script></body></html>