<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>Erlang Programming Language</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"/><meta name="description" content="Erlang Programming Language"/><meta name="keywords" content="erlang, functional, programming, fault-tolerant, distributed, multi-platform, portable, software, multi-core, smp, concurrency"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="alternate" type="application/rss+xml" title="Overall RSS 2.0 Feed" href="/rss"/><link rel="alternate" type="application/rss+xml" title="News RSS 2.0 Feed" href="/rss/news"/><link rel="alternate" type="application/rss+xml" title="Article RSS 2.0 Feed" href="/rss/articles"/><link rel="alternate" type="application/rss+xml" title="Events RSS 2.0 Feed" href="/rss/event"/><link rel="alternate" type="application/rss+xml" title="Downloads RSS 2.0 Feed" href="/rss/download"/><script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><link rel="preload" href="/_next/static/css/f91c265ba4ec340e076d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f91c265ba4ec340e076d.css" data-n-g=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-a9a86200c2afaf3f233a.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.8b4ad2366e12f882e3d5.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-79f3144d8877c67ce98b.js" as="script"/><link rel="preload" href="/_next/static/chunks/9f96d65d.6d2fb2f6923d41a412a8.js" as="script"/><link rel="preload" href="/_next/static/chunks/87b308f4a72b1b263da2fe072492c20d1199252a.8368050723e578161621.js" as="script"/><link rel="preload" href="/_next/static/chunks/2968c2e854f156f56dcf4f3a0f05db49ee39d399.938502fc97c89e1a18f7.js" as="script"/><link rel="preload" href="/_next/static/chunks/a9595808cf5ee3285d96b2a14ee913304b9362ca.20c663042d024f8cc93c.js" as="script"/><link rel="preload" href="/_next/static/chunks/202314b546da1167b19f69946a64c65ef91ce335.8b78b06e27cf5f3f1fd9.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/blog/%5Bblog%5D-68965b4ff10aaca11026.js" as="script"/></head><body><div id="__next"><div class="navbar" style="background-color:#FFF;margin-bottom:0px"><div class="container"><button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse" style="position:absolute;right:5px;margin-bottom:0px"><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand" href="/"><img src="/img/erlang.png" width="60"/></a><div class="nav-collapse collapse navbar-responsive-collapse" style="padding:20px"><ul class="nav navbar-nav"><li><a class="menu-headlines" href="/downloads/"> DOWNLOADS </a></li><li><a class="menu-headlines" href="/docs/"> DOCUMENTATION </a></li><li><a class="menu-headlines" href="/community/"> COMMUNITY </a></li><li><a class="menu-headlines" href="/news/"> NEWS </a></li><li><a class="menu-headlines" href="/eeps/"> EEPS </a></li><li><a class="menu-headlines" href="/blog/"> BLOG </a></li><li><a class="menu-headlines" href="/about/"> ABOUT </a></li></ul></div></div></div><div class="container"><div class="row"><div class="col-lg-12"><div class="divider"><p></p></div></div><div class="col-lg-12"><h3 class="sub-headlines"><img src="/img/news.png"/><span style="position:relative;top:5px;left:20px">NEWS</span></h3></div><div class="col-lg-2"><p></p></div><div class="col-lg-8"><div class="inside-cols"><h3><a href="/blog/OTP-22-Highlights/">OTP 22 Highlights</a></h3><p><em>Monday, 13 May 2019<!-- --> - <!-- -->Lukas Larsson</em></p><p>OTP 22 has just been released. It has been a long process with three release
candidates before the final release. We decided this year to try to get one month
more testing of the major release and I think that the extra time has paid off.
We&#x27;ve received many bug reports from the community about large and small bugs
that our internal tests did not find.</p><p>This blog post will describe some highlights of what is released in OTP 22
and in OTP 21 maintenance patches.</p><p>You can download the readme describing the changes here:
<a href="http://erlang.org/download/otp_src_22.0.readme">OTP 22 Readme</a>.
Or, as always, look at the release notes of the application you are interested in.
For instance here: <a href="http://erlang.org/doc/apps/erts/notes.html">OTP 22 Erts Release Notes</a>.</p><h1>Compiler</h1><p>In OTP 22 we have completely re-implemented the lower levels of the Erlang compiler.
Before this change the Erlang compiler consisted of a number of
IRs (intermediate representations):</p><pre><code>Erlang AST -&gt; Core Erlang -&gt; Kernel Erlang -&gt; Beam Asm</code></pre><p>When compiling an Erlang module, the code is optimized and transformed between
these different IRs. In OTP 22 we have almost removed the <code>Kernel Erlang</code> IR and
added a new IR called <code>Beam SSA</code>. There are a series of blog posts describing this
change in greater details for those that are interested.</p><ul><li><a href="http://blog.erlang.org/introducing-ssa/">Introduction to SSA</a></li><li><a href="http://blog.erlang.org/digging-deeper-in-ssa//">Digging deeper in SSA</a></li><li><a href="http://blog.erlang.org/ssa-history/">SSA History</a></li></ul><p>With this change the compile pipeline now looks like this:</p><pre><code>Erlang AST -&gt; Core Erlang -&gt; Kernel Erlang -&gt; Beam SSA -&gt; Beam Asm</code></pre><p>Together with the SSA rewrite a number of new optimizations have been introduced. One such
is <a href="https://github.com/erlang/otp/pull/1958">strengthening</a> of the
<a href="http://erlang.org/doc/reference_manual/expressions.html#bit-syntax-expressions">bit syntax</a>.
Before the change, you had to be very careful with how you wrote your binary matching in
order for the binary match context optimization to work properly. There were also scenarios
where it was impossible to get the optimization to trigger at all. One place in Erlang/OTP
where this had a great effect was the internal <a href="https://github.com/erlang/otp/blob/master/lib/stdlib/src/string.erl#L1638-L1671">string:bin_search_inv_1</a> function used by <code>string:lexemes/1</code>
and other string functions. We can see the change in the benchmark graph below (where higher
is better and <!-- -->&lt;span style=&quot;color:#0c839c&quot;&gt;<!-- -->the turquoise line<!-- -->&lt;/span&gt;<!-- --> in the OTP 22 branch):</p><p><img src="../images/bsm_opt_lexemes.png" alt="String Lexemes OTP 22 benchmark"/></p><p>You can read more about this optimization in <a href="https://github.com/erlang/otp/pull/1958">PR1958</a>
and <a href="http://blog.erlang.org/retired-pitfalls-22/">Retiring old performance pitfalls</a>.</p><p>Another great optimization is <a href="https://github.com/erlang/otp/pull/2100">PR2100</a> which
makes the compiler&#x27;s type optimization pass work across functions within the same module.
For instance in the code below:</p><pre><code>-record(myrecord, {value}).

h(#myrecord{value=Val}) -&gt;
    #myrecord{value=Val+1}.

i(A) -&gt;
    #myrecord{value=V} = h(#myrecord{value=A}),
    V.</code></pre><p>The new compiler is able to detect the type of the term passed as an argument to
<code>h/1</code> and also the return value of <code>h/1</code> so it can eliminate the record checks
completely. Looking at the BEAM code (produced by <code>erlc -S</code>) of the <code>h/1</code> function we get:</p><p>OTP 21:</p><pre><code>    {test,is_tagged_tuple,{f,9},[{x,0},2,{atom,myrecord}]}.
    {get_tuple_element,{x,0},0,{x,1}}.
    {get_tuple_element,{x,0},1,{x,2}}.
    {gc_bif,&#x27;+&#x27;,{f,0},3,[{x,2},{integer,1}],{x,0}}.
    {test_heap,3,1}.</code></pre><p>OTP 22:</p><pre><code>    {get_tuple_element,{x,0},1,{x,0}}.
    {gc_bif,&#x27;+&#x27;,{f,0},1,[{x,0},{integer,1}],{x,0}}.
    {test_heap,3,1}.</code></pre><p>The <code>is_tagged_tuple</code> instruction has been completely eliminated and as an added bonus
one <code>get_tuple_element</code> was also removed.</p><p>However, this is only the start and we are already looking into making even
better optimizations for OTP 23, building on top of the SSA rewrite.</p><h1>Socket</h1><p>OTP 22 comes with a new experimental <a href="http://erlang.org/doc/man/socket.html">socket</a> API.
The idea behind this API is to have a stable intermediary API that users can use
to create features that are not part of the higher-level gen APIs. We will also be using
this API to re-implement the higher-level gen APIs in OTP 23.</p><p>Another aspect of the new socket API is that it can be used to greatly reduce the
overhead that is inherent with using ports. I wrote this
<a href="https://gist.github.com/garazdawi/cd8ea31acb3284bfc526ae4b1bcb67af">microbenchmark</a>
called gen_tcp2 to see what the difference could be.</p><pre><code>Erlang/OTP 22 [erts-10.4] [source] [64-bit]

Eshell V10.4  (abort with ^G)
1&gt; gen_tcp2:run().
              client             server
 gen_tcp:       12.4 ns/byte       12.4 ns/byte
gen_tcp2:        7.3 ns/byte        7.3 ns/byte
   ratio:       58.9 %             58.9%
ok</code></pre><p>The results seem promising. The socket implementation of gen_tcp uses roughly 40%
less CPU to send the same amount of packets. Of course, gen_tcp does a lot more
than gen_tcp2 (dealing with lots of buffers, error cases and IPv6 to name a new),
so it is not by any means a fair comparison. Though if an application can live
without all the guarantees that come with gen_tcp, then using socket could be
very good for performance.</p><h1>Write concurrency in <code>ordered_sets</code></h1><p><a href="https://github.com/erlang/otp/pull/1952">PR1952</a> contributed by Kjell
Winblad from Uppsala University makes it possible to do updates in
parallel on <code>ets</code> tables of the type <code>ordered_set</code>. This together with
other improvements by Kjell Winblad and Sverker Eriksson
(<a href="https://github.com/erlang/otp/pull/1997">PR1997</a> and
<a href="https://github.com/erlang/otp/pull/2190">PR2190</a>) has greatly
increased the scalability of such ets tables that are the base for
many applications, for instance,
<a href="http://erlang.org/doc/man/pg2.html">pg2</a> and the default <a href="http://erlang.org/doc/man/ssl_session_cache_api.html">ssl session
cache</a>.</p><p><img src="../images/ordered_set_write_conc.png" alt="Ordered Set Write Concurrency OTP 22 benchmark"/></p><p>In the benchmark above we can see that on an <code>ordered_set</code> table the
operations per seconds possible on a 64 core machine has increased
dramatically between OTP 21 and OTP 22. You can see a description of
the benchmark and the results of many more benchmarks
<a href="/bench/ets_ord_set_21_vs_22/21_vs_22.html">here</a>.</p><p>The data structure used to enable <code>write_concurrency</code> in the
<code>ordered_set</code> is called contention adaptive search tree. In a
nutshell, the data structure keeps a shadow tree that represents the
locks needed to read or write a term in the tree. When conflicts
between multiple writers happen, the shadow tree is updated to have
more fine-grained locks for specific branches of the tree. You can
read more about the details of the algorithm in <a href="https://www.sciencedirect.com/science/article/pii/S0743731517303052">A Contention Adapting
Approach to Concurrent Ordered
Sets</a>
(<a href="http://www.it.uu.se/research/group/languages/software/ca_tree/catree_proofs.pdf">PDF</a>).</p><h1>TLS Improvements</h1><p>In OTP 21.3 the culmination of many optimizations in the ssl application was released.
For certain use-cases, the overhead of a using TLS has been significantly reduced. For
instance in this TLS distribution benchmark:</p><p><img src="../images/tls_dist_until_opt.png" alt="TLS Dist OTP 22 benchmark"/></p><p>The bytes per second that the Erlang distribution over TLS is able to send has been
increased from 17K to about 80K, so more than 4 times as much data as before. The
throughput gain above is mostly due to better batching of distribution messages
which makes it so that ssl does not have to add a lot of padding to each message
sent. So it does not translate over to using ssl directly but is still a very nice
performance improvement.</p><p>In OTP 22 the <a href="http://blog.erlang.org/ssl-logging-in-otp-22/">logging facility for ssl</a>
has been greatly improved and there is now basic server support for <code>TLSv1.3</code>. In order to
work with <code>TLSv1.3</code> you need to install an <a href="https://github.com/openssl/openssl">OpenSSL</a>
version that supports <code>TLSv1.3</code> (for instance 1.1.1b), compile Erlang/OTP using
that OpenSSL version and generate the correct certificates. Then we can start a <code>TLSv1.3</code>
server like this:</p><pre><code>LOpts = [{certfile, &quot;tls_server_cert.pem&quot;},
	     {keyfile, &quot;tls_server_key.pem&quot;},
	     {versions, [&#x27;tlsv1.3&#x27;]},
	     {log_level, debug}
	    ],
{ok, LSock} = ssl:listen(8443, LOpts),
{ok, CSock} = ssl:transport_accept(LSock),
{ok, S} = ssl:handshake(CSock).</code></pre><p>And use the <code>OpenSSL</code> client to connect:</p><pre><code>openssl s_client -debug -connect localhost:8443 \
  -CAfile tls_client_cacerts.pem \
  -tls1_3 -groups P-256:X25519</code></pre><p>This will produce a huge amount of logs, but somewhere in there we can see this in Erlang:</p><pre><code>&lt;&lt;&lt; TLS 1.3 Handshake, ClientHello</code></pre><p>and this in <code>OpenSSL</code>:</p><pre><code>New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384</code></pre><p>which means that we have successfully created a new <code>TLSv1.3</code> connection. If you want to
duplicate what I&#x27;ve done you can follow
<a href="https://gist.github.com/garazdawi/062627973b2887e50e9c9bbc86740b63">these instructions</a>.</p><p>Not all features of <code>TLSv1.3</code> have been implemented, you can see which parts of the RFCs
that are missing in the <code>ssl</code> application&#x27;s <a href="http://erlang.org/doc/apps/ssl/standards_compliance.html#tls-1.3">Standard Complience documentation</a>.</p><h1>Fragmented distribution messages</h1><p>In order to deal with the <a href="https://en.wikipedia.org/wiki/Head-of-line_blocking">head of line blocking</a>
caused by sending very large messages over Erlang Distribution, we have added
<a href="https://github.com/erlang/otp/pull/2133">fragmentation of distribution messages</a> in OTP 22.
This means that large messages will now be split up into smaller fragments
allowing smaller messages to be sent without being blocked for a long time.</p><p>If we run the code below that does small rpc calls every 100ms millisecond and
concurrently sends many 1/2 GB terms.</p><pre><code>1&gt; spawn(fun() -&gt;
           (fun F(Max) -&gt;
             {T, _} = timer:tc(fun() -&gt;
                 rpc:call(RemoteNode, erlang, length, [[]])
               end),
             NewMax = lists:max([Max, T]),
             [io:format(&quot;Max: ~p~n&quot;,[NewMax]) || NewMax &gt; Max],
             timer:sleep(100),
             F(NewMax)
           end)(0)
         end).
2&gt; D = lists:duplicate(100000000,100000000),
   [{kjell, RemoteNode} ! D || _ &lt;- lists:seq(1,100)],
   ok.</code></pre><p>Using two of our test machines I get a max latency of about 0.4 seconds on OTP 22,
whereas on OTP 21 the max latency is around 50 seconds. So with the network at our
test site the max latency is decreased by roughly 99%, which is a nice improvement.</p><h1>Counter/Atomics and persistent_terms</h1><p>Three new modules,
<a href="http://erlang.org/doc/man/counters.html"><code>counters</code></a>,
<a href="http://erlang.org/doc/man/atomics.html"><code>atomics</code></a>, and
<a href="http://erlang.org/doc/man/persistent_term.html"><code>persistent_term</code></a>,
were added in OTP 21.2.
These modules make it possible for the user to access low-level primitives of the
runtime to make some spectacular performance improvements.</p><p>For instance, the <code>cover</code> tool was recently re-written to use <code>counters</code> and <code>persistent_term</code>.
Previously it used a bunch of <code>ets</code> tables to keep the counters for when the code was executed,
but now it uses <code>counters</code> and the overhead of running <code>cover</code> has decreased by up to 80%.</p><p><code>persistent_term</code> is adding run-time support for
<a href="https://github.com/mochi/mochiweb/blob/master/src/mochiglobal.erl">mochiglobal</a>
and <a href="https://github.com/discordapp/fastglobal">similar</a> tools. It makes it possible to
very efficiently access data globally but at the cost of making updates very expensive.
In Erlang/OTP we so far use it to optimize <a href="https://github.com/erlang/otp/blob/9c8075413728e3be373d7dff2a7168b3983e0be3/lib/kernel/src/logger_proxy.erl#L45">logger backends</a>
but the use cases are numerous.</p><p>A fun (and possibly useful) use case for <code>atomics</code> is to create a
<a href="https://gist.github.com/garazdawi/48f1284c0d533ab5a39eeac6f8ff99a0">shared mutable bit-vector</a>.
So, now we can spawn 100 processes and play flip that bit with each other:</p><pre><code>BV = bit_vector:new(80),
[spawn(fun F() -&gt;
            bit_vector:flip(BV, rand:uniform(80)-1),
            F()
          end) || _ &lt;- lists:seq(1,100)],
timer:sleep(1000),
bit_vector:print(BV).</code></pre><h1>Documentation Changes</h1><p>In OTP 21.3, the version when all functions and modules were
<a href="https://github.com/erlang/otp/pull/2044">introduced</a> was added to the documentation.</p><p><img src="../images/otp_22_docs.png" alt="Documentation Version OTP 21.3"/></p><p>Sverker used some git magic to figure out when functions and modules were added
and automatically updated all the reference manuals. So now it should be a lot easier
to see when some functionality was introduced. Knowing when an option to functions was
added is still problematic, but we are trying to be better there as well.</p><p>In OTP 22 a new documentation top section called <code>Internal Documentation</code> has been added to
the <a href="http://erlang.org/doc/apps/erts/internal_docs.html">erts</a> and
<a href="http://erlang.org/doc/apps/compiler/internal_docs.html">compiler</a> applications.
The sections contain the internal documentation that previously only has been
available on github so that it easier to access.</p><h1>More Memory optimizations</h1><p>Each major OTP release wouldn&#x27;t be complete without a set of memory allocator improvements
and OTP 22 is no exception. The ones with the most potential to impact your
applications are <a href="https://github.com/erlang/otp/pull/2046">PR2046</a> and
<a href="https://github.com/erlang/otp/pull/1854">PR1854</a>. Both of these optimizations
should allow systems to better utilize memory carriers in high memory
situations allowing your systems to handle more load.</p></div></div><div class="col-lg-2"><p><a href="/rss/blog/"><img src="/img/rss-icon.png" width="64"/></a></p></div></div></div><div class="container"><div class="row"><div class="col-lg-12"><div class="divider"><p></p></div></div><div class="col-lg-12 text-center"><div class="col-lg-4"><a title="DOWNLOAD" href="/download.html"><img src="/img/download.png"/></a></div><div class="col-lg-4"><a href="http://www.github.com/erlang/otp/"><img src="/img/GitHub-Mark-32px.png"/></a></div><div class="col-lg-4"><a href="http://www.twitter.com/erlang_org/"><img src="/img/twitter.png" width="32"/></a></div></div><div class="col-lg-12"><div class="divider"><p></p></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"item":{"id":"OTP-22-Highlights","title":"OTP 22 Highlights","author":"Lukas Larsson","excerpt":"\nOTP 22 has just been released. It has been a long process with three release\ncandidates before the final release. We decided this year to try to get one month\nmore testing of the major release and I think that the extra time has paid off.\nWe've received many bug reports from the community about large and small bugs\nthat our internal tests did not find.","article_date":1557705600000,"tags":["otp","22","release"],"frontmatter":{"layout":"post","title":"OTP 22 Highlights","tags":"otp 22 release","author":"Lukas Larsson"},"content":"\nOTP 22 has just been released. It has been a long process with three release\ncandidates before the final release. We decided this year to try to get one month\nmore testing of the major release and I think that the extra time has paid off.\nWe've received many bug reports from the community about large and small bugs\nthat our internal tests did not find.\n\nThis blog post will describe some highlights of what is released in OTP 22\nand in OTP 21 maintenance patches.\n\nYou can download the readme describing the changes here:\n[OTP 22 Readme](http://erlang.org/download/otp_src_22.0.readme).\nOr, as always, look at the release notes of the application you are interested in.\nFor instance here: [OTP 22 Erts Release Notes](http://erlang.org/doc/apps/erts/notes.html).\n\n# Compiler\n\nIn OTP 22 we have completely re-implemented the lower levels of the Erlang compiler.\nBefore this change the Erlang compiler consisted of a number of\nIRs (intermediate representations):\n\n    Erlang AST -\u003e Core Erlang -\u003e Kernel Erlang -\u003e Beam Asm\n\nWhen compiling an Erlang module, the code is optimized and transformed between\nthese different IRs. In OTP 22 we have almost removed the `Kernel Erlang` IR and\nadded a new IR called `Beam SSA`. There are a series of blog posts describing this\nchange in greater details for those that are interested.\n\n  * [Introduction to SSA](http://blog.erlang.org/introducing-ssa/)\n  * [Digging deeper in SSA](http://blog.erlang.org/digging-deeper-in-ssa//)\n  * [SSA History](http://blog.erlang.org/ssa-history/)\n\nWith this change the compile pipeline now looks like this:\n\n    Erlang AST -\u003e Core Erlang -\u003e Kernel Erlang -\u003e Beam SSA -\u003e Beam Asm\n\nTogether with the SSA rewrite a number of new optimizations have been introduced. One such\nis [strengthening](https://github.com/erlang/otp/pull/1958) of the\n[bit syntax](http://erlang.org/doc/reference_manual/expressions.html#bit-syntax-expressions).\nBefore the change, you had to be very careful with how you wrote your binary matching in\norder for the binary match context optimization to work properly. There were also scenarios\nwhere it was impossible to get the optimization to trigger at all. One place in Erlang/OTP\nwhere this had a great effect was the internal [string:bin\\_search\\_inv\\_1](https://github.com/erlang/otp/blob/master/lib/stdlib/src/string.erl#L1638-L1671) function used by `string:lexemes/1`\nand other string functions. We can see the change in the benchmark graph below (where higher\nis better and \u003cspan style=\"color:#0c839c\"\u003ethe turquoise line\u003c/span\u003e in the OTP 22 branch):\n\n![String Lexemes OTP 22 benchmark](../images/bsm_opt_lexemes.png)\n\nYou can read more about this optimization in [PR1958](https://github.com/erlang/otp/pull/1958)\nand [Retiring old performance pitfalls](http://blog.erlang.org/retired-pitfalls-22/).\n\nAnother great optimization is [PR2100](https://github.com/erlang/otp/pull/2100) which\nmakes the compiler's type optimization pass work across functions within the same module.\nFor instance in the code below:\n\n```\n-record(myrecord, {value}).\n\nh(#myrecord{value=Val}) -\u003e\n    #myrecord{value=Val+1}.\n\ni(A) -\u003e\n    #myrecord{value=V} = h(#myrecord{value=A}),\n    V.\n```\n\nThe new compiler is able to detect the type of the term passed as an argument to\n`h/1` and also the return value of `h/1` so it can eliminate the record checks\ncompletely. Looking at the BEAM code (produced by `erlc -S`) of the `h/1` function we get:\n\nOTP 21:\n```\n    {test,is_tagged_tuple,{f,9},[{x,0},2,{atom,myrecord}]}.\n    {get_tuple_element,{x,0},0,{x,1}}.\n    {get_tuple_element,{x,0},1,{x,2}}.\n    {gc_bif,'+',{f,0},3,[{x,2},{integer,1}],{x,0}}.\n    {test_heap,3,1}.\n```\n\nOTP 22:\n```\n    {get_tuple_element,{x,0},1,{x,0}}.\n    {gc_bif,'+',{f,0},1,[{x,0},{integer,1}],{x,0}}.\n    {test_heap,3,1}.\n```\n\nThe `is_tagged_tuple` instruction has been completely eliminated and as an added bonus\none `get_tuple_element` was also removed.\n\nHowever, this is only the start and we are already looking into making even\nbetter optimizations for OTP 23, building on top of the SSA rewrite.\n\n# Socket\n\nOTP 22 comes with a new experimental [socket](http://erlang.org/doc/man/socket.html) API.\nThe idea behind this API is to have a stable intermediary API that users can use\nto create features that are not part of the higher-level gen APIs. We will also be using\nthis API to re-implement the higher-level gen APIs in OTP 23.\n\nAnother aspect of the new socket API is that it can be used to greatly reduce the\noverhead that is inherent with using ports. I wrote this\n[microbenchmark](https://gist.github.com/garazdawi/cd8ea31acb3284bfc526ae4b1bcb67af)\ncalled gen\\_tcp2 to see what the difference could be.\n\n```\nErlang/OTP 22 [erts-10.4] [source] [64-bit]\n\nEshell V10.4  (abort with ^G)\n1\u003e gen_tcp2:run().\n              client             server\n gen_tcp:       12.4 ns/byte       12.4 ns/byte\ngen_tcp2:        7.3 ns/byte        7.3 ns/byte\n   ratio:       58.9 %             58.9%\nok\n```\n\nThe results seem promising. The socket implementation of gen\\_tcp uses roughly 40%\nless CPU to send the same amount of packets. Of course, gen\\_tcp does a lot more\nthan gen\\_tcp2 (dealing with lots of buffers, error cases and IPv6 to name a new),\nso it is not by any means a fair comparison. Though if an application can live\nwithout all the guarantees that come with gen_tcp, then using socket could be\nvery good for performance.\n\n# Write concurrency in `ordered_sets`\n\n[PR1952](https://github.com/erlang/otp/pull/1952) contributed by Kjell\nWinblad from Uppsala University makes it possible to do updates in\nparallel on `ets` tables of the type `ordered_set`. This together with\nother improvements by Kjell Winblad and Sverker Eriksson\n([PR1997](https://github.com/erlang/otp/pull/1997) and\n[PR2190](https://github.com/erlang/otp/pull/2190)) has greatly\nincreased the scalability of such ets tables that are the base for\nmany applications, for instance,\n[pg2](http://erlang.org/doc/man/pg2.html) and the default [ssl session\ncache](http://erlang.org/doc/man/ssl_session_cache_api.html).\n\n![Ordered Set Write Concurrency OTP 22 benchmark](../images/ordered_set_write_conc.png)\n\nIn the benchmark above we can see that on an `ordered_set` table the\noperations per seconds possible on a 64 core machine has increased\ndramatically between OTP 21 and OTP 22. You can see a description of\nthe benchmark and the results of many more benchmarks\n[here](/bench/ets_ord_set_21_vs_22/21_vs_22.html).\n\nThe data structure used to enable `write_concurrency` in the\n`ordered_set` is called contention adaptive search tree. In a\nnutshell, the data structure keeps a shadow tree that represents the\nlocks needed to read or write a term in the tree. When conflicts\nbetween multiple writers happen, the shadow tree is updated to have\nmore fine-grained locks for specific branches of the tree. You can\nread more about the details of the algorithm in [A Contention Adapting\nApproach to Concurrent Ordered\nSets](https://www.sciencedirect.com/science/article/pii/S0743731517303052)\n([PDF](http://www.it.uu.se/research/group/languages/software/ca_tree/catree_proofs.pdf)).\n\n# TLS Improvements\n\nIn OTP 21.3 the culmination of many optimizations in the ssl application was released.\nFor certain use-cases, the overhead of a using TLS has been significantly reduced. For\ninstance in this TLS distribution benchmark:\n\n![TLS Dist OTP 22 benchmark](../images/tls_dist_until_opt.png)\n\nThe bytes per second that the Erlang distribution over TLS is able to send has been\nincreased from 17K to about 80K, so more than 4 times as much data as before. The\nthroughput gain above is mostly due to better batching of distribution messages\nwhich makes it so that ssl does not have to add a lot of padding to each message\nsent. So it does not translate over to using ssl directly but is still a very nice\nperformance improvement.\n\nIn OTP 22 the [logging facility for ssl](http://blog.erlang.org/ssl-logging-in-otp-22/)\nhas been greatly improved and there is now basic server support for `TLSv1.3`. In order to\nwork with `TLSv1.3` you need to install an [OpenSSL](https://github.com/openssl/openssl)\nversion that supports `TLSv1.3` (for instance 1.1.1b), compile Erlang/OTP using\nthat OpenSSL version and generate the correct certificates. Then we can start a `TLSv1.3`\nserver like this:\n\n```\nLOpts = [{certfile, \"tls_server_cert.pem\"},\n\t     {keyfile, \"tls_server_key.pem\"},\n\t     {versions, ['tlsv1.3']},\n\t     {log_level, debug}\n\t    ],\n{ok, LSock} = ssl:listen(8443, LOpts),\n{ok, CSock} = ssl:transport_accept(LSock),\n{ok, S} = ssl:handshake(CSock).\n```\n\nAnd use the `OpenSSL` client to connect:\n\n    openssl s_client -debug -connect localhost:8443 \\\n      -CAfile tls_client_cacerts.pem \\\n      -tls1_3 -groups P-256:X25519\n\nThis will produce a huge amount of logs, but somewhere in there we can see this in Erlang:\n\n    \u003c\u003c\u003c TLS 1.3 Handshake, ClientHello\n\nand this in `OpenSSL`:\n\n    New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384\n\nwhich means that we have successfully created a new `TLSv1.3` connection. If you want to\nduplicate what I've done you can follow \n[these instructions](https://gist.github.com/garazdawi/062627973b2887e50e9c9bbc86740b63).\n\nNot all features of `TLSv1.3` have been implemented, you can see which parts of the RFCs\nthat are missing in the `ssl` application's [Standard Complience documentation](http://erlang.org/doc/apps/ssl/standards_compliance.html#tls-1.3).\n\n# Fragmented distribution messages\n\nIn order to deal with the [head of line blocking](https://en.wikipedia.org/wiki/Head-of-line_blocking)\ncaused by sending very large messages over Erlang Distribution, we have added\n[fragmentation of distribution messages](https://github.com/erlang/otp/pull/2133) in OTP 22.\nThis means that large messages will now be split up into smaller fragments\nallowing smaller messages to be sent without being blocked for a long time.\n\nIf we run the code below that does small rpc calls every 100ms millisecond and\nconcurrently sends many 1/2 GB terms.\n\n```\n1\u003e spawn(fun() -\u003e\n           (fun F(Max) -\u003e\n             {T, _} = timer:tc(fun() -\u003e\n                 rpc:call(RemoteNode, erlang, length, [[]])\n               end),\n             NewMax = lists:max([Max, T]),\n             [io:format(\"Max: ~p~n\",[NewMax]) || NewMax \u003e Max],\n             timer:sleep(100),\n             F(NewMax)\n           end)(0)\n         end).\n2\u003e D = lists:duplicate(100000000,100000000),\n   [{kjell, RemoteNode} ! D || _ \u003c- lists:seq(1,100)],\n   ok.\n```\n\nUsing two of our test machines I get a max latency of about 0.4 seconds on OTP 22,\nwhereas on OTP 21 the max latency is around 50 seconds. So with the network at our\ntest site the max latency is decreased by roughly 99%, which is a nice improvement.\n\n# Counter/Atomics and persistent_terms\n\nThree new modules,\n[`counters`](http://erlang.org/doc/man/counters.html),\n[`atomics`](http://erlang.org/doc/man/atomics.html), and\n[`persistent_term`](http://erlang.org/doc/man/persistent_term.html),\nwere added in OTP 21.2.\nThese modules make it possible for the user to access low-level primitives of the\nruntime to make some spectacular performance improvements.\n\nFor instance, the `cover` tool was recently re-written to use `counters` and `persistent_term`.\nPreviously it used a bunch of `ets` tables to keep the counters for when the code was executed,\nbut now it uses `counters` and the overhead of running `cover` has decreased by up to 80%.\n\n`persistent_term` is adding run-time support for\n[mochiglobal](https://github.com/mochi/mochiweb/blob/master/src/mochiglobal.erl)\nand [similar](https://github.com/discordapp/fastglobal) tools. It makes it possible to\nvery efficiently access data globally but at the cost of making updates very expensive.\nIn Erlang/OTP we so far use it to optimize [logger backends](https://github.com/erlang/otp/blob/9c8075413728e3be373d7dff2a7168b3983e0be3/lib/kernel/src/logger_proxy.erl#L45)\nbut the use cases are numerous.\n\nA fun (and possibly useful) use case for `atomics` is to create a\n[shared mutable bit-vector](https://gist.github.com/garazdawi/48f1284c0d533ab5a39eeac6f8ff99a0).\nSo, now we can spawn 100 processes and play flip that bit with each other:\n\n```\nBV = bit_vector:new(80),\n[spawn(fun F() -\u003e\n            bit_vector:flip(BV, rand:uniform(80)-1),\n            F()\n          end) || _ \u003c- lists:seq(1,100)],\ntimer:sleep(1000),\nbit_vector:print(BV).\n```\n\n# Documentation Changes\n\nIn OTP 21.3, the version when all functions and modules were\n[introduced](https://github.com/erlang/otp/pull/2044) was added to the documentation.\n\n![Documentation Version OTP 21.3](../images/otp_22_docs.png)\n\nSverker used some git magic to figure out when functions and modules were added\nand automatically updated all the reference manuals. So now it should be a lot easier\nto see when some functionality was introduced. Knowing when an option to functions was\nadded is still problematic, but we are trying to be better there as well.\n\nIn OTP 22 a new documentation top section called `Internal Documentation` has been added to\nthe [erts](http://erlang.org/doc/apps/erts/internal_docs.html) and\n[compiler](http://erlang.org/doc/apps/compiler/internal_docs.html) applications.\nThe sections contain the internal documentation that previously only has been\navailable on github so that it easier to access.\n\n# More Memory optimizations\n\nEach major OTP release wouldn't be complete without a set of memory allocator improvements\nand OTP 22 is no exception. The ones with the most potential to impact your\napplications are [PR2046](https://github.com/erlang/otp/pull/2046) and\n[PR1854](https://github.com/erlang/otp/pull/1854). Both of these optimizations\nshould allow systems to better utilize memory carriers in high memory\nsituations allowing your systems to handle more load.\n"}},"__N_SSG":true},"page":"/blog/[blog]","query":{"blog":"OTP-22-Highlights"},"buildId":"In61zFhZNXGgIV-idPSaW","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"charSet":"utf-8"}],["title",{"children":"Erlang Programming Language"}],["meta",{"httpEquiv":"Content-Type","content":"text/html;charset=utf-8"}],["meta",{"name":"description","content":"Erlang Programming Language"}],["meta",{"name":"keywords","content":"erlang, functional, programming, fault-tolerant, distributed, multi-platform, portable, software, multi-core, smp, concurrency"}],["meta",{"name":"viewport","content":"width=device-width, initial-scale=1.0"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Overall RSS 2.0 Feed","href":"/rss"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"News RSS 2.0 Feed","href":"/rss/news"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Article RSS 2.0 Feed","href":"/rss/articles"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Events RSS 2.0 Feed","href":"/rss/event"}],["link",{"rel":"alternate","type":"application/rss+xml","title":"Downloads RSS 2.0 Feed","href":"/rss/download"}],["script",{"src":"https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"}],["script",{"src":"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js","integrity":"sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa","crossOrigin":"anonymous"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-8bebc7aacc0076174a73.js"></script><script src="/_next/static/chunks/main-a9a86200c2afaf3f233a.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.9707fddd9ae5927c17c3.js" async=""></script><script src="/_next/static/chunks/commons.8b4ad2366e12f882e3d5.js" async=""></script><script src="/_next/static/chunks/pages/_app-79f3144d8877c67ce98b.js" async=""></script><script src="/_next/static/chunks/9f96d65d.6d2fb2f6923d41a412a8.js" async=""></script><script src="/_next/static/chunks/87b308f4a72b1b263da2fe072492c20d1199252a.8368050723e578161621.js" async=""></script><script src="/_next/static/chunks/2968c2e854f156f56dcf4f3a0f05db49ee39d399.938502fc97c89e1a18f7.js" async=""></script><script src="/_next/static/chunks/a9595808cf5ee3285d96b2a14ee913304b9362ca.20c663042d024f8cc93c.js" async=""></script><script src="/_next/static/chunks/202314b546da1167b19f69946a64c65ef91ce335.8b78b06e27cf5f3f1fd9.js" async=""></script><script src="/_next/static/chunks/pages/blog/%5Bblog%5D-68965b4ff10aaca11026.js" async=""></script><script src="/_next/static/In61zFhZNXGgIV-idPSaW/_buildManifest.js" async=""></script><script src="/_next/static/In61zFhZNXGgIV-idPSaW/_ssgManifest.js" async=""></script></body></html>